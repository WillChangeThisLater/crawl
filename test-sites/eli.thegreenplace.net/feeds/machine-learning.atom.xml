<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eli Bendersky's website - Machine Learning</title><link href="https://eli.thegreenplace.net/" rel="alternate"></link><link href="https://eli.thegreenplace.net/feeds/machine-learning.atom.xml" rel="self"></link><id>https://eli.thegreenplace.net/</id><updated>2024-07-10T03:11:34-07:00</updated><entry><title>Asking an LLM to build a simple web tool</title><link href="https://eli.thegreenplace.net/2024/asking-an-llm-to-build-a-simple-web-tool/" rel="alternate"></link><published>2024-07-09T20:09:00-07:00</published><updated>2024-07-10T03:11:34-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-07-09:/2024/asking-an-llm-to-build-a-simple-web-tool/</id><summary type="html">&lt;p&gt;I've been really enjoying following &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's blog posts&lt;/a&gt; recently. Simon shows other
programmers the way LLMs will be used for code assistance in the future, and
posts full interactions with LLMs to build small tools or parts of larger
applications.&lt;/p&gt;
&lt;p&gt;A &lt;a class="reference external" href="https://simonwillison.net/2024/Jul/8/box-shadow-css-generator/"&gt;recent post&lt;/a&gt;
caught my attention; here Simon got …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been really enjoying following &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's blog posts&lt;/a&gt; recently. Simon shows other
programmers the way LLMs will be used for code assistance in the future, and
posts full interactions with LLMs to build small tools or parts of larger
applications.&lt;/p&gt;
&lt;p&gt;A &lt;a class="reference external" href="https://simonwillison.net/2024/Jul/8/box-shadow-css-generator/"&gt;recent post&lt;/a&gt;
caught my attention; here Simon got an LLM (Claude 3.5 Sonnet in this case) to
build a complete tool that lets one configure/tweak box shadow settings
and copy the resulting CSS code for use in a real application. One thing that
seemed interesting is that the LLM in this case used some heavyweight
dependencies (React + JSX) to implement this; Almost 3 MiB of dependency for
something that clearly needs only a few dozen lines of HTML + JS to implement;
yikes.&lt;/p&gt;
&lt;p&gt;So I've decided to try my own experiment and get an LLM to do this without any
dependencies. It turned out to be very easy, because the LLM I used (in this
case ChatGPT 4o, but it could really have been any of the top-tier LLMs, I
think) opted for the no-dependency approach from the start. I was preparing to
ask it to adjust the code to remove dependencies, but this turned out to be
unnecessary.&lt;/p&gt;
&lt;p&gt;The resulting tool is very similar to Simon's in functionality; it's deployed
at &lt;a class="reference external" href="https://eliben.org/box-shadow-tool/"&gt;https://eliben.org/box-shadow-tool/&lt;/a&gt;; here's a screenshot:&lt;/p&gt;
&lt;img alt="Screenshot of box shadow tool" class="align-center" src="https://eli.thegreenplace.net/images/2024/box-shadow-screenshot.png" /&gt;
&lt;p&gt;Here are my prompts:&lt;/p&gt;
&lt;blockquote&gt;
CSS for a slight box shadow, build me a tool that helps me twiddle settings
and preview them and copy and paste out the CSS&lt;/blockquote&gt;
&lt;p&gt;ChatGPT produced a working tool but it didn't really look good on the page.&lt;/p&gt;
&lt;blockquote&gt;
Yes, make the tool itself look a bit better with some CSS so it's all centered
on the screen and there's enough space for the preview box&lt;/blockquote&gt;
&lt;p&gt;It still wasn't quite what I wanted.&lt;/p&gt;
&lt;blockquote&gt;
the container has to be wider so all the text and sliders fix nicely, and
there's still not enough space for the shadows of the preview box to show
without overlapping with other elements&lt;/blockquote&gt;
&lt;p&gt;Now it was looking better; I wanted a button to copy-paste, like in Simon's
demo:&lt;/p&gt;
&lt;blockquote&gt;
this looks better; now add a nice-looking button at the bottom that copies the
resulting css code to the clipboard&lt;/blockquote&gt;
&lt;p&gt;The code ChatGPT produced for the clipboard copy operation was flagged by
vscode as deprecated, so I asked:&lt;/p&gt;
&lt;blockquote&gt;
it seems like &amp;quot;document.execCommand('copy')&amp;quot; is deprecated; is there a more
accepted way to do this?&lt;/blockquote&gt;
&lt;p&gt;The final version can be seen in the &lt;a class="reference external" href="https://eliben.org/box-shadow-tool/"&gt;online demo&lt;/a&gt; (view-source). The complete ChatGPT
transcript is &lt;a class="reference external" href="https://chatgpt.com/share/a05935cb-3e6d-4f5d-997f-5d8541450d1c"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="insights"&gt;
&lt;h2&gt;Insights&lt;/h2&gt;
&lt;p&gt;Overall, this was a positive experience. While a tool like this is very simple
to implement manually, doing it with an LLM was even quicker. The results are
still not perfect in terms of alignment and space, but they're good enough. At
this point one would probably just take over and do the final tweaks manually.&lt;/p&gt;
&lt;p&gt;I was pleasantly surprised by how stable the LLM managed to keep its output
throughout the interaction; it only modified the parts I asked it to, and the
rest of the code remained identical. Stability has been an issue with LLMs
(particularly for images), and I'm happy to see it holds well for code (there
could be some special tuning or prompt engineering for ChatGPT to make this work
well).&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Machine Learning"></category><category term="Javascript"></category></entry><entry><title>Tokens for LLMs: Byte Pair Encoding in Go</title><link href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/" rel="alternate"></link><published>2024-04-25T06:34:00-07:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-04-25:/2024/tokens-for-llms-byte-pair-encoding-in-go/</id><summary type="html">&lt;p&gt;A basic unit of currency in modern LLMs is the &lt;em&gt;token&lt;/em&gt;; &lt;a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"&gt;exciting new models&lt;/a&gt;
have long context windows of millions of &lt;em&gt;tokens&lt;/em&gt;. API pricing for the large
providers is &lt;em&gt;per-token&lt;/em&gt;. We're even seeing the invention of new, derived units
like TPM (&lt;a class="reference external" href="https://ai.google.dev/pricing"&gt;tokens per minute&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But what are tokens?&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"&gt;This OpenAI …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A basic unit of currency in modern LLMs is the &lt;em&gt;token&lt;/em&gt;; &lt;a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"&gt;exciting new models&lt;/a&gt;
have long context windows of millions of &lt;em&gt;tokens&lt;/em&gt;. API pricing for the large
providers is &lt;em&gt;per-token&lt;/em&gt;. We're even seeing the invention of new, derived units
like TPM (&lt;a class="reference external" href="https://ai.google.dev/pricing"&gt;tokens per minute&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But what are tokens?&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"&gt;This OpenAI help article&lt;/a&gt;
tells us that tokens are &lt;em&gt;pieces of words&lt;/em&gt;, and gives some useful rules of thumb
like a token being equivalent to approximately 4 characters or 3/4 of a word
for the English language.&lt;/p&gt;
&lt;p&gt;In this post I want to review the most commonly used algorithm for splitting
text into tokens, provide a complete implementation in Go, and show a playground
for experimenting with it. While my implementation isn't tuned for speed, it
aims to be complete, readable and compatible with OpenAI's &lt;a class="reference external" href="https://github.com/openai/tiktoken"&gt;tiktoken library&lt;/a&gt;, generating identical results and
working with the same vocabulary files.&lt;/p&gt;
&lt;div class="section" id="byte-pair-encoding-introduction"&gt;
&lt;h2&gt;Byte pair encoding - introduction&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Byte_pair_encoding"&gt;Byte pair encoding&lt;/a&gt; (BPE) is
an algorithm originally designed for data compression. A &lt;a class="reference external" href="https://arxiv.org/abs/1508.07909"&gt;2016 paper&lt;/a&gt; suggested re-purposing it for &amp;quot;word
segmentation&amp;quot; for machine learning tasks. The colloquial term for word
segmentation is &lt;em&gt;tokenization&lt;/em&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Input: arbitrary text with words, numbers, whitespace and punctuation.&lt;/li&gt;
&lt;li&gt;Output: list of tokens representing the same text. Each token is an integer
identifier which can be looked up in a vocabulary to reproduce the input text
&lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The BPE algorithm has an important pre-processing step: splitting the input text
into words. The splitting is customizable and different models / vocabularies
use different regexps for splitting (more on this later). The main idea is
some sort of whitespace-based splitting (though whitespace itself is preserved)
because we typically don't want inter-word tokens &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We'll be using this line from a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Blue_(Da_Ba_Dee)"&gt;catchy 1990s song&lt;/a&gt;
as an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i&amp;#39;m blue dabadee dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A word splitter will produce something like the following list, where spaces are
replaced by underscores &lt;tt class="docutils literal"&gt;_&lt;/tt&gt; for the sake of presentation (they remain as
spaces in the actual implementation of the algorithm and its trained
vocabulary):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i
&amp;#39;m
_blue
_dabadee
_dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few things to note:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The contraction &lt;tt class="docutils literal"&gt;'m&lt;/tt&gt; is split from &lt;tt class="docutils literal"&gt;i&lt;/tt&gt; - this is common for English language
splitters, which want things like &lt;tt class="docutils literal"&gt;'m&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;'ll&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;'re&lt;/tt&gt; as separate words.&lt;/li&gt;
&lt;li&gt;Whitespace is preserved and attached at the start of a word. Whitespace is
important because tokens at the beginning of words sometimes have different
semantic meaning from tokens not at the beginning of words.
The choice of where it's attached is arbitrary. From this point on, whitespace
bytes are considered like any other bytes in the BPE algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now is a good time for some terminology we'll be using while talking about BPE:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;Word&lt;/em&gt;: produced by the splitter in pre-processing, like the list shown above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Token&lt;/em&gt;: typically a sub-word sequence of bytes; the output of the tokenizer
is a list of tokens, by ID.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Token ID&lt;/em&gt;: unique numerical identifier for a token.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Vocabulary&lt;/em&gt;: a mapping of token IDs --&amp;gt; token values learned by the tokenizer
during the training process.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Training&lt;/em&gt;: the process in which BPE learns a vocabulary from a corpus of
text.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Splitter regexp&lt;/em&gt;: regular expression used to split text into words during
pre-processing. Given an algorithm (in this case BPE), the pair vocabulary +
splitter regexp unambiguously defines how a given text will be tokenized.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Encoder&lt;/em&gt;: given a vocabulary and a splitter regexp, tokenizes any text into
a list of IDs from the vocabulary.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Decoder&lt;/em&gt;: given a list of IDs and the vocabulary, reconstructs the
original text.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="training"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;BPE training proceeds by first assuming each byte is its own token, and then
successively merging pairs of tokens into longer tokens and adding these to
the vocabulary, until the desired vocabulary size is achieved.&lt;/p&gt;
&lt;p&gt;Let's reuse our example, starting with these words:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i
&amp;#39;m
_blue
_dabadee
_dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The BPE process starts by creating a token for each byte in the inclusive
range [0..255]. So the minimal vocabulary size is 256; this guarantees that
from the very start, there's a valid encoded representation of any text.&lt;/p&gt;
&lt;p&gt;Then, the following process is repeated:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Count how many times each ordered pair of bytes appears in the input.
&lt;em&gt;Ordered pair&lt;/em&gt; here means two bytes right next to each other. In our example,
some such pairs are &amp;quot;bl&amp;quot;, &amp;quot;da&amp;quot;, &amp;quot;de&amp;quot;, &amp;quot;ee&amp;quot; etc.&lt;/li&gt;
&lt;li&gt;Find the pair with the highest count, and create a new token from it (create
a new token ID, mapping it to the concatenation of the most common pair).&lt;/li&gt;
&lt;li&gt;Replace this most common pair with the combined token in the input set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, we start by splitting input words to bytes, so it's a list of
single-byte token lists. This is our working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_ d a b a d e e]
[_ d a b a d a m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we count the frequency of appearance of each ordered pair:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[d a] --&amp;gt; 3
[a b] --&amp;gt; 2
[b a] --&amp;gt; 2
[&amp;#39; m] --&amp;gt; 1
[_ b] --&amp;gt; 1
[l u] --&amp;gt; 1
[u e] --&amp;gt; 1
[_ d] --&amp;gt; 2
[a d] --&amp;gt; 2
[d e] --&amp;gt; 1
[e e] --&amp;gt; 1
[b l] --&amp;gt; 1
[a m] --&amp;gt; 1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The pair &amp;quot;da&amp;quot; is the most common one, so we're creating a new token for it,
and substituting it everywhere in the working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_ da b a d e e]
[_ da b a da m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see, in every instance &amp;quot;d&amp;quot; followed by &amp;quot;a&amp;quot; was combined into &amp;quot;da&amp;quot;.
Now repeat the process; finding the most common pairs in this new working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[e e] --&amp;gt; 1
[a da] --&amp;gt; 1
[l u] --&amp;gt; 1
[_ da] --&amp;gt; 2
[da b] --&amp;gt; 2
[a d] --&amp;gt; 1
[d e] --&amp;gt; 1
[da m] --&amp;gt; 1
[&amp;#39; m] --&amp;gt; 1
[_ b] --&amp;gt; 1
[b l] --&amp;gt; 1
[u e] --&amp;gt; 1
[b a] --&amp;gt; 2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Several pairs have a count of 2, so we pick one arbitrarily. Let's say it's
&lt;tt class="docutils literal"&gt;_da&lt;/tt&gt; (a space followed by &amp;quot;da&amp;quot;). We add &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt; as a new token and
make replacements in the working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_da b a d e e]
[_da b a da m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And so on. When does this process stop? When we either run out of pairs (every
word consists of a single token) or - more realistically for an actual training
corpus - when we reach our desired vocabulary size. For example the vocabulary
used for GPT-4 has around 100,000 tokens (more on this later).&lt;/p&gt;
&lt;p&gt;The output of the training process is a vocabulary; let's say we've only run
two cycles on our input text as described. The vocabulary will have 258 tokens
in it: 256 for the single bytes, one for &lt;tt class="docutils literal"&gt;da&lt;/tt&gt; and another for &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt;. Each
of these would have a unique integer ID.&lt;/p&gt;
&lt;p&gt;In our Go sample code, the training is implemented &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/train.go"&gt;in this file&lt;/a&gt;. You
can set the &lt;tt class="docutils literal"&gt;debugTrain&lt;/tt&gt; variable to &lt;tt class="docutils literal"&gt;true&lt;/tt&gt; to follow the process on some
sample text.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="encoding"&gt;
&lt;h2&gt;Encoding&lt;/h2&gt;
&lt;p&gt;Having learned a vocabulary, the process of encoding is what happens every time
we feed text into an LLM and it needs to be tokenized. The input is arbitrary
text, a splitting regexp and a vocabulary. For example, let's take the input
text &amp;quot;yada daba&amp;quot;. Splitting is performed as before, and the input is broken
into individual bytes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a d a]
[_ d a b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;BPE encoding takes the vocabulary and tries to apply learned tokens to the input
text, word by word. The process is &lt;em&gt;greedy&lt;/em&gt; - tokens are applied in the same
order they've been learned (this is easy to accomplish by assigning
monotonically increasing integer IDs to new tokens in the vocabulary, and then
prioritizing lower-numbered tokens for encoding).&lt;/p&gt;
&lt;p&gt;The first token we learned was &lt;tt class="docutils literal"&gt;da&lt;/tt&gt;, so let's apply that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a da]
[_ da b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next token we learned was &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a da]
[_da b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the final stage; there are no more learned tokens to apply. The result
will consist of 6 tokens.&lt;/p&gt;
&lt;p&gt;In our sample code, the encoder is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/encode.go"&gt;in this file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="realistic-vocabulary-and-splitting"&gt;
&lt;h2&gt;Realistic vocabulary and splitting&lt;/h2&gt;
&lt;p&gt;The examples shown so far have been toys, but the algorithms are real and work
with the actual vocabularies and splitters used in modern models. As a case
study, the tokenizer used for OpenAI's GPT-4 uses a vocabulary called
&lt;tt class="docutils literal"&gt;cl100k_base&lt;/tt&gt;, which contains 100k tokens in addition to the 256 byte-sized
ones. This is also the vocabulary (encoding) the
&lt;a class="reference external" href="https://github.com/openai/tiktoken"&gt;tiktoken&lt;/a&gt; library uses. It can be
freely downloaded from OpenAI - a copy is available in my
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/bpe/data"&gt;sample repository&lt;/a&gt;.
The file is base64 encoded, which is easy to unravel and we'll see
tokens like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot; Fritz&amp;quot;  91083
&amp;quot;Initially&amp;quot;  91084
&amp;quot;nodeValue&amp;quot;  91085
&amp;quot;_TRIANGLES&amp;quot;  91086
&amp;quot;-backend&amp;quot;  91087
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The token string value is to the left, and the numerical token ID is to the
right. As you can see, the algorithm is not particularly discerning about what
it learns - names, pieces of code - whatever works!&lt;/p&gt;
&lt;p&gt;The other important data needed to reproduce OpenAI's tokenization is the
splitting regexp, which is this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(?i:&amp;#39;s|&amp;#39;t|&amp;#39;re|&amp;#39;ve|&amp;#39;m|&amp;#39;ll|&amp;#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's just a combination of several alternatives. You could
use one of the many &amp;quot;regexp explainer&amp;quot; websites out there to study
it, or ask a modern LLM, but the gist of it is: this regexp splits
space-delimited words, leaving spaces in front of the words, with some special
provisions like English contractions (being separate words) and long numbers
being split to groups of 3. For Go programmers, it's important to note that
this pattern uses &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;?!&lt;/span&gt;&lt;/tt&gt; - negative lookahead - which the standard &lt;tt class="docutils literal"&gt;regexp&lt;/tt&gt;
package doesn't support. Therefore, we'll have to reach for the 3rd party
&lt;a class="reference external" href="https://github.com/dlclark/regexp2"&gt;regexp2&lt;/a&gt; to implement this &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our sample repository, take a look at &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/tiktoken_tokenize_test.go"&gt;this test&lt;/a&gt;
that ties everything together - it loads the &lt;tt class="docutils literal"&gt;cl100k_base&lt;/tt&gt; encoding and uses
it alongside the splitting regexp to tokenize some real text.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="full-online-demo-with-a-web-ui-and-webassembly"&gt;
&lt;h2&gt;Full online demo with a web UI and WebAssembly&lt;/h2&gt;
&lt;p&gt;My goal with this project wasn't only to understand the BPE algorithm, but to
also try reproducing the actual tokenizer used by OpenAI for its most modern
models. And this goal was accomplished!&lt;/p&gt;
&lt;p&gt;OpenAI has a nice &lt;a class="reference external" href="https://platform.openai.com/tokenizer"&gt;website here&lt;/a&gt;
that lets you enter text and see how it's tokenized. I've managed to reproduce
this UI - see the &lt;tt class="docutils literal"&gt;cmd/wasm&lt;/tt&gt; directory in the repository. I've also placed it
online - it can ran in your browser &lt;a class="reference external" href="https://eliben.org/bpe/"&gt;from here&lt;/a&gt;.
Here's a screenshot &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Screenshot of tokenizer with a sample text, showing tokens" class="align-center" src="https://eli.thegreenplace.net/images/2024/tokenizer-screenshot.png" /&gt;
&lt;p&gt;How it works: the Go implementation of BPE is compiled to a WebAssembly binary
that's loaded from a bit of glue JavaScript embedded in a simple HTML page.
The JavaScript watches the text box as you type and sends the string to a Go
function exported from the WASM, which tokenizes it on the fly. So we get a nice
effect of &amp;quot;tokens updated as we type&amp;quot;. The selection button at the bottom also
lets us see the numerical IDs for these tokens - they should be equivalent to
what tiktoken is producing.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For simplicity, this post will focus on English. As you'll see, however,
the BPE algorithm is language-agnostic.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;There's also a performance implication: if we make tokenization
word-oriented, we can easily implement streaming tokenization without
depending on previous words.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I think it would be possible - with a bit of effort - to work around this
limitation and stick to the standard library, but just using &lt;tt class="docutils literal"&gt;regexp2&lt;/tt&gt;
is simpler, and it's also what &lt;a class="reference external" href="https://github.com/pkoukk/tiktoken-go"&gt;tiktoken-go&lt;/a&gt;
is doing.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;You'll notice that in this example every word (except contractions) is
a separate token; this shouldn't be surprising, since these are all very
common words and the vocabulary is large! Try playing with it a bit
though, giving it longer words (like &amp;quot;discombobulated&amp;quot;) or non-trivial
variable names from a programming language.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="WebAssembly"></category></entry><entry><title>The life of an Ollama prompt</title><link href="https://eli.thegreenplace.net/2024/the-life-of-an-ollama-prompt/" rel="alternate"></link><published>2024-03-06T05:28:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-03-06:/2024/the-life-of-an-ollama-prompt/</id><summary type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;a previous post&lt;/a&gt;
I've described how - thanks to standardized tooling - we
could use a locally-running &lt;a class="reference external" href="https://blog.google/technology/developers/gemma-open-models/"&gt;Gemma model&lt;/a&gt;
from a Go program within hours from its public release.&lt;/p&gt;
&lt;p&gt;This post dives into the internals of &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt; -
a popular and extremely convenient open-source Go project that makes such
workflows possible.&lt;/p&gt;
&lt;img alt="Mechanical llama being taken apart" class="align-center" src="https://eli.thegreenplace.net/images/2024/mllama.png" /&gt;
&lt;div class="section" id="http-request-to-ollama"&gt;
&lt;h2&gt;HTTP …&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;a previous post&lt;/a&gt;
I've described how - thanks to standardized tooling - we
could use a locally-running &lt;a class="reference external" href="https://blog.google/technology/developers/gemma-open-models/"&gt;Gemma model&lt;/a&gt;
from a Go program within hours from its public release.&lt;/p&gt;
&lt;p&gt;This post dives into the internals of &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt; -
a popular and extremely convenient open-source Go project that makes such
workflows possible.&lt;/p&gt;
&lt;img alt="Mechanical llama being taken apart" class="align-center" src="https://eli.thegreenplace.net/images/2024/mllama.png" /&gt;
&lt;div class="section" id="http-request-to-ollama"&gt;
&lt;h2&gt;HTTP request to Ollama&lt;/h2&gt;
&lt;p&gt;Having &lt;a class="reference external" href="https://ollama.com/download"&gt;installed Ollama&lt;/a&gt; and
run &lt;tt class="docutils literal"&gt;ollama run gemma&lt;/tt&gt;, we're ready to send HTTP requests to it. There are
several ways to do so:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Sending a raw HTTP request with a tool like &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Using Ollama's own client libraries (currently available in Go, Python and
JS)&lt;/li&gt;
&lt;li&gt;Using a provider-agnostic client like LangChainGo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For options (2) and (3) see the Appendix; here we'll focus on (1) for
simplicity and to remove layers from the explanation.&lt;/p&gt;
&lt;p&gt;Let's send an HTTP request to the &lt;tt class="docutils literal"&gt;api/generate&lt;/tt&gt; endpoint of Ollama with
&lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl http://localhost:11434/api/generate -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;gemma&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;,
  &amp;quot;stream&amp;quot;: false
}&amp;#39; | jq .

[...]

{
  &amp;quot;model&amp;quot;: &amp;quot;gemma&amp;quot;,
  &amp;quot;created_at&amp;quot;: &amp;quot;2024-03-04T14:43:51.665311735Z&amp;quot;,
  &amp;quot;response&amp;quot;: &amp;quot;Sure, here is the difference between a comet and a meteor:

  **Comet:**
  - A celestial object that orbits the Sun in a highly elliptical path.
  - Can be seen as a streak of light in the sky, often with a tail.
  - Comets typically have a visible nucleus, meaning a solid core that
    can be seen from Earth.

  **Meteor:**
  - A streak of hot gas or plasma that appears to move rapidly across the sky.
  - Can be caused by small pieces of rock or dust from space that burn up
    in the atmosphere.
  - Meteors do not have a visible nucleus.&amp;quot;,
  &amp;quot;done&amp;quot;: true,
  &amp;quot;context&amp;quot;:
[...]
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(The response is JSON and I've reformatted the text for clarity)&lt;/p&gt;
&lt;p&gt;Ollama's HTTP API is &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/api.md"&gt;documented here&lt;/a&gt;.
For each endpoint, it lists a description of parameters and the data returned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ollama-service"&gt;
&lt;h2&gt;Ollama service&lt;/h2&gt;
&lt;img alt="Internals of ollama, showing service connecting to clients and loading GGUF" class="align-center" src="https://eli.thegreenplace.net/images/2024/ollama-internals.png" /&gt;
&lt;p&gt;Ollama itself is a client-server application; when the installation script is
run, it does several things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Download Ollama binary&lt;/li&gt;
&lt;li&gt;Place it in &lt;tt class="docutils literal"&gt;$PATH&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;tt class="docutils literal"&gt;ollama serve&lt;/tt&gt; as a background service&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The service checks the value of the &lt;tt class="docutils literal"&gt;OLLAMA_HOST&lt;/tt&gt; env var to figure out which
host and port to use. The default is port 11434 on &lt;tt class="docutils literal"&gt;localhost&lt;/tt&gt; (hence you can
see our &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt; request is made to &lt;tt class="docutils literal"&gt;localhost:11434&lt;/tt&gt;). It then listens on
the port, presenting the API discussed above.&lt;/p&gt;
&lt;p&gt;What's interesting to note is that when we run &lt;tt class="docutils literal"&gt;ollama run &amp;lt;model&amp;gt;&lt;/tt&gt; from
the command-line, this invokes the Ollama binary in client mode; in this mode,
it sends requests to the service using the same API. For example, here are
two ways to invoke it - interactive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ollama run gemma
&amp;gt;&amp;gt;&amp;gt; translate naranjo to english
Naranjo translates to Orange in English.

Naranjo is the Spanish word for Orange.

&amp;gt;&amp;gt;&amp;gt; &amp;lt;Ctrl+D&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And piping to stdin:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ echo &amp;quot;translate naranjo to english&amp;quot; | ollama run gemma
Naranjo translates to Orange in English. Orange is the English word equivalent of the word Naranjo.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In both these cases, the Ollama binary sends an HTTP request to
&lt;a class="reference external" href="http://localhost:11434/api/generate"&gt;http://localhost:11434/api/generate&lt;/a&gt;, just like the one we've made manually
with &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-generate-api-endpoint"&gt;
&lt;h2&gt;The &lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; API endpoint&lt;/h2&gt;
&lt;p&gt;Now that we know where our prompt to Ollama ends up (whether we issue it using
an HTTP request or the Ollama command-line tool), let's see what the
&lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; API endpoint actually does.&lt;/p&gt;
&lt;p&gt;Ollama uses the Gin web framework, and the API route is fairly standard:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;POST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/api/generate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GenerateHandler&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This routes HTTP POST requests for &lt;tt class="docutils literal"&gt;/api/generate&lt;/tt&gt; to a handler function called
&lt;tt class="docutils literal"&gt;GenerateHandler&lt;/tt&gt;, which is defined &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/server/routes.go"&gt;in the same source file&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GenerateHandler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;gin&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After parsing and validating the request, &lt;tt class="docutils literal"&gt;GenerateHandler&lt;/tt&gt; starts by
fetching the model the request asked for with the &lt;tt class="docutils literal"&gt;&amp;quot;model&amp;quot;&lt;/tt&gt; field. It then
loads the right model and runs it, feeding it with the prompt provided
in the request. The next sections describe these two steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fetching-and-loading-the-model"&gt;
&lt;h2&gt;Fetching and loading the model&lt;/h2&gt;
&lt;p&gt;When Ollama is looking for a model (by name), it first checks if it already
has it downloaded and stored locally. On my Linux machine, Ollama stores its
local cache of models at &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/usr/share/ollama/.ollama/models/blobs&lt;/span&gt;&lt;/tt&gt;. If the
model is already available locally, there's not much to do for this step.&lt;/p&gt;
&lt;p&gt;Otherwise, Ollama looks in its &lt;a class="reference external" href="https://ollama.com/library"&gt;online library&lt;/a&gt;
of models. Specifically, the service makes a request to
&lt;a class="reference external" href="https://registry.ollama.ai/v2/library/"&gt;https://registry.ollama.ai/v2/library/&lt;/a&gt; to check if a model exists. At the time
of writing, it's not clear if anyone except the Ollama maintainers can upload
new models to the library - but it seems like they're working on this option.&lt;/p&gt;
&lt;p&gt;But where do these models come from? As &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/import.md"&gt;this doc explains&lt;/a&gt;, models are
imported from other sources in formats like GGUF or Safetensors. The topic
of these formats is very interesting, but I won't be covering it in this
post; if you're interested, &lt;a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"&gt;a recent blog post by Vicki Boykis&lt;/a&gt;
provides useful historic background.&lt;/p&gt;
&lt;p&gt;While models can be imported from a variety of formats, Ollama's library
stores them as GGUF and that's what the service expects to find.&lt;/p&gt;
&lt;p&gt;For the purpose of this explanation, it's sufficient to know that GGUF
stores some metadata about the model (e.g. its architecture and parameters,
like numbers of layers in different parts, etc) as well as its actual weights.
The weights can be stored in different formats - some more suitable for
GPUs, some for CPUs. Quantization is common, especially for CPU-oriented models.
The model file is usually a giant multi-GiB binary blob that needs to be
downloaded and cached locally.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="running-the-underlying-model-with-a-prompt"&gt;
&lt;h2&gt;Running the underlying model with a prompt&lt;/h2&gt;
&lt;p&gt;To run the model, Ollama turns to another project - &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;. &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; arose as a local
inference engine for the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/LLaMA"&gt;Llama model&lt;/a&gt;
when it was originally released. Since the model architecture and weights
were published, it became possible to implement inference for the model
without relying on full-blown Python ML frameworks like TensorFlow, PyTorch or
JAX. It uses its author's separate project - &lt;a class="reference external" href="https://github.com/ggerganov/ggml"&gt;ggml&lt;/a&gt;, for an efficient C++ library of ML
primitives that can run on CPUs and GPUs.&lt;/p&gt;
&lt;p&gt;Originally &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; just hard-coded Llama's architecture and loaded the
weights, but in time it grew to incorporate additional open-sourced models
and its implementation became a kind of a &lt;tt class="docutils literal"&gt;switch&lt;/tt&gt; based on
the model's architecture.&lt;/p&gt;
&lt;p&gt;For example, &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/commit/580111d42b3b6ad0a390bfb267d6e3077506eb31"&gt;this commit&lt;/a&gt;
added Gemma support to &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;. Once this is in place, all it needs is to
load the weights and some parameterization of the model from its GGUF file and
it's ready to go.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; is a C++ project that was originally designed as a command-line
utility you can use to load models and chat with them. C++ is not known for
having a pleasant or stable ABI to work with, so many projects wrapped
&lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; with a lightweight C ABI in order to create bindings into other
languages.&lt;/p&gt;
&lt;p&gt;Ollama, as a Go project, did the same. It went a step further though, and
cleverly leverages &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;'s &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server"&gt;server sample&lt;/a&gt;,
which encapsulates all operations in functions that take JSON inputs and
return JSON outputs. Ollama added some glue in &lt;a class="reference external" href="https://github.com/ollama/ollama/tree/main/llm/ext_server"&gt;ext_server&lt;/a&gt;,
and wrapped it with &lt;tt class="docutils literal"&gt;cgo&lt;/tt&gt; to be able to invoke &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; inference
in-process.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; endpoint calls &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama&amp;#64;v0.1.28/llm#LLM.Predict"&gt;llm.Predict&lt;/a&gt;,
which after some hops ends &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;'s &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/server.cpp#L1260"&gt;request_completion&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="afterword-standard-interfaces"&gt;
&lt;h2&gt;Afterword: standard interfaces&lt;/h2&gt;
&lt;p&gt;In my &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;previous post&lt;/a&gt;,
I've mentioned that the flow works and is easy to set up due to standardized
interfaces that have been implemented in OSS projects.&lt;/p&gt;
&lt;p&gt;After reading this post with Ollama internals, I hope it's clear what
standardized interfaces come into play here.&lt;/p&gt;
&lt;p&gt;First and foremost is &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; and its associated GGUF format. While the
internals of &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; are somewhat clunky, this project is unapologetically
pragmatic and a true boon for
the ecosystem because of the way it standardizes LLM inference (and embeddings).
Given a model architecture implemented in C++ in the innards of &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;,
variations can be easily explored and run on compatible CPUs and GPUs. Slight
model modifications? Tuning? Trying some new kind of quantizations? Just create
a GGUF file and &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; will run it for you.&lt;/p&gt;
&lt;p&gt;The other half of the solution is Ollama, which wraps &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; in a
conveniently packaged tool, API and ecosystem &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. As a Go project, it's easily
distributable and makes it trivial to hack on a powerful API server. The REST
API it presents can then be leveraged by any tool capable of issuing HTTP
requests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-go-client-libraries-for-the-ollama-api"&gt;
&lt;h2&gt;Appendix: Go client libraries for the Ollama API&lt;/h2&gt;
&lt;p&gt;If you want to use LLMs programmatically from Go through Ollama, the most
convenient options are either using Ollama's own Go client library or through
LangChainGo. Another option - as discussed above - is to send raw HTTP requests.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama/api"&gt;Ollama Go client library&lt;/a&gt;
is a great option because it's what the Ollama client itself uses to talk to the
service; it's as battle-tested and functional as you can hope for. On the other
hand, LangChainGo is convenient if you use multiple providers and want code
that's consistent and provider-agnostic.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/ollama-go-clients"&gt;This sample&lt;/a&gt;
lists Go code to ask Ollama a question using (1) the Ollama Go library or (2)
LangChainGo.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The Gemma announcement points to this official documentation and
implementation - &lt;a class="reference external" href="https://github.com/google-deepmind/gemma"&gt;https://github.com/google-deepmind/gemma&lt;/a&gt; - it can be
used to re-implement Gemma inference, along with the pre-trained model
weights Google released.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Ollama has additional capabilities I haven't mentioned here, like &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;Modelfiles&lt;/a&gt; for creating
and sharing models.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Gemma, Ollama and LangChainGo</title><link href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/" rel="alternate"></link><published>2024-02-22T16:24:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-02-22:/2024/gemma-ollama-and-langchaingo/</id><summary type="html">&lt;p&gt;Yesterday Google released Gemma - an open LLM that folks can run locally on
their machines (similarly to &lt;tt class="docutils literal"&gt;llama2&lt;/tt&gt;). I was wondering how easy it would be
to run Gemma on my computer, chat with it and interact with it from a Go
program.&lt;/p&gt;
&lt;p&gt;Turns it - thanks to &lt;a class="reference external" href="https://ollama.com/download"&gt;Ollama&lt;/a&gt; - it's extremely …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday Google released Gemma - an open LLM that folks can run locally on
their machines (similarly to &lt;tt class="docutils literal"&gt;llama2&lt;/tt&gt;). I was wondering how easy it would be
to run Gemma on my computer, chat with it and interact with it from a Go
program.&lt;/p&gt;
&lt;p&gt;Turns it - thanks to &lt;a class="reference external" href="https://ollama.com/download"&gt;Ollama&lt;/a&gt; - it's extremely
easy! Gemma was already &lt;a class="reference external" href="https://ollama.com/library/gemma"&gt;added to Ollama&lt;/a&gt;,
so all one has to do is run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ollama run gemma
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And wait for a few minutes while the model downloads. From this point on, my
previous post about &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/"&gt;using Ollama locally in Go&lt;/a&gt; applies
with pretty much no changes. Gemma becomes available through a REST API locally,
and can be accessed from ollama-aware libraries like &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;LangChainGo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I went ahead and added a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--model&lt;/span&gt;&lt;/tt&gt; flag to all my &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain"&gt;code samples from that post&lt;/a&gt;,
and they can all run with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--model&lt;/span&gt; gemma&lt;/tt&gt; now. It all just works, due to the
magic of standard interfaces:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Gemma is packaged in a standard interface for inclusion in Ollama&lt;/li&gt;
&lt;li&gt;Ollama then presents a standardized REST API for this model, just like it
does for other compatible models&lt;/li&gt;
&lt;li&gt;LangChainGo has an Ollama provider that lets us write code to interact with
any model running through Ollama&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we can write code like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;flag&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/ollama&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ollama model name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateFromSinglePrompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Response:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then run it as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ go run ollama-completion-arg.go --model gemma &amp;quot;what should be added to 91 to make -20?&amp;quot;
Response:
 The answer is -111.

91 + (-111) = -20
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Gemma seems relatively fast for a model running on a CPU.
I find that the default 7B model, while much more capable than the default 7B
llama2 based on published benchmarks - also runs about 30% faster on my machine.&lt;/p&gt;
&lt;div class="section" id="without-langchaingo"&gt;
&lt;h2&gt;Without LangChainGo&lt;/h2&gt;
&lt;p&gt;While LangChainGo offers a conveneint API that's standardized across LLM
providers, its use is by no means required for this sample. Ollama itself has
a &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama/api"&gt;Go API&lt;/a&gt; as part of
its structure and it can be used externally as well. Here's an equivalent
sample that doesn't require LangChainGo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;flag&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/jmorganca/ollama/api&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ollama model name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ClientFromEnvironment&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateRequest&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Prompt&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Stream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// disable streaming&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;respFunc&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateResponse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Response&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;respFunc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Response:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>gemini-cli: Access Gemini models from the command-line</title><link href="https://eli.thegreenplace.net/2024/gemini-cli-access-gemini-models-from-the-command-line/" rel="alternate"></link><published>2024-02-21T06:04:00-08:00</published><updated>2024-02-21T14:06:19-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-02-21:/2024/gemini-cli-access-gemini-models-from-the-command-line/</id><summary type="html">&lt;p&gt;This post is about a new command-line tool I've recently built in Go -
&lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;gemini-cli&lt;/a&gt;, and how to use it for
LLM-based data analysis with Google's Gemini models.&lt;/p&gt;
&lt;p&gt;Background: I've been reading &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's&lt;/a&gt; posts about LLMs with interest, especially his
work on tools that leverage LLMs and SQLite to create …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about a new command-line tool I've recently built in Go -
&lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;gemini-cli&lt;/a&gt;, and how to use it for
LLM-based data analysis with Google's Gemini models.&lt;/p&gt;
&lt;p&gt;Background: I've been reading &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's&lt;/a&gt; posts about LLMs with interest, especially his
work on tools that leverage LLMs and SQLite to create &lt;a class="reference external" href="https://til.simonwillison.net/llms/openai-embeddings-related-content"&gt;fun little
analysis pipelines for local documents&lt;/a&gt;.
Since I've recently done some Go work on &lt;a class="reference external" href="https://github.com/google/generative-ai-go"&gt;Google's Gemini SDKs&lt;/a&gt; (also in &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;langchaingo&lt;/a&gt;)
and wrote a &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;couple of&lt;/a&gt;
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/using-gemini-models-in-go-with-langchaingo/"&gt;blog posts&lt;/a&gt;
about it, I was interested in creating a similar pipeline for myself using
Go and Gemini models. This is how the idea for &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; was born.&lt;/p&gt;
&lt;div class="section" id="the-tool"&gt;
&lt;h2&gt;The tool&lt;/h2&gt;
&lt;p&gt;Like any Go command-line tool, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; is very easy to install:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ go install github.com/eliben/gemini-cli@latest
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you're good to go! It will want a Gemini API key set in the
&lt;tt class="docutils literal"&gt;GEMINI_API_KEY&lt;/tt&gt; env var or passed with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--key&lt;/span&gt;&lt;/tt&gt; flag. If you don't have
an API key yet, you can get one quickly and for free from &lt;a class="reference external" href="https://ai.google.dev/"&gt;https://ai.google.dev/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-motivating-task"&gt;
&lt;h2&gt;The motivating task&lt;/h2&gt;
&lt;p&gt;For a while I've been interested in adding a &amp;quot;related posts&amp;quot; feature to my blog.
It was clear that I'll want to use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sentence_embedding"&gt;embeddings&lt;/a&gt; to convert my posts to
vector space and then use vector similarity to find related posts. Check
out my earlier &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/"&gt;post on RAG&lt;/a&gt;
for additional information on these techniques.&lt;/p&gt;
&lt;p&gt;Before starting to write the code, however, I wanted to experiment with
a command-line tool so I could rapidly prototype. Think of it as crafting
some text processing pipeline from classical Unix command-line tools before
trying to implement it in a programming language. &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; excels for
precisely such prototyping.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="finding-related-posts"&gt;
&lt;h2&gt;Finding related posts&lt;/h2&gt;
&lt;p&gt;Let's see how to use &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; for my task. I have access to the contents
of my blog posts on the file system as a large bunch of
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/ReStructuredText"&gt;reStructuredText&lt;/a&gt; and HTML
files. These are private, but you're free to replicate this experiment for
any collection of textual documents you have handy. It will even work on
programming language source code!&lt;/p&gt;
&lt;p&gt;Let's first get the lay of the land - how many files are there &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pss -f --rst content/|wc -l
279
$ pss -f --html content/|wc -l
1064
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;OK, so a bit over 1300 overall. Let's start by computing the embeddings for
the reST files. We'll ask &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; to write it into a new SQLite
DB called &lt;tt class="docutils literal"&gt;blogemb.db&lt;/tt&gt;, using its &lt;tt class="docutils literal"&gt;embed db&lt;/tt&gt; subcommand:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ export GEMINI_API_KEY=...
$ gemini-cli embed db blogemb.db --files content/,&amp;quot;*.rst&amp;quot;
Found 279 values to embed
Splitting to 9 batches
Embedding batch #1 / 9, size=32
Embedding batch #2 / 9, size=32
Embedding batch #3 / 9, size=32
Embedding batch #4 / 9, size=32
Embedding batch #5 / 9, size=32
Embedding batch #6 / 9, size=32
Embedding batch #7 / 9, size=32
Embedding batch #8 / 9, size=32
Embedding batch #9 / 9, size=23
Collected 279 embeddings; inserting into table embeddings
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's look at the DB file using the &lt;tt class="docutils literal"&gt;sqlite3&lt;/tt&gt; command-line tool:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sqlite3 blogemb.db
SQLite version 3.37.2 2022-01-06 13:25:41
Enter &amp;quot;.help&amp;quot; for usage hints.

sqlite&amp;gt; .tables
embeddings

sqlite&amp;gt; .schema
CREATE TABLE embeddings (
id TEXT PRIMARY KEY,
embedding BLOB
);

sqlite&amp;gt; select count(*) from embeddings;
279

sqlite&amp;gt; select id, length(embedding) from embeddings limit 10;
content/2014/blogging-setup-with-pelican.rst|3072
content/2014/c++-perfect-forwarding-and-universal-references.rst|3072
content/2014/derivation-normal-equation-linear-regression.rst|3072
content/2014/goodbye-wordpress.rst|3072
content/2014/highlight-tab-gnome-terminal.rst|3072
content/2014/meshgrids-and-disambiguating-rows-and-columns-from-cartesian-coordinates.rst|3072
content/2014/samples-for-llvm-clang-library.rst|3072
content/2014/sfinae-and-enable-if.rst|3072
content/2014/summary-of-reading-july-september-2014.rst|3072
content/2014/summary-of-reading-october-december-2014.rst|3072
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As expected, we see 279 entries in the table; for each row the &lt;tt class="docutils literal"&gt;id&lt;/tt&gt; column
value is the path of the file and &lt;tt class="docutils literal"&gt;embedding&lt;/tt&gt; contains the embedding as a
blob. Embeddings are returned by the model as arrays of 32-bit floats, and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; encodes them into a blob as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;// encodeEmbedding encodes an embedding into a byte buffer, e.g. for DB&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// storage as a blob.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;encodeEmbedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;byte&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bytes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Buffer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;f&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;LittleEndian&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nb"&gt;panic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Bytes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Each &lt;tt class="docutils literal"&gt;float32&lt;/tt&gt; thus occupies 4 bytes; since our DB blobs are 3072 bytes long,
we can infer that each embedding vector has 768 elements; the embedding model
projects our text into 768-dimensional space &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Back to our task, though. Note that &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; uses the batch-embedding
API of Gemini under the hood, so it's efficient for large input corpora. We
can control the batch size with a flag; just for fun, let's do this when
embedding the HTML files since there are so many of them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed db blogemb.db --batch-size=64 --files content/,&amp;quot;*.html&amp;quot;
Found 1064 values to embed
Splitting to 17 batches
Embedding batch #1 / 17, size=64
Embedding batch #2 / 17, size=64
Embedding batch #3 / 17, size=64
Embedding batch #4 / 17, size=64
Embedding batch #5 / 17, size=64
Embedding batch #6 / 17, size=64
Embedding batch #7 / 17, size=64
Embedding batch #8 / 17, size=64
Embedding batch #9 / 17, size=64
Embedding batch #10 / 17, size=64
Embedding batch #11 / 17, size=64
Embedding batch #12 / 17, size=64
Embedding batch #13 / 17, size=64
Embedding batch #14 / 17, size=64
Embedding batch #15 / 17, size=64
Embedding batch #16 / 17, size=64
Embedding batch #17 / 17, size=40
Collected 1064 embeddings; inserting into table embeddings
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A brief note on performance: with a batch size of 64, this process took only
17 seconds - not bad for over a thousand documents. In the future I plan to
improve this time further with more concurrency and smarter batch size selection
&lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's examine the resulting SQLite DB with all the embeddings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ stat -c %s blogemb.db
5627904
$ echo &amp;quot;select count(*) from embeddings&amp;quot; | sqlite3 blogemb.db
1343
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All 1343 entries have made it into the &lt;tt class="docutils literal"&gt;embeddings&lt;/tt&gt; table, and the total
size of the DB is just over 5 MiB.&lt;/p&gt;
&lt;p&gt;Now we're ready to look for related posts. The &lt;tt class="docutils literal"&gt;embed similar&lt;/tt&gt; subcommand
takes the name of a SQLite DB that holds all embeddings (like the one we've just
created) and a string of content to compare; it also accepts &lt;tt class="docutils literal"&gt;-&lt;/tt&gt; as an
indication that the input content will be piped through standard input, so
let's use that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed similar blogemb.db - &amp;lt; content/2023/better-http-server-routing-in-go-122.rst
{&amp;quot;id&amp;quot;:&amp;quot;content/2023/better-http-server-routing-in-go-122.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;1.0000001&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/rest-servers-in-go-part-2-using-a-router-package.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.8904768&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/life-of-an-http-request-in-a-go-server.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.83037585&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/rest-servers-in-go-part-5-middleware.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.8136583&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2022/serving-static-files-and-web-apps-in-go.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7732284&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is in &lt;a class="reference external" href="https://jsonlines.org/examples/"&gt;the JSON Lines format&lt;/a&gt;,
and by default prints the ID and the similarity score (using cosine
similarity), sorted by decreasing similarity. Unsurprisingly, the most similar
post is... itself, with a perfect similarity score of 1.0&lt;/p&gt;
&lt;p&gt;The results look pretty good! The most similar posts found indeed are very
relevant to the one we were asking about. For fun, let's try a book review and
now with a larger list of output candidates (by using the &lt;tt class="docutils literal"&gt;topk&lt;/tt&gt; flag):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed similar blogemb.db --topk=10 - &amp;lt; content/2011/book-review-the-voyage-of-the-beagle-by-charles-darwin.html
{&amp;quot;id&amp;quot;:&amp;quot;content/2011/book-review-the-voyage-of-the-beagle-by-charles-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;1&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-the-origin-of-species-by-charles-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.80570847&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2006/book-review-the-selfish-gene-by-richard-dawkins.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7845073&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2011/summary-of-reading-april-june-2011.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7939675&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2004/book-review-a-short-history-of-nearly-by-bill-bryson.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7784306&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-around-the-world-in-80-days-by-jules-verne.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7792236&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-the-double-helix-by-james-watson.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7658307&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-after-tamerlane-by-john-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7641713&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-mysterious-island-by-jules-verne.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7605505&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-the-adventures-of-tom-sawyer-by-mark-twain.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.75610566&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next&lt;/h2&gt;
&lt;p&gt;For my task, I now have the basic information available to implement it, and
all the infrastructure for running experiments; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; in hand,
this took less than 5 minutes. All I needed to do is
&lt;a class="reference external" href="https://xkcd.com/1205/"&gt;write the tool&lt;/a&gt; :-)&lt;/p&gt;
&lt;p&gt;I really enjoyed building &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt;; it's true to the spirit of simple,
textual Unix CLIs that can be easily combined together through pipes. Using
SQLite as the storage and retrieval format is also quite pleasant, and provides
interoperability for free.&lt;/p&gt;
&lt;p&gt;For you - if you're a Go developer interested in building stuff with LLMs and
getting started for free - I hope you find &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; useful. I've only
shown its &lt;tt class="docutils literal"&gt;embed *&lt;/tt&gt; subcommands, but the CLI also lets you chat with an LLM
through the terminal, query the API for various model details, and everything
is configurable with extra flags.&lt;/p&gt;
&lt;p&gt;It's &lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;open-source&lt;/a&gt;, of course; the
README file rendered on GitHub has extensive documentation, and more is
available by running &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt; help&lt;/tt&gt;. Try it, ask questions, open issues!&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I like using &lt;a class="reference external" href="https://github.com/eliben/pss/"&gt;pss&lt;/a&gt;, but feel free
to use your favorite tools - &lt;tt class="docutils literal"&gt;git grep&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;ag&lt;/tt&gt; or just a concoction
of &lt;tt class="docutils literal"&gt;find&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;grep&lt;/tt&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;A word of caution: LLMs have limited context window sizes; for embeddings,
if the input is larger than the model's context window it may get
truncated - so it's the user's responsibility to ensure that input
documents are properly sized.&lt;/p&gt;
&lt;p class="last"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; will report the maximal number of input tokens for
supported models when you invoke the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt; models&lt;/tt&gt; command.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We have to be careful with too much parallelism, because at the free
tier the Gemini SDK may be rate-limited.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="Software &amp; Tools"></category></entry><entry><title>Using Gemini models in Go with LangChainGo</title><link href="https://eli.thegreenplace.net/2024/using-gemini-models-in-go-with-langchaingo/" rel="alternate"></link><published>2024-01-30T18:23:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-01-30:/2024/using-gemini-models-in-go-with-langchaingo/</id><summary type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;a previous post&lt;/a&gt; I've
discussed how to access Google's multimodal Gemini models from Go (with a nice
free tier!)&lt;/p&gt;
&lt;p&gt;Recently, Google's SDKs were added as providers for &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;LangChainGo&lt;/a&gt;; this makes it possible to use the
capabilities of the LangChain framework with Google's Gemini models as LLM
providers.&lt;/p&gt;
&lt;p&gt;This post …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;a previous post&lt;/a&gt; I've
discussed how to access Google's multimodal Gemini models from Go (with a nice
free tier!)&lt;/p&gt;
&lt;p&gt;Recently, Google's SDKs were added as providers for &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;LangChainGo&lt;/a&gt;; this makes it possible to use the
capabilities of the LangChain framework with Google's Gemini models as LLM
providers.&lt;/p&gt;
&lt;p&gt;This post shows some samples of using these new providers and
how simple it is to switch providers from &lt;a class="reference external" href="https://ai.google.dev/"&gt;Google AI&lt;/a&gt; (which uses API keys) to &lt;a class="reference external" href="https://cloud.google.com/vertex-ai?hl=en"&gt;Vertex&lt;/a&gt; (which requires a GCP project).&lt;/p&gt;
&lt;div class="section" id="langchaingo-examples-with-googleai"&gt;
&lt;h2&gt;LangChainGo examples with GoogleAI&lt;/h2&gt;
&lt;p&gt;Let's start with the GoogleAI provider. We'll need the latest release of
&lt;tt class="docutils literal"&gt;langchaingo&lt;/tt&gt;. Here's a complete example of asking the model a basic textual
question:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;os&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/googleai&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;prompt&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;What is the L2 Lagrange point?&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateFromSinglePrompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;llms.GenerateFromSinglePrompt&lt;/tt&gt; is a convenience function in LangChainGo
for cases where we have a single string input and want a single string output.
The more general API is the &lt;tt class="docutils literal"&gt;Model.GenerateContent&lt;/tt&gt; method, which supports
multiple messages and different message kinds (text, images, etc.); Here's
an example using it to reproduce our question about the difference between
turtle images from &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;the previous post&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;imgData1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReadFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;imagesPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;turtle1.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;imgData2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReadFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;imagesPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;turtle2.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;parts&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ContentPart&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;BinaryPart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;image/png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imgData1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;BinaryPart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;image/png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imgData2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;TextPart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Describe the difference between these two pictures, with scientific detail&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;content&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MessageContent&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nx"&gt;Role&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ChatMessageTypeHuman&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nx"&gt;Parts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;parts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateContent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;content&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gemini-pro-vision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MarshalIndent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;    &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that we pass the specific model we want to utilize here using the
&lt;tt class="docutils literal"&gt;WithModel&lt;/tt&gt; option to &lt;tt class="docutils literal"&gt;llm.GenerateContent&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Finally, an example of using an embedding model to calculate embeddings for
text:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;googleai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;apiKey&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;texts&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;parrot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;CreateEmbedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Num of embedding vectors:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%d: %v...\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="switching-to-vertex"&gt;
&lt;h2&gt;Switching to Vertex&lt;/h2&gt;
&lt;p&gt;Switching to the Vertex provider is very easy, since it implements exactly
the same LangChainGo interfaces. We have to change the import line from:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/googleai&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/googleai/vertex&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then replace our &lt;tt class="docutils literal"&gt;llm&lt;/tt&gt; value creation with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nx"&gt;project&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;VERTEX_PROJECT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nx"&gt;location&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;VERTEX_LOCATION&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vertex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vertex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithCloudProject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;project&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vertex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithCloudLocation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;location&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The rest of the code remains the same!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full, runnable code for these samples (both for Google AI and Vertex) is
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/langchaingo-gemini"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Using Gemini models from Go</title><link href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/" rel="alternate"></link><published>2023-12-22T17:45:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-12-22:/2023/using-gemini-models-from-go/</id><summary type="html">&lt;p&gt;Google has recently made their newest family of multimodal LLMs available
&lt;a class="reference external" href="https://ai.google.dev/"&gt;via an API&lt;/a&gt; with a generous free tier.
Google also released SDKs in several popular programming languages, including
Go.&lt;/p&gt;
&lt;p&gt;This post is a quick overview of how to get started with the Go SDK to ask
the model questions …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Google has recently made their newest family of multimodal LLMs available
&lt;a class="reference external" href="https://ai.google.dev/"&gt;via an API&lt;/a&gt; with a generous free tier.
Google also released SDKs in several popular programming languages, including
Go.&lt;/p&gt;
&lt;p&gt;This post is a quick overview of how to get started with the Go SDK to ask
the model questions that mix text with images.&lt;/p&gt;
&lt;div class="section" id="the-task"&gt;
&lt;h2&gt;The task&lt;/h2&gt;
&lt;p&gt;We'll be asking the model to explain the difference between two images
of turtles; this one:&lt;/p&gt;
&lt;img alt="first turtle image" class="align-center" src="https://eli.thegreenplace.net/images/2023/turtle1.png" style="width: 320px;" /&gt;
&lt;p&gt;And this one:&lt;/p&gt;
&lt;img alt="second turtle image" class="align-center" src="https://eli.thegreenplace.net/images/2023/turtle2.png" style="width: 320px;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="using-the-google-ai-sdk"&gt;
&lt;h2&gt;Using the Google AI SDK&lt;/h2&gt;
&lt;p&gt;With the &lt;a class="reference external" href="https://ai.google.dev/"&gt;Google AI SDK&lt;/a&gt;, all you need to access
the model is generate an API key (similarly to how it works with OpenAI's API).
The Go SDK lives at &lt;a class="reference external" href="https://github.com/google/generative-ai-go"&gt;https://github.com/google/generative-ai-go&lt;/a&gt;, with
package documentation at &lt;a class="reference external" href="https://pkg.go.dev/github.com/google/generative-ai-go"&gt;https://pkg.go.dev/github.com/google/generative-ai-go&lt;/a&gt;;
it has a good &lt;a class="reference external" href="https://pkg.go.dev/github.com/google/generative-ai-go/genai#pkg-examples"&gt;section of examples&lt;/a&gt;
we can follow.&lt;/p&gt;
&lt;p&gt;Here's the code for our task:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;encoding/json&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;os&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/google/generative-ai-go/genai&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;google.golang.org/api/option&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;NewClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;option&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithAPIKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerativeModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gemini-pro-vision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;imgData1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReadFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../images/turtle1.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;imgData2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReadFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;../images/turtle2.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;prompt&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Part&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ImageData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imgData1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ImageData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imgData2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Describe the difference between these two pictures, with scientific detail&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateContent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;prompt&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MarshalIndent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;    &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since the LLM API is multimodal, the SDK provides helper types like
&lt;tt class="docutils literal"&gt;genai.ImageData&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;genai.Text&lt;/tt&gt; to wrap inputs in a type-safe way.
When we run this sample, we get the model's response dumped as a JSON object.
The important part is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot;Content&amp;quot;: {
  &amp;quot;Parts&amp;quot;: [
    &amp;quot;The first picture is of a tortoise, which is a reptile characterized by
    its hard shell. The second picture is of a sea turtle, which is a reptile
    characterized by its flippers and streamlined shell. Tortoises are
    terrestrial animals, while sea turtles are marine animals. Tortoises have
    a domed shell, while sea turtles have a flattened shell. Tortoises have
    thick, scaly skin, while sea turtles have smooth, leathery skin. Tortoises
    have short legs with claws, while sea turtles have long flippers.
    Tortoises have a slow metabolism and can live for over 100 years, while
    sea turtles have a faster metabolism and typically live for around 50
    years.&amp;quot;
  ],
  &amp;quot;Role&amp;quot;: &amp;quot;model&amp;quot;
},
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;OK, so now we know :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-the-gcp-vertex-sdk"&gt;
&lt;h2&gt;Using the GCP Vertex SDK&lt;/h2&gt;
&lt;p&gt;If you're a GCP customer and have your GCP project set up with billing and
everything else, you may want to use the
&lt;a class="reference external" href="https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview"&gt;Vertex Go SDK instead&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The great thing about how the Go SDKs work is that you barely have to change
your code at all! The only changes are the &lt;tt class="docutils literal"&gt;import&lt;/tt&gt; line, from:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/google/generative-ai-go/genai&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;cloud.google.com/go/vertexai/genai&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then change how you create the client, since the auth is different. For
Vertex, the client should be created like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;genai&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;NewClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;GCP_PROJECT_ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;us-central1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;tt class="docutils literal"&gt;GCP_PROJECT_ID&lt;/tt&gt; is an env var with your GCP project and the
location/region can be set based on your preferences. The rest of the code
remains exactly the same!&lt;/p&gt;
&lt;p&gt;There are two SDKs because the features offered by the two products can differ
in some cases. For example, the GCP one may allow you to read data directly
from your storage buckets or database tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for all the samples in this post - along with the sample
images - is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-google-ai-gemini"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 2024-01-31&lt;/strong&gt;: see &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/using-gemini-models-in-go-with-langchaingo/"&gt;this post about accessing the Gemini models
via langchaingo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Using Ollama with LangChainGo</title><link href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/" rel="alternate"></link><published>2023-11-22T05:25:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-11-22:/2023/using-ollama-with-langchaingo/</id><summary type="html">&lt;p&gt;One of the most exciting areas of LLM-related development in 2023 is the
availability of powerful (and sometimes even open-source) models we can run
locally on our machines.&lt;/p&gt;
&lt;p&gt;Several tools exist that make it relatively easy to obtain, run and manage
such models locally; for example &lt;a class="reference external" href="https://ollama.ai/"&gt;Ollama&lt;/a&gt; (written in Go …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the most exciting areas of LLM-related development in 2023 is the
availability of powerful (and sometimes even open-source) models we can run
locally on our machines.&lt;/p&gt;
&lt;p&gt;Several tools exist that make it relatively easy to obtain, run and manage
such models locally; for example &lt;a class="reference external" href="https://ollama.ai/"&gt;Ollama&lt;/a&gt; (written in Go!)
&lt;a class="reference external" href="https://localai.io/"&gt;LocalAI&lt;/a&gt; (also largely in Go!).&lt;/p&gt;
&lt;p&gt;In this post I'm going to describe how to use Ollama to run a model locally,
communicate with it using its API and integrate it into a Go program using
LangChainGo.&lt;/p&gt;
&lt;img alt="Ollama logo, taken from the Ollama website" class="align-center" src="https://eli.thegreenplace.net/images/2023/ollama.png" /&gt;
&lt;div class="section" id="setting-up-ollama"&gt;
&lt;h2&gt;Setting up Ollama&lt;/h2&gt;
&lt;p&gt;To start, follow the installation and setup instructions &lt;a class="reference external" href="https://ollama.ai/download"&gt;from the Ollama
website&lt;/a&gt;. Ollama runs as a service, exposing a
REST API on a localhost port. Once installed, you can invoke &lt;tt class="docutils literal"&gt;ollama run
&amp;lt;modelname&amp;gt;&lt;/tt&gt; to talk to this model; the model is downloaded and cached the
first time it's requested.&lt;/p&gt;
&lt;p&gt;In this blog post, we'll be talking to the &lt;tt class="docutils literal"&gt;llama2&lt;/tt&gt; model, so run &lt;tt class="docutils literal"&gt;ollama run
llama2&lt;/tt&gt;. After the &lt;tt class="docutils literal"&gt;ollama&lt;/tt&gt; command finishes installing the model, we'll see
a prompt and will be able to chat with it &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; very briefly, tell me the difference between a comet and a meteor

 Sure! Here&amp;#39;s a brief difference:

A comet is a small, icy body that orbits the sun. When a comet approaches the
inner solar system, the heat from the sun causes the comet to release gas and
dust, creating a bright tail that can be seen from Earth.

A meteor, on the other hand, is a small piece of rock or metal that enters the
Earth&amp;#39;s atmosphere. As it travels through the atmosphere, the friction causes
the meteor to heat up and burn, producing a bright streak of light in the sky,
commonly known as a shooting star. If the meteor survives its passage through
the atmosphere and lands on Earth, it is called a meteorite.
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="manually-invoking-the-rest-api"&gt;
&lt;h2&gt;Manually invoking the REST API&lt;/h2&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;ollama&lt;/tt&gt; runs in the background and exposes a REST API on port 11434. We
can talk to it &amp;quot;manually&amp;quot; using &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt; commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl http://localhost:11434/api/generate -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;llama2&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;,
  &amp;quot;stream&amp;quot;: false
}&amp;#39;
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:53:47.32607236Z&amp;quot;,
 &amp;quot;response&amp;quot;:&amp;quot;\nSure! Here&amp;#39;s the difference:\n\nA comet is a small,
  icy body that orbits the sun. Comets are composed of dust and frozen
  gases, such as water, methane, and ammonia. When a comet approaches
  the inner solar system, the sun&amp;#39;s heat causes the comet&amp;#39;s ices
  to vaporize, creating a bright tail of gas and dust that can be seen
  from Earth.\n\nA meteor, on the other hand, is a small body of rock
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This may take a bit of time, especially if your machine doesn't have a powerful
GPU. We can also ask Ollama to &lt;em&gt;stream&lt;/em&gt; the model's responses so we get output
as soon as it's ready, before waiting for the model to complete its reply. We
can do that by passing &lt;tt class="docutils literal"&gt;&amp;quot;stream&amp;quot;: true&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl http://localhost:11434/api/generate -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;llama2&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;,
  &amp;quot;stream&amp;quot;: true
}&amp;#39;
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:06.709696317Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;\n&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:06.89584866Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; Sure&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.053242632Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;!&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.217867169Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; Here&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.374557181Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;&amp;#39;&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.560674269Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;s&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.719981235Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; the&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:07.878008762Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; quick&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:08.035846088Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; and&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:08.192951527Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot; dirty&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:08.372491712Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;:&amp;quot;,&amp;quot;done&amp;quot;:false}
{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;,&amp;quot;created_at&amp;quot;:&amp;quot;2023-11-20T14:57:08.530388951Z&amp;quot;,&amp;quot;response&amp;quot;:&amp;quot;\n&amp;quot;,&amp;quot;done&amp;quot;:false}
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The response is broken into separate JSON messages with &lt;tt class="docutils literal"&gt;&amp;quot;done&amp;quot;: false&lt;/tt&gt;. The
last message will have &lt;tt class="docutils literal"&gt;&amp;quot;done&amp;quot;: true&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;We can send other kinds of requests to the model; for example, we can ask it
to calculate embeddings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl http://localhost:11434/api/embeddings -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;llama2&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;article about asteroids&amp;quot;
}&amp;#39; | jq
{
  &amp;quot;embedding&amp;quot;: [
    0.5615004897117615,
    -2.90958833694458,
    0.836567759513855,
    -0.3081018626689911,
    -1.1424092054367065,
    -1.5503573417663574,
    0.93345707654953,
    -3.008531093597412,
    3.6917684078216553,
    0.3383431136608124,
    1.0924581289291382,
    -2.1573197841644287,
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="programmatic-access-to-models-through-ollama"&gt;
&lt;h2&gt;Programmatic access to models through Ollama&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://github.com/jmorganca/ollama"&gt;Ollama README&lt;/a&gt; lists some ways to
interact with &lt;tt class="docutils literal"&gt;ollama&lt;/tt&gt; models programmatically; the most common way seems
to be through &lt;a class="reference external" href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt; and related tools.
LangChain is emerging as a common framework for interacting with LLMs; it has
high-level tools for chaining LLM-related tasks together, but also low-level
SDKs for each model's REST API.&lt;/p&gt;
&lt;p&gt;Here I will show how to talk to Ollama via the Go
port of LangChain - &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;LangChainGo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with a simple non-streaming completion request:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/ollama&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;llama2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateFromSinglePrompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Response:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For streaming, &lt;tt class="docutils literal"&gt;GenerateFromSinglePrompt&lt;/tt&gt; will take a streaming function as an
option. The streaming function is invoked with each chunk of data as it's
received; at the end, it's called with an empty chunk:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/ollama&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;llama2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateFromSinglePrompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithStreamingFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;chunk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;chunk len=%d: %s\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final completion is still returned from &lt;tt class="docutils literal"&gt;GenerateFromSinglePrompt&lt;/tt&gt;, in
case it's needed. Running this, we'll get something like the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ go run ollama-completion-stream.go
chunk len=1:

chunk len=5:  Sure
chunk len=1: !
chunk len=5:  Here
chunk len=1: &amp;#39;
chunk len=1: s
chunk len=2:  a
chunk len=6:  brief
chunk len=12:  explanation
[...]
chunk len=0:
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we can also obtain embeddings from the model using the &lt;tt class="docutils literal"&gt;langchain&lt;/tt&gt;
package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/ollama&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;llama2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;texts&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meteor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;comet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;puppy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;embs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;CreateEmbedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Got %d embeddings:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;embs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;embs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%d: len=%d; first few=%v\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for this post is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 2024-02-22&lt;/strong&gt;: See &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;a followup post on using additional models&lt;/a&gt;
like Google's Gemma with the same setup.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;ML models involve a huge amount of mathematical computations and
typically run best on beefy GPUs. If your machine (like mine!) doesn't
have a GPU installed, the model will still work on the CPU, but runs
very slowly.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Retrieval Augmented Generation in Go</title><link href="https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/" rel="alternate"></link><published>2023-11-10T09:02:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-11-10:/2023/retrieval-augmented-generation-in-go/</id><summary type="html">&lt;p&gt;I've been reading more and more about LLM-based applications recently, itching
to build something useful as a learning experience. In this post, I want to
share a Retrieval Augmented Generation (RAG) system I've built in 100% Go and
some insights I learned along the way.&lt;/p&gt;
&lt;div class="section" id="some-limitations-of-current-llms"&gt;
&lt;h2&gt;Some limitations of current LLMs …&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I've been reading more and more about LLM-based applications recently, itching
to build something useful as a learning experience. In this post, I want to
share a Retrieval Augmented Generation (RAG) system I've built in 100% Go and
some insights I learned along the way.&lt;/p&gt;
&lt;div class="section" id="some-limitations-of-current-llms"&gt;
&lt;h2&gt;Some limitations of current LLMs&lt;/h2&gt;
&lt;p&gt;Let's take OpenAI's API as an example; for your hard-earned dollars, it gives
you access to powerful LLMs and related models. These LLMs have some
limitations as a general knowledge system &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;They have a training cutoff date somewhere in the past; recently, OpenAI
moved the cutoff of their GPT models from 2021 to April 2023, but it's
still not real-time.&lt;/li&gt;
&lt;li&gt;Even if LLMs develop more real-time training, they still only have
access to public data. They aren't familiar with your internal documents,
which you may want to use them on.&lt;/li&gt;
&lt;li&gt;You pay per &lt;em&gt;token&lt;/em&gt;, which is
about 3/4 of a word; there can be different pricing for input tokens and
output tokens. The prices are low if you're only experimenting, but can grow
fast if you're working at scale. This may limit how many tokens you want an
LLM to crunch for you in each request.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="retrieval-augmented-generation"&gt;
&lt;h2&gt;Retrieval Augmented Generation&lt;/h2&gt;
&lt;p&gt;One of the most popular emerging techniques to address these limitations
is Retrieval Augmented Generation (RAG). Here's a useful diagram borrowed
from a &lt;a class="reference external" href="https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain"&gt;GCP blog post&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Diagram of RAG" class="align-center" src="https://eli.thegreenplace.net/images/2023/gcprag.png" /&gt;
&lt;p&gt;The idea is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We want the LLM to &amp;quot;ingest&amp;quot; a large body of text it wasn't trained on, and
then chat to it about it&lt;/li&gt;
&lt;li&gt;Even if the full body of text fits the LLM's context window, this may be
too expensive for each query &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Therefore, we'll run a separate information retrieval stage, finding
the most relevant information for our query&lt;/li&gt;
&lt;li&gt;Finally, we'll add this information as the context for our query and chat
with the LLM about it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The third step in the list above is the trickiest part - finding the most
&amp;quot;relevant&amp;quot; information is difficult in the general case. Are we supposed to
build a search engine? Well, that would be one approach! Powerful full-text
search engines exist and could be helpful here, but there's a better way
using embeddings. Read on to see how it works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementing-rag-in-go"&gt;
&lt;h2&gt;Implementing RAG in Go&lt;/h2&gt;
&lt;p&gt;In the course of my research on the subject, I wrote a bunch of Python code to
perform RAG, and then ported it to Go. It was easier to find Python samples
online, but once everything clicked in my head, porting to Go was trivial.&lt;/p&gt;
&lt;p&gt;This process led me to the following observation:&lt;/p&gt;
&lt;blockquote&gt;
LLM-based applications like RAG are a data-pipeline task, not
a machine-learning task.&lt;/blockquote&gt;
&lt;p&gt;What I mean by this is that the application doesn't crunch matrices, doesn't
explore the best loss function or gradient update, doesn't
train and evaluate models. It simply hooks up textual tools together; LLMs
are one such textual tool, embeddings are another. Therefore, Go is very well
suited for such applications! Go is much faster than Python, just as capable
with text processing, and its easy concurrency is helpful for applications that
spend a long time waiting for network I/O.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-motivating-problem"&gt;
&lt;h2&gt;The motivating problem&lt;/h2&gt;
&lt;p&gt;When I started hacking on this project, OpenAI models had a cutoff in 2021,
but it remains relevant today. I wanted the LLM to answer questions about the
Go documentation - including recently published pages. For example, Go shipped
forward compatibility and toolchain management &lt;a class="reference external" href="https://go.dev/blog/toolchain"&gt;in 1.21 this summer&lt;/a&gt;.
Asking ChatGPT about it yields sub-optimal results, because of the training
cutoff:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Me: what does GOTOOLCHAIN mean in go?

ChatGPT: In Go, the GOTOOLCHAIN environment variable is used to specify an
alternative toolchain to be used for compiling and linking Go code. This can
be useful when you need to use tools that are different from the default ones
provided with the Go installation, such as gccgo or other experimental
toolchains.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a classical LLM hallucination, because ChatGPT is not familiar with the
&lt;tt class="docutils literal"&gt;GOTOOLCHAIN&lt;/tt&gt; env var released in Go 1.21; it's a &lt;em&gt;plausible&lt;/em&gt; explanation
of what such an env var &lt;em&gt;could&lt;/em&gt; do in theory, but clearly not what I'm looking
for.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Me: what is the latest released version of the Go programming language?

ChatGPT: As of my last update in April 2023, the latest stable version of Go
was 1.20. Please check the official Go website or the Go GitHub repository for
the most current version.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To address this, I wanted to build a RAG system that:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Reads all the latest Go documentation pages (Markdown files straight from the Go
&lt;a class="reference external" href="https://github.com/golang/website"&gt;website repository&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;When a question is asked, finds the most relevant information from these
documentation pages&lt;/li&gt;
&lt;li&gt;This relevant information is added as context to the question via basic
prompt engineering, and the question + context is passed to ChatGPT via its
API&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get to it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="step-1-ingest-documentation"&gt;
&lt;h2&gt;Step 1: ingest documentation&lt;/h2&gt;
&lt;p&gt;Step 1 is trivial and doesn't involve any LLM-related technology. It's a
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai/cmd/chunker"&gt;command-line tool&lt;/a&gt; that recursively walks a locally cloned
&lt;tt class="docutils literal"&gt;_content&lt;/tt&gt; directory of the Go website's source code, reads each Markdown file
and splits it to &amp;quot;chunks&amp;quot; of approximately 1000 tokens &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;, consisting of
whole paragraphs. Each chunk is then stored in a SQLite DB with some additional
information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;TABLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;IF&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;NOT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;EXISTS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;chunks&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;INTEGER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;PRIMARY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;KEY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;AUTOINCREMENT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;nchunk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;INTEGER&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="step-2a-calculate-embedding-for-each-chunk"&gt;
&lt;h2&gt;Step 2a: calculate embedding for each chunk&lt;/h2&gt;
&lt;p&gt;Embeddings are a fascinating aspect of modern ML and LLMs. I won't
cover them in detail here - there are plenty of excellent resources online.
For our needs - an embedding model is a function that takes arbitrary text
and returns a fixed-size vector of real numbers that represents this text in
vector space (N-dimensional Cartesian coordinates). Related chunks of text will
be closer to each other in this space (using regular vector space distance
metrics) than unrelated chunks.&lt;/p&gt;
&lt;p&gt;For this step, &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai/cmd/rag"&gt;a command-line tool&lt;/a&gt;
with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--calculate&lt;/span&gt;&lt;/tt&gt; flag will read the DB table created in step 1,
calculate the embedding for each chunk using &lt;a class="reference external" href="https://platform.openai.com/docs/guides/embeddings"&gt;OpenAI's API&lt;/a&gt;, and store the embedding
for each chunk in a new table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;TABLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;IF&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;NOT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;EXISTS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;INTEGER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;PRIMARY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BLOB&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's easy to correlate each chunk's text to its embedding by joining the
tables on &lt;tt class="docutils literal"&gt;id&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;This step takes a bit of time and costs a bit of $$$, because it sends an
embedding API request to OpenAI for each chunk. The embeddings are stored
in the DB as a cache, so we shouldn't have to compute them more than once.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="step-2b-find-the-most-relevant-context-for-a-question"&gt;
&lt;h2&gt;Step 2b: find the most relevant context for a question&lt;/h2&gt;
&lt;p&gt;When a user asks a question - we can calculate the question's embedding, and
use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity"&gt;a bit of math&lt;/a&gt; to
find the most relevant (closest in vector space) chunks to it from our DB
collected in the previous step.&lt;/p&gt;
&lt;p&gt;This is configurable, but my demo program finds the 3 closest
chunks to the question (total of ~3K tokens).&lt;/p&gt;
&lt;p&gt;Theoretically, this step requires a bit of engineering sophistication to
efficiently find the closest chunks; a whole exciting domain of
infrastructure called &lt;em&gt;vector databases&lt;/em&gt; exists to address this need. Many of
them are implemented in Go, e.g. &lt;a class="reference external" href="https://github.com/milvus-io/milvus"&gt;milvus&lt;/a&gt;
or &lt;a class="reference external" href="https://github.com/weaviate/weaviate"&gt;weaviate&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since our sample is small, I simply do this by iterating over all the chunks
in the DB one by one, calculating the question's vector similarity with each one
and keep the 3 chunks with best similarity.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="step-3-prompt-engineering-to-answer-the-question-with-context"&gt;
&lt;h2&gt;Step 3: prompt engineering to answer the question with context&lt;/h2&gt;
&lt;p&gt;Now comes a small bit of prompt engineering, which is just a
fancy name for string interpolation :-) This is the literal Go code doing
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;`Use the below information to answer the subsequent question.&lt;/span&gt;
&lt;span class="s"&gt;Information:&lt;/span&gt;
&lt;span class="s"&gt;%v&lt;/span&gt;

&lt;span class="s"&gt;Question: %v`&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;contextInfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;theQuestion&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;tt class="docutils literal"&gt;contextInfo&lt;/tt&gt; is simply the 3 relevant chunks pasted together with
some newlines. That's it!&lt;/p&gt;
&lt;p&gt;Asking our question about &lt;tt class="docutils literal"&gt;GOTOOLCHAIN&lt;/tt&gt; again using this approach, we get
a much better answer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;In Go, `GOTOOLCHAIN` is an environment variable that determines which Go
toolchain version should be used. The value of `GOTOOLCHAIN` can be set to
various options:

1. `local`: This indicates that the bundled Go toolchain should be used.
2. `&amp;lt;name&amp;gt;`: This specifies a specific Go toolchain version that should
   be used. If a binary with that name is found in the system PATH, it will be
   used; otherwise, the `go` command will download and use the specified Go
   toolchain.
3. `&amp;lt;name&amp;gt;+auto` or `&amp;lt;name&amp;gt;+path`: This tells the `go` command to
   select and run a newer Go version as needed. It will consult the
   `toolchain` and `go` lines in the current workspace&amp;#39;s `go.work` file (or
   the main module&amp;#39;s `go.mod` file if there is no workspace) to determine
   which toolchain version to use. If a newer version is specified in those
   files, it will be used; otherwise, it will fallback to the default Go
   toolchain.

Overall, `GOTOOLCHAIN` is used to specify the specific Go toolchain version or
the selection process of a newer Go version that should be used by the `go`
command.
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code-and-final-words"&gt;
&lt;h2&gt;Code and final words&lt;/h2&gt;
&lt;p&gt;The full code for this project is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai"&gt;available on GitHub&lt;/a&gt;; all
you need to run it is your own &lt;tt class="docutils literal"&gt;OPENAI_API_KEY&lt;/tt&gt;. The repository includes the
SQLite DB with the embeddings already pre-populated, so you don't even need to
run the &lt;tt class="docutils literal"&gt;rag&lt;/tt&gt; tool with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--calculate&lt;/span&gt;&lt;/tt&gt;. See the &lt;tt class="docutils literal"&gt;README&lt;/tt&gt; file for full
instructions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (2024-01-03)&lt;/strong&gt;: the GitHub repository now includes a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;cmd/gemini-rag&lt;/span&gt;&lt;/tt&gt;
directory that reimplements this RAG tool using the
&lt;a class="reference external" href="https://ai.google.dev/"&gt;Google Gemini model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I'd like to thank &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison&lt;/a&gt;, whose lucid
writing on this subject has been very helpful in my research for this project.
Specifically, the following resources were invaluable:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://simonwillison.net/2023/Oct/23/embeddings/"&gt;Embeddings: What they are and why they matter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/"&gt;Making Large Language Models work for you&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/simonw/llm"&gt;llm command-line tool&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;LLMs have much more serious limitations, of course, w.r.t. factfulness
and hallucinations. This list is focused on the topic of our specific
example and isn't a general review of LLMs.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Let's take OpenAI's newly announced &lt;a class="reference external" href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday"&gt;GPT 4 Turbo&lt;/a&gt;, for example.
It has a whopping 128K token context window and costs 1 cent per 1K
tokens. If we use the full context for the input (ignoring for
the moment output tokens, which are more expensive), that's $1.28 per
query. Not for the faint of heart, if you want this to run at scale!&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Tokens are counted using the &lt;a class="reference external" href="https://github.com/pkoukk/tiktoken-go"&gt;Go port of OpenAI's tiktoken library&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Minimal character-based LSTM implementation</title><link href="https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/" rel="alternate"></link><published>2018-06-07T05:34:00-07:00</published><updated>2022-10-04T14:08:24-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2018-06-07:/2018/minimal-character-based-lstm-implementation/</id><summary type="html">&lt;p&gt;Following up on &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/"&gt;the earlier post&lt;/a&gt;
deciphering a minimal vanilla RNN implementation, here I'd like to extend the
example to a simple LSTM model.&lt;/p&gt;
&lt;p&gt;Once again, the idea is to combine a well-commented code sample
(&lt;a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-lstm.py"&gt;available here&lt;/a&gt;)
with some high-level diagrams and math to enable someone to
fully understand the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Following up on &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/"&gt;the earlier post&lt;/a&gt;
deciphering a minimal vanilla RNN implementation, here I'd like to extend the
example to a simple LSTM model.&lt;/p&gt;
&lt;p&gt;Once again, the idea is to combine a well-commented code sample
(&lt;a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-lstm.py"&gt;available here&lt;/a&gt;)
with some high-level diagrams and math to enable someone to
fully understand the code. The LSTM architecture presented herein is the
standard one originating from Hochreiter's and Schmidthuber's &lt;a class="reference external" href="https://www.google.com/search?q=lstm+hochreiter"&gt;1997 paper&lt;/a&gt;. It's described pretty much
everywhere; &lt;a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Chris Olah's post&lt;/a&gt; has particularly
nice diagrams and is worth reading.&lt;/p&gt;
&lt;div class="section" id="lstm-cell-structure"&gt;
&lt;h2&gt;LSTM cell structure&lt;/h2&gt;
&lt;p&gt;From 30,000 feet, LSTMs look just like regular RNNs; there's a &amp;quot;cell&amp;quot; that has a
recurrent connection (output tied to input), and when trained this cell is
usually unrolled to some fixed length.&lt;/p&gt;
&lt;p&gt;So we can take the basic RNN structure from the &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model"&gt;previous post&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Basic RNN diagram" class="align-center" src="https://eli.thegreenplace.net/images/2018/rnnbasic.png" /&gt;
&lt;p&gt;LSTMs are a bit trickier because there are two recurrent connections; these
can be &amp;quot;packed&amp;quot; into a single vector &lt;em&gt;h&lt;/em&gt;, so the above diagram still applies.
Here's how an LSTM cell looks inside:&lt;/p&gt;
&lt;img alt="LSTM cell" class="align-center" src="https://eli.thegreenplace.net/images/2018/lstm-cell.png" /&gt;
&lt;p&gt;&lt;em&gt;x&lt;/em&gt; is the input; &lt;em&gt;p&lt;/em&gt; is the probabilities computed from the output &lt;em&gt;y&lt;/em&gt; (these
symbols are named consistently with my earlier RNN post) and exit the cell at
the bottom purely due to topological convenience. The two memory vectors are &lt;em&gt;h&lt;/em&gt;
and &lt;em&gt;c&lt;/em&gt; - as mentioned earlier, they could be combined into a single vector, but
are shown here separately for clarity.&lt;/p&gt;
&lt;p&gt;The main idea of LSTMs is to enable training of longer sequences by providing
a &amp;quot;fast-path&amp;quot; to back-propagate information farther down in memory. Hence the
&lt;em&gt;c&lt;/em&gt; vector is not multiplied by any matrices on its path. The circle-in-circle
block means element-wise multiplication of two vectors; plus-in-square is
element-wise addition. The funny greek letter is the Sigmoid non-linearity:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8b0db8368e8a617143fa6566f42c1e47cd833c9c.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\sigma(x) =\frac{1}{1+e^{-x}}\]&lt;/object&gt;
&lt;p&gt;The only other block we haven't seen in the vanilla RNN diagram is the
colon-in-square in the bottom-left corner; this is simply the concatenation of
&lt;em&gt;h&lt;/em&gt; and &lt;em&gt;x&lt;/em&gt; into a single column vector. In addition, I've combined the
&amp;quot;multiply by matrix &lt;em&gt;W&lt;/em&gt;, then add bias &lt;em&gt;b&lt;/em&gt;&amp;quot; operation into a single rectantular
box to save on precious diagram space.&lt;/p&gt;
&lt;p&gt;Here are the equations computed by a cell:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c2cc966ba7ce8075317b87885bc9c432aafe2dba.svg" style="height: 249px;" type="image/svg+xml"&gt;\[\begin{align*}
xh&amp;amp;=x^{[t]}:h^{[t-1]}\\
f&amp;amp;=\sigma(W_f\cdot xh+b_f)\\
i&amp;amp;=\sigma(W_i\cdot xh+b_i)\\
o&amp;amp;=\sigma(W_o\cdot xh+b_o)\\
cc&amp;amp;=tanh(W_{cc}\cdot xh+b_{cc})\\
c^{[t]}&amp;amp;=c^{[t-1]}\odot f +cc\odot i\\
h^{[t]}&amp;amp;=tanh(c^{[t]})\odot o\\
y^{[t]}&amp;amp;=W_{y}\cdot h^{[t]}+b_y\\
p^{[t]}&amp;amp;=softmax(y^{[t]})\\
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="backpropagating-through-an-lstm-cell"&gt;
&lt;h2&gt;Backpropagating through an LSTM cell&lt;/h2&gt;
&lt;p&gt;This works &lt;em&gt;exactly&lt;/em&gt; like backprop through a vanilla RNN; we have to carefully
compute how the gradient flows through every node and make sure we properly
combine gradients at fork points. Most of the elements in the LSTM diagram are
familiar from the &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model"&gt;previous post&lt;/a&gt;.
Let's briefly work through the new ones.&lt;/p&gt;
&lt;p&gt;First, the Sigmoid function; it's an elementwise function, and computing its
derivative is very similar to the &lt;em&gt;tanh&lt;/em&gt; function discussed in the previous
post. As usual, given &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/e9ef6bd037537d5fe08743736acadccc09e70b06.svg" style="height: 18px;" type="image/svg+xml"&gt;f=\sigma(k)&lt;/object&gt;, from the chain rule we have the
following derivative w.r.t. some weight &lt;em&gt;w&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/57e3f2cab3c9b46a03d763a2f73b83963a1cd500.svg" style="height: 39px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial w}=\frac{\partial \sigma(k)}{\partial k}\frac{\partial k}{\partial w}\]&lt;/object&gt;
&lt;p&gt;To compute the derivative &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/8aa59f2f536b727cf97239b345ddcc98e41c2c91.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial \sigma(k)}{\partiak k}&lt;/object&gt;, we'll
use the ratio-derivative formula:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9e006cf5e9f1f8ccac82ba1f2bcdabd710731756.svg" style="height: 42px;" type="image/svg+xml"&gt;\[(\frac{f}{g})&amp;#x27;=\frac{f&amp;#x27;g-g&amp;#x27;f}{g^2}\]&lt;/object&gt;
&lt;p&gt;So:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e3f7af782f52215e8326b389271709a440993984.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\sigma &amp;#x27;(k)=\frac{e^{-k}}{(1+e^{-k})^2}\]&lt;/object&gt;
&lt;p&gt;A clever way to express this is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/eb1953be928287ff01ae23dfb4ff1cb2290854c9.svg" style="height: 20px;" type="image/svg+xml"&gt;\[\sigma &amp;#x27;(k)=\sigma(k)(1-\sigma(k))\]&lt;/object&gt;
&lt;p&gt;Going back to the chain rule with &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/e9ef6bd037537d5fe08743736acadccc09e70b06.svg" style="height: 18px;" type="image/svg+xml"&gt;f=\sigma(k)&lt;/object&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/885829ecab969c96daed7f0df6e5864339ad9d8b.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial w}=f(1-f)\frac{\partial k}{\partial w}\]&lt;/object&gt;
&lt;p&gt;The other new operation we'll have to find the derivative of is element-wise
multiplication. Let's say we have the column vectors &lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt; and &lt;em&gt;z&lt;/em&gt;, each with
&lt;em&gt;m&lt;/em&gt; rows, and we have &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/660b1e0dacc15aa3737b8170c3ecfdcbc6e77db4.svg" style="height: 18px;" type="image/svg+xml"&gt;z(x)=x\odot y&lt;/object&gt;. Since &lt;em&gt;z&lt;/em&gt; as a function of &lt;em&gt;x&lt;/em&gt;
has &lt;em&gt;m&lt;/em&gt; inputs and &lt;em&gt;m&lt;/em&gt; outputs, its Jacobian has dimensions [m,m].&lt;/p&gt;
&lt;p&gt;&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/0ab96cb4e5d8c6ba3ac8038fda07d518bbe1f388.svg" style="height: 18px;" type="image/svg+xml"&gt;D_{j}z_{i}&lt;/object&gt; is the derivative of the i-th element of &lt;em&gt;z&lt;/em&gt; w.r.t. the j-th
element of &lt;em&gt;x&lt;/em&gt;. For &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/660b1e0dacc15aa3737b8170c3ecfdcbc6e77db4.svg" style="height: 18px;" type="image/svg+xml"&gt;z(x)=x\odot y&lt;/object&gt; this is non-zero only
when &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; are equal, and in that case the derivative is &lt;img alt="y_i" class="valign-m4" src="https://eli.thegreenplace.net/images/math/35c2ac2f82d0ff8f9011b596ed7e54bfcc55f471.png" style="height: 12px;" /&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/e6631f3b13f877a8bb7b3b6a0c0d2ca110ecce23.svg" style="height: 18px;" type="image/svg+xml"&gt;Dz(x)&lt;/object&gt; is a square matrix with the elements of &lt;em&gt;y&lt;/em&gt; on the
diagonal and zeros elsewhere:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2450b2e2a827054f5d292822ff292eaa63c77d1b.svg" style="height: 97px;" type="image/svg+xml"&gt;\[Dz=\begin{bmatrix}
y_1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; y_2 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \ddots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; y_m \\
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;If we want to backprop some loss &lt;em&gt;L&lt;/em&gt; through this function, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/48b17da284ae52bc4b9fdeb7b98b73f398bd4458.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}Dz\]&lt;/object&gt;
&lt;p&gt;As &lt;em&gt;x&lt;/em&gt; has &lt;em&gt;m&lt;/em&gt; elements, the right-hand side of this equation multiplies a [1,m]
vector by a [m,m] matrix which is diagonal, resulting in element-wise multiplication
with the matrix's diagonal elements. In other words:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e2a6c0742fb006e35e3001d3b3d33f78316fb1e8.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}\odot y\]&lt;/object&gt;
&lt;p&gt;In code, it looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Assuming dz is the gradient of loss w.r.t. z; dz, y and dx are all&lt;/span&gt;
&lt;span class="c1"&gt;# column vectors.&lt;/span&gt;
&lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dz&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="model-quality"&gt;
&lt;h2&gt;Model quality&lt;/h2&gt;
&lt;p&gt;In the &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/"&gt;post about min-char-rnn&lt;/a&gt;,
we've seen that the vanilla RNN generates fairly low quality text:&lt;/p&gt;
&lt;blockquote&gt;
one, my dred, roriny. qued bamp gond hilves non froange saws, to mold
his a work, you shirs larcs anverver strepule thunboler
muste, thum and cormed sightourd
so was rewa her besee pilman&lt;/blockquote&gt;
&lt;p&gt;The LSTM's generated text quality is somewhat better when trained with roughtly
the same hyper-parameters:&lt;/p&gt;
&lt;blockquote&gt;
the she, over is was besiving the fact to seramed for i said over he
will round, such when a where, &amp;quot;i went of where stood it at eye heardul rrawed
only coside the showed had off with the refaurtoned&lt;/blockquote&gt;
&lt;p&gt;I'm fairly sure that it can be made to perform even better with larger memory
vectors and more training data. That said, an even more advanced architecture
can be helpful here. Moreover, since this is a &lt;em&gt;character&lt;/em&gt;-based model, to
really capture effects between words a few words apart we'll need a much deeper
LSTM (I'm unrolling to 16 characters we can only capture 2-3 words), and hence
much more training data and time.&lt;/p&gt;
&lt;p&gt;Once again, the goal here is not to develop a state-of-the-art language model,
but to show a simple, comprehensible example of how and LSTM is implemented
end-to-end in Python code. &lt;a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-lstm.py"&gt;The full code is here&lt;/a&gt;
- please let me know if you find any issues with it or something still remains
unclear.&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Machine Learning"></category></entry></feed>