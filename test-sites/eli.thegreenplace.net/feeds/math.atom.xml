<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eli Bendersky's website - Math</title><link href="https://eli.thegreenplace.net/" rel="alternate"></link><link href="https://eli.thegreenplace.net/feeds/math.atom.xml" rel="self"></link><id>https://eli.thegreenplace.net/</id><updated>2024-07-24T01:55:31-07:00</updated><entry><title>Notes on Taylor and Maclaurin series</title><link href="https://eli.thegreenplace.net/2024/notes-on-taylor-and-maclaurin-series/" rel="alternate"></link><published>2024-07-23T18:55:00-07:00</published><updated>2024-07-24T01:55:31-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-07-23:/2024/notes-on-taylor-and-maclaurin-series/</id><summary type="html">&lt;p&gt;A Maclaurin series is a power series - a polynomial with carefully selected
coefficients and an infinite number of terms - used to approximate arbitrary
functions with some conditions (e.g. differentiability). The Maclaurin series
does this for input values close to 0, and is a special case of the Taylor
series …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Maclaurin series is a power series - a polynomial with carefully selected
coefficients and an infinite number of terms - used to approximate arbitrary
functions with some conditions (e.g. differentiability). The Maclaurin series
does this for input values close to 0, and is a special case of the Taylor
series which can be used to find a polynomial approximation around any value.&lt;/p&gt;
&lt;div class="section" id="intuition"&gt;
&lt;h2&gt;Intuition&lt;/h2&gt;
&lt;p&gt;Let's say we have a function &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; and we want to approximate it with
some other - polynomial - function &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;. To make sure that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;
is as close as possible to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt;, we'll create a function that has
similar derivatives to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We start with a constant polynomial, such that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/012caf9ca6f5f20d23916c2628ebef524cefeed7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(0)=f(0)&lt;/object&gt;. This
approximation is perfect at 0 itself, but not as much elsewhere.&lt;/li&gt;
&lt;li&gt;We want &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt; to behave similarly to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; around 0, so we'll
set the derivative of our approximation to be the same as the derivative
of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at 0; in other words &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/210f8d24ca46f8afef927a302d65117bea405af9.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;(0)=f&amp;#x27;(0)&lt;/object&gt;. This approximation
will be decent &lt;em&gt;very&lt;/em&gt; close to 0 (at least in the direction of the slope),
but will become progressively worse as we get farther away from 0.&lt;/li&gt;
&lt;li&gt;We continue this process, by setting the second derivative to be
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/cf52e0957b8105e96f4a7d58c94212757571f4cf.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;&amp;#x27;(0)=f&amp;#x27;&amp;#x27;(0)&lt;/object&gt;, the third derivative to be &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/54583c50b4c7db87eb9546ee7bf42dbfddac2777.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;&amp;#x27;&amp;#x27;(0)=f&amp;#x27;&amp;#x27;&amp;#x27;(0)&lt;/object&gt;
and so on, for as many terms as we need to achieve a good approximation in
our desired range. Intuitively, if many derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt; are
identical to the corresponding derivatives of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at some point,
the two functions will have very similar behaviors around that point &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full Maclaurin series that accomplishes this approximation is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/766aa8a11f2b92dd363c0dab88fff5eb333165bd.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(0)+\frac{f&amp;#x27;(0)}{1!}x+\frac{f&amp;#x27;&amp;#x27;(0)}{2!}x^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}x^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n\]&lt;/object&gt;
&lt;p&gt;We'll get to how this equation is found in a moment, but first an example that
demonstrates its approximation capabilities. Suppose we want to find a polynomial
approximation for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a70fbea75540fef14e8ab2c910d8f9616c5e9f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x)=cos(x)&lt;/object&gt;. Following the definition of the Maclaurin
series, it's easy to calculate:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/359e30713a5e6accabbfb43c37c2798cd451df02.svg" style="height: 39px;" type="image/svg+xml"&gt;\[p_{cos}(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}-\cdots\]&lt;/object&gt;
&lt;p&gt;(try it as an exercise).&lt;/p&gt;
&lt;img alt="Successive approximation of cos(x) with Maclaurin series" class="align-center" src="https://eli.thegreenplace.net/images/2024/maclaurin-cos.png" /&gt;
&lt;p&gt;The dark blue line is the cosine function &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a70fbea75540fef14e8ab2c910d8f9616c5e9f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x)=cos(x)&lt;/object&gt;. The light blue
lines are successive approximations, with &lt;em&gt;k&lt;/em&gt; terms of the power series
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/c739a406062fd6fb971e3f322a30a0f603757e25.svg" style="height: 19px;" type="image/svg+xml"&gt;p_{cos}(x)&lt;/object&gt; included:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;With &lt;em&gt;k=1&lt;/em&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ec45f9d0f46db99de6b7864c09d9f22b462d11e6.svg" style="height: 19px;" type="image/svg+xml"&gt;p_{cos}(x)=1&lt;/object&gt; since that's just the value of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; at 0.&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;k=2&lt;/em&gt;, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/dee962f0d6f639c30a4c02d2063fa6679fff3120.svg" style="height: 25px;" type="image/svg+xml"&gt;p_{cos}(x)=1-\frac{x^2}{2}&lt;/object&gt;, and indeed the line looks parabolic&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;k=3&lt;/em&gt; we get a 4th degree polynomial which tracks the function better,
and so on&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With more terms in the power series, the approximation resembles
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; more and more, at least close to 0. The farther away we get from
0, the more terms we need for a good approximation &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-the-maclaurin-series-works"&gt;
&lt;h2&gt;How the Maclaurin series works&lt;/h2&gt;
&lt;p&gt;This section shows how one arrives at the formula for the Maclaurin series,
and connects it to the intuition of equating derivatives.&lt;/p&gt;
&lt;p&gt;We'll start by observing that the Maclaurin series is developed around 0 for
a good reason. The generalized form of a power series is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/17143ce86a9aa99908cc5ef88840f3a46a6e6216.svg" style="height: 22px;" type="image/svg+xml"&gt;\[p(x)=a_0+a_1 x+a_2 x^2 + a_3 x^3 + a_4 x^4 + \cdots\]&lt;/object&gt;
&lt;p&gt;To properly approximate a function, we need this series to &lt;em&gt;converge&lt;/em&gt;; therefore,
it would be desirable for its terms to decrease. An &lt;em&gt;x&lt;/em&gt; value close to zero
guarantees that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a297bb91c9703af1975402dded3ab9b7e6431dde.svg" style="height: 12px;" type="image/svg+xml"&gt;x^n&lt;/object&gt; becomes smaller and smaller with each successive
term. There's a whole section on convergence further down with more details.&lt;/p&gt;
&lt;p&gt;Recall from the &lt;em&gt;Intuition&lt;/em&gt; section that we're looking for a polynomial that
passes through the same point as &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at 0, and that has derivatives
equal to those of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at that point.&lt;/p&gt;
&lt;p&gt;Let's calculate a few of the first derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;; the function
itself can be considered as the 0-th derivative:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/305bdd6918c39e9b7fa95191f323ee811e744907.svg" style="height: 124px;" type="image/svg+xml"&gt;\[\begin{align*}
 p(x)&amp;amp;=a_0+a_1 x+a_2 x^2 + a_3 x^3+ a_4 x^4+\cdots\\
 p&amp;#x27;(x)&amp;amp;= a_1 +2 a_2 x + 3 a_3 x^2+4 a_4 x^3+\cdots\\
 p&amp;#x27;&amp;#x27;(x)&amp;amp;= 2 a_2 + 3 \cdot 2 a_3 x+ 4 \cdot 3 x^2+\cdots\\
 p&amp;#x27;&amp;#x27;&amp;#x27;(x)&amp;amp;= 3\cdot 2 a_3 + 4\cdot 3 \cdot 2 x+\cdots \\
 \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now, equate these to corresponding derivatives of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/8bdf21367eee06097384c37b0448375f07f950f0.svg" style="height: 12px;" type="image/svg+xml"&gt;x=0&lt;/object&gt;.
All the non-constant terms drop out, and we're left with:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/73ca5017e4342b3dc099f9895f78b58cb7eb486d.svg" style="height: 175px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(0)&amp;amp;=p(0)=a_0\\
 f&amp;#x27;(0)&amp;amp;=p&amp;#x27;(0)= a_1 \\
 f&amp;#x27;&amp;#x27;(0)&amp;amp;=p&amp;#x27;&amp;#x27;(0)= 2 a_2 \\
 f&amp;#x27;&amp;#x27;&amp;#x27;(0)&amp;amp;=p&amp;#x27;&amp;#x27;&amp;#x27;(0)= 3\cdot 2 a_3 \\
 \cdots\\
 f^{(n)}(0)&amp;amp;=p^{(0)}(0)=n!a_n\\
 \cdots\\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;So we can set the coefficients of the power series, generalizing the
denominators using factorials:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/37d903463af52a2e233564a101a6ee25cf5dd85c.svg" style="height: 228px;" type="image/svg+xml"&gt;\[\begin{align*}
 a_0 &amp;amp;= f(0)\\
 a_1 &amp;amp;= \frac{f&amp;#x27;(0)}{1!}\\
 a_2 &amp;amp;= \frac{f&amp;#x27;&amp;#x27;(0)}{2!}\\
 a_3 &amp;amp;= \frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}\\
 \cdots \\
 a_n &amp;amp;= \frac{f^{(n)}(0)}{n!}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Which gives us the definition of the Maclaurin series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/766aa8a11f2b92dd363c0dab88fff5eb333165bd.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(0)+\frac{f&amp;#x27;(0)}{1!}x+\frac{f&amp;#x27;&amp;#x27;(0)}{2!}x^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}x^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="taylor-series"&gt;
&lt;h2&gt;Taylor series&lt;/h2&gt;
&lt;p&gt;The Maclaurin series is suitable for finding approximations for functions
around 0; what if we want to approximate a function around a different value?
First, let's see why we would even want that. A couple of major reasons come
to mind:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We have a non-cyclic function and we're really interested in approximating
it around some specific value of &lt;em&gt;x&lt;/em&gt;; if we use Maclaurin series, we
get a good approximation around 0, but its quality is diminishing the
farther away we get. We may be able to use much fewer terms for a good
approximation if we start it around our target value.&lt;/li&gt;
&lt;li&gt;The function we're approximating is not well behaved around 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's the second reason which is most common, at least in calculus. By &amp;quot;not well
behaved&amp;quot; I mean a function that's not finite at 0 (or close to it), or that
isn't differentiable at that point, or whose derivatives aren't finite.&lt;/p&gt;
&lt;p&gt;There's a very simple and common example of such a function - the natural
logarithm &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt;. This function is undefined at 0 (it approaches
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/18787d835dea1ca698e365c252f82b506cecfce7.svg" style="height: 8px;" type="image/svg+xml"&gt;-\infty&lt;/object&gt;). Moreover, its derivatives are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1220eae393ca586b7f501bdd2f4887701cdcf9e9.svg" style="height: 223px;" type="image/svg+xml"&gt;\[\begin{align*}
 ln&amp;#x27;(x)&amp;amp;= \frac{1}{x}\\
 ln&amp;#x27;&amp;#x27;(x)&amp;amp;= -\frac{1}{x^2}\\
 ln&amp;#x27;&amp;#x27;&amp;#x27;(x)&amp;amp;= \frac{2}{x^3}\\
 ln^{(4)}(x)&amp;amp;= -\frac{6}{x^4}\\
 ln^{(5)}(x)&amp;amp;= \frac{24}{x^5}\\
 \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;&lt;em&gt;None&lt;/em&gt; of these is defined at 0 either! The Maclaurin series won't work here,
and we'll have to turn to its generalization - the Taylor series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d901c7af02dad6f8fefe97711dd287ccf8bb7bf4.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(a)+\frac{f&amp;#x27;(a)}{1!}(x-a)+\frac{f&amp;#x27;&amp;#x27;(a)}{2!}(x-a)^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(a)}{3!}(x-a)^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n\]&lt;/object&gt;
&lt;p&gt;This is a power series that provides an approximation for &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; around
any point &lt;em&gt;a&lt;/em&gt; where &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; is &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Smoothness"&gt;finite and differentiable&lt;/a&gt;. It's easy to use exactly the
same technique to develop this series as we did for Maclaurin.&lt;/p&gt;
&lt;p&gt;Let's use this to approximate &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; around &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt;, where this
function is well behaved. &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/597f6fdf6538b7be50426035591ea5ca5b157af3.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(1)=0&lt;/object&gt; and substituting &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt; into its
derivatives (as listed above) at this point, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a9abb6e999792d3ac77444b4c6e951881f1f016e.svg" style="height: 23px;" type="image/svg+xml"&gt;\[f&amp;#x27;(1)=1\quad f&amp;#x27;&amp;#x27;(1)=-1\quad f&amp;#x27;&amp;#x27;&amp;#x27;(1)=2\quad f^{(4)}(1)=-6\quad f^{(5)}(1)=24\]&lt;/object&gt;
&lt;p&gt;There's a pattern here: generally, the &lt;em&gt;n&lt;/em&gt;-th derivative at 1 is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f1528bf6eeacf1268c34b1983e1911719c83be8.svg" style="height: 19px;" type="image/svg+xml"&gt;(n-1)!&lt;/object&gt;
with an alternating sign. Substituting into the Taylor series equation from
above we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/047a288b1f4b33e86bb44c8efe2efe38841900fb.svg" style="height: 36px;" type="image/svg+xml"&gt;\[p_{ln}(x)=(x-1)-\frac{1}{2}(x-1)^2+\frac{1}{3}(x-1)^3-\frac{1}{4}(x-1)^4+\cdots\]&lt;/object&gt;
&lt;p&gt;Here's a plot of approximations with the first &lt;em&gt;k&lt;/em&gt; terms (the function itself
is dark blue, as before):&lt;/p&gt;
&lt;img alt="Successive approximation of ln(x) with Taylor series around a=1" class="align-center" src="https://eli.thegreenplace.net/images/2024/taylor-ln.png" /&gt;
&lt;p&gt;While the approximation looks good in the vicinity of 1, it seems like all
approximations diverge dramatically at some point.
The next section helps understand what's going on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convergence-of-power-series-and-the-ratio-test"&gt;
&lt;h2&gt;Convergence of power series and the ratio test&lt;/h2&gt;
&lt;p&gt;When approximating a function with power series (e.g. with Maclaurin or Taylor
series), a natural question to ask is: does the series actually converge to the
function it's approximating, and what are the conditions on this convergence?&lt;/p&gt;
&lt;p&gt;Now it's time to treat these questions a bit more rigorously. We'll be using
the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ratio_test"&gt;ratio test&lt;/a&gt; to check for
convergence. Generally, for a series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/13c57e0553761f7737331bbccfdc94b42bb038de.svg" style="height: 49px;" type="image/svg+xml"&gt;\[\sum_{n=1}^\infty a_n\]&lt;/object&gt;
&lt;p&gt;We'll administer this test:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/490f7edeea11652357aadca50c49c20606626f9c.svg" style="height: 44px;" type="image/svg+xml"&gt;\[L = \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\]&lt;/object&gt;
&lt;p&gt;And check the conditions for which &lt;object class="valign-m2" data="https://eli.thegreenplace.net/images/math/8ecda4d5752e9fa856c4cfc01e67e59c12960eeb.svg" style="height: 14px;" type="image/svg+xml"&gt;L &amp;lt; 1&lt;/object&gt;, meaning that our series
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Absolute_convergence"&gt;converges absolutely&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with our Maclaurin series for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d65e37d74a88b8c2f36e5cdf395111e243986b2c.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p_{cos}(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}-\cdots=1+\sum_{n=1}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}\]&lt;/object&gt;
&lt;p&gt;Ignoring the constant term, we'll write out the ratio limit. Note that because
of the absolute value, we can ignore the power-of-minus-one term too:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9ac968436c92d8abae1906d06c9d561e5b91279d.svg" style="height: 144px;" type="image/svg+xml"&gt;\[\begin{align*}
L &amp;amp;= \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{x^{2n+2} (2n)!}{(2n+2)! x^{2n}}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{x^2}{(2n+1)(2n+2)}\right|
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Since the limit contents are independent of &lt;em&gt;x&lt;/em&gt;, it's obvious that
that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/eb9fbe260c35041e881bc5c2d5a31041e22f8ca8.svg" style="height: 12px;" type="image/svg+xml"&gt;L=0&lt;/object&gt; for any &lt;em&gt;x&lt;/em&gt;. This means that the series converges to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt;
at any &lt;em&gt;x&lt;/em&gt;, given an infinite number of terms. This matches our intuition for
this function, which is well-behaved (smooth everywhere).&lt;/p&gt;
&lt;p&gt;Now on to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; with its Taylor series around &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt;. The
series is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/fcd9a0e50cfc2114bd0cbe3d9867d1e2a8b68740.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p_{ln}(x)=(x-1)-\frac{1}{2}(x-1)^2+\frac{1}{3}(x-1)^3-\frac{1}{4}(x-1)^4+\cdots=\sum_{n=1}^{\infty} \frac{(-1)^{n+1} (x-1)^n}{n}\]&lt;/object&gt;
&lt;p&gt;Once again, writing out the ratio limit:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c527a49c5271b5b9df3883ccb0491a77ca9eb6da.svg" style="height: 192px;" type="image/svg+xml"&gt;\[\begin{align*}
L &amp;amp;= \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{(x-1)^{n+1} n}{(n+1) (x-1)^n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{n(x-1)}{(n+1)}\right|\\
 &amp;amp;= \left|x-1\right| \lim_{n\to\infty}\left| \frac{n}{(n+1)}\right|=\left| x-1\right|
\end{align*}\]&lt;/object&gt;
&lt;p&gt;To converge, we require:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d741683ff7d9cd9515baf587944b501a778e860f.svg" style="height: 19px;" type="image/svg+xml"&gt;\[L=\left| x-1\right|&amp;lt;1\]&lt;/object&gt;
&lt;p&gt;The solution of this inequality is &lt;object class="valign-m2" data="https://eli.thegreenplace.net/images/math/0019851d0336bfbf91c4645cd3afab9eb4e3d29c.svg" style="height: 14px;" type="image/svg+xml"&gt;0 &amp;lt; x &amp;lt; 2&lt;/object&gt;. Therefore, the series
converges to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; only in this range of &lt;em&gt;x&lt;/em&gt;. This is also what we
observe in the latest plot. Another way to say it: the &lt;em&gt;radius of convergence&lt;/em&gt;
of the series around &lt;em&gt;x=1&lt;/em&gt; is 1.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If this explanation and the plot of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; following it don't
convince you, consider watching &lt;a class="reference external" href="https://www.youtube.com/watch?v=3d6DsjIBzJ4"&gt;this video by 3Blue1Brown&lt;/a&gt; - it
includes more visualizations as well as a compelling alternative intuition
using integrals and area.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;Note that since &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; is cyclic, all we really need is good
approximations in the range &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/0dd5a9ce1ef753379639c54a347fd611ca7a1937.svg" style="height: 19px;" type="image/svg+xml"&gt;[-\pi, \pi)&lt;/object&gt;. Our plot only shows the
positive &lt;em&gt;x&lt;/em&gt; axis; it looks like a mirror image on the negative side, so
we see that a pretty good approximation is achieved by the time we reach
&lt;em&gt;k=5&lt;/em&gt;.&lt;/p&gt;
&lt;p class="last"&gt;This is also a good place to
note that while Maclaurin series are important in Calculus, it's not the
&lt;em&gt;best&lt;/em&gt; approximation for numerical analysis purposes; there are
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Remez_algorithm"&gt;better approximations&lt;/a&gt;
that converge faster.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Projections and Projection Matrices</title><link href="https://eli.thegreenplace.net/2024/projections-and-projection-matrices/" rel="alternate"></link><published>2024-06-26T05:56:00-07:00</published><updated>2024-06-26T12:58:37-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-06-26:/2024/projections-and-projection-matrices/</id><summary type="html">&lt;p&gt;We'll start with a visual and intuitive representation of what a projection is.
In the following diagram, we have vector &lt;em&gt;b&lt;/em&gt; in the usual 3-dimensional space
and two possible projections - one onto the &lt;em&gt;z&lt;/em&gt; axis, and another onto the &lt;em&gt;x,y&lt;/em&gt;
plane.&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto axis and plane" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-3d.png" /&gt;
&lt;p&gt;If we think of 3D space as spanned …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We'll start with a visual and intuitive representation of what a projection is.
In the following diagram, we have vector &lt;em&gt;b&lt;/em&gt; in the usual 3-dimensional space
and two possible projections - one onto the &lt;em&gt;z&lt;/em&gt; axis, and another onto the &lt;em&gt;x,y&lt;/em&gt;
plane.&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto axis and plane" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-3d.png" /&gt;
&lt;p&gt;If we think of 3D space as spanned by the usual basis vectors, a projection
onto the &lt;em&gt;z&lt;/em&gt; axis is simply:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cb40026197c880dea45a56980b2c16a20248fddc.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_z=\begin{bmatrix}
0 \\
0 \\
z
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;A couple of intuitive ways to think about what a projection means:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The projection of &lt;em&gt;b&lt;/em&gt; on the &lt;em&gt;z&lt;/em&gt; axis is a vector in the direction of the
&lt;em&gt;z&lt;/em&gt; axis that's closest to &lt;em&gt;b&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The projection of &lt;em&gt;b&lt;/em&gt; on the &lt;em&gt;z&lt;/em&gt; axis is the shadow cast by &lt;em&gt;b&lt;/em&gt; when a flashlight
is pointed at it in the direction of the &lt;em&gt;z&lt;/em&gt; axis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We'll see a more formal definition soon. A projection onto the &lt;em&gt;x,y&lt;/em&gt; plane is
similarly easy to express.&lt;/p&gt;
&lt;div class="section" id="projection-onto-a-line"&gt;
&lt;h2&gt;Projection onto a line&lt;/h2&gt;
&lt;p&gt;Projecting onto an axis is easy - as the diagram shows, it's simply taking the
vector component in the direction of the axis. But how about projections onto
arbitrary lines?&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto another 3D vector" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-line.png" /&gt;
&lt;p&gt;In vector space, a line is just all possible scalings of some vector &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speaking more formally now, we're interested in the projection of
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, where the arrow over a letter means it's a
vector. The projection (which we call &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/507636722a388cbbc6ae26997a38a622bf9108ff.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{b_a}&lt;/object&gt;) is the
closest vector to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; in the direction of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;. In other
words, the dotted line in the diagram is at a right angle to the line &lt;em&gt;a&lt;/em&gt;;
therefore, the error vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; is orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;This orthogonality gives us the tools we need to find the projection. We'll want
to find a constant &lt;em&gt;c&lt;/em&gt; such that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8ecbc15ef33b2a9c390302e10702a9a2b0d93af9.svg" style="height: 20px;" type="image/svg+xml"&gt;\[\vec{b_a}=c\vec{a}\]&lt;/object&gt;
&lt;p&gt;&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; is orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, meaning that their dot
product is zero: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/95d0c31e1d2b9ede3ff6136e7d2f93975f66f266.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}\cdot\vec{a}=0&lt;/object&gt;. We'll use the distributive
property of the dot product in what follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6bb149c58b4741e8c2c06288a5325080c40b8a33.svg" style="height: 128px;" type="image/svg+xml"&gt;\[\begin{align*}
\vec{a}\cdot\vec{e}&amp;amp;=0 \\
\vec{a}\cdot(\vec{b}-c\vec{a})&amp;amp;=0\\
\vec{a}\cdot\vec{b}-c\vec{a}\cdot\vec{a}&amp;amp;=0\\
c&amp;amp;=\frac{\vec{a}\cdot\vec{b}}{\vec{a}\cdot\vec{a}}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Note that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7865ca07c8b7f891f073525c24dad20d9095cef5.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}\cdot\vec{a}&lt;/object&gt; is the squared &lt;em&gt;magnitude&lt;/em&gt; of
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;; for a unit vector this would be 1. This is why it doesn't
matter if &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; is a unit vector or not - we normalize it anyway.&lt;/p&gt;
&lt;p&gt;We have a formula for &lt;em&gt;c&lt;/em&gt; now - we can find it given &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; and
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;. To prepare for what comes next, however, we'll switch
notations. We'll use matrix notation, in which vectors are - by convention -
column vectors, and a dot product can be expressed by a matrix multiplication
between a row and a column vector. Therefore:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8c4bec17d5f0605dade7d1ad5c95344cbe857db0.svg" style="height: 86px;" type="image/svg+xml"&gt;\[\begin{align*}
c&amp;amp;=\frac{a^T b}{a^T a} \Rightarrow \\
b_a&amp;amp;=\frac{a^T b}{a^T a}a
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="projection-matrix"&gt;
&lt;h2&gt;Projection matrix&lt;/h2&gt;
&lt;p&gt;Since the fraction representing &lt;em&gt;c&lt;/em&gt; is a constant, we can switch the order of
the multiplication by &lt;em&gt;a&lt;/em&gt;, and then use the fact that matrix multiplication
is associative to write:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ef5a8af0ac0603aeba420410629435920eb636ab.svg" style="height: 86px;" type="image/svg+xml"&gt;\[\begin{align*}
b_a&amp;amp;=a\frac{a^T b}{a^T a}\\
b_a&amp;amp;=\frac{a a^T}{a^T a}b
\end{align*}\]&lt;/object&gt;
&lt;p&gt;In our case, since &lt;em&gt;a&lt;/em&gt; is a 3D vector, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0fb73bd46a79f79a74feb870a6f2773674cb4144.svg" style="height: 15px;" type="image/svg+xml"&gt;a a^T&lt;/object&gt; is a 3x3 matrix &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;, while
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a0f226ea3b10935f08820134b2eb4340a3c639e9.svg" style="height: 15px;" type="image/svg+xml"&gt;a^Ta&lt;/object&gt; is a scalar. Thus we get our
&lt;em&gt;projection matrix&lt;/em&gt; - call it &lt;em&gt;P&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b7e89f0b6600dac3697a45c4103066b6c6b16a0b.svg" style="height: 64px;" type="image/svg+xml"&gt;\[\begin{align*}
P&amp;amp;=\frac{a a^T}{a^T a}\\
b_a&amp;amp;=Pb
\end{align*}\]&lt;/object&gt;
&lt;p&gt;A recap: given some vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, we can construct a projection
matrix &lt;em&gt;P&lt;/em&gt;. This projection matrix can take any vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; and
help us calculate its projection onto &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; by means of a simple
matrix multiplication!&lt;/p&gt;
&lt;div class="section" id="example-of-line-projection"&gt;
&lt;h3&gt;Example of line projection&lt;/h3&gt;
&lt;p&gt;Consider our original example - projection on the &lt;em&gt;z&lt;/em&gt; axis. First, we'll
find a vector that spans the subspace represented by the &lt;em&gt;z&lt;/em&gt; axis: a trivial
vector is the unit vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e6d1b2131e20e1178d846e12f05db51d719bc20d.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_z=\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;What's the projection matrix corresponding to this vector?&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/dee6efc0fcfd585574f933869ceaf4af566cdffd.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P = \frac{a_z a_{z}^{T}}{1} = \begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}\begin{bmatrix}0&amp;amp;0&amp;amp;1\end{bmatrix}=\begin{bmatrix}
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now, given any arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; we can find its projection onto
the &lt;em&gt;z&lt;/em&gt; axis by multiplying with &lt;em&gt;P&lt;/em&gt;. For example:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/32259bb09674a6e6f361944736de956678c1ccbb.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=\begin{bmatrix}
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{bmatrix}\begin{bmatrix}
x\\
y\\
z
\end{bmatrix}=\begin{bmatrix}
0\\
0\\
z
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Another example - less trivial this time. Say we want to project vectors onto
the line spanned by the vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/45fa0fde893e5e0b211ecd84f96636732570073f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a=\begin{bmatrix}
1 \\
3 \\
7
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Let's compute the projection matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d583958c635852c142cf22a0d0b6441a9e5bf4c3.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P = \frac{a a^{T}}{a^T a} = \frac{1}{59}\begin{bmatrix}
1 \\
3 \\
7
\end{bmatrix}\begin{bmatrix}1&amp;amp;3&amp;amp;7\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
1&amp;amp;3&amp;amp;7\\
3&amp;amp;9&amp;amp;21\\
7&amp;amp;21&amp;amp;49
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now we'll use it to calculate the projection of
&lt;object class="valign-m7" data="https://eli.thegreenplace.net/images/math/3b696fddd0995dece7097f93105cb8c3f6095dd2.svg" style="height: 26px;" type="image/svg+xml"&gt;b=\begin{bmatrix}2 &amp;amp; 8 &amp;amp; -4\end{bmatrix}^T&lt;/object&gt; onto this line:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f5f7dc240fa88cae8e630cc2774167ccffeb9812.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=\frac{1}{59}\begin{bmatrix}
1&amp;amp;3&amp;amp;7\\
3&amp;amp;9&amp;amp;21\\
7&amp;amp;21&amp;amp;49
\end{bmatrix}\begin{bmatrix}
2\\
8\\
-4
\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
-2\\
-6\\
-14
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;To verify this makes sense, we can calculate the error vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8921404c3b6ce16571a4695866ee4b1d7c04a1fc.svg" style="height: 64px;" type="image/svg+xml"&gt;\[\begin{align*}
e&amp;amp;=b-b_a=\begin{bmatrix}
2\\
8\\
-4
\end{bmatrix}-\frac{1}{59}\begin{bmatrix}
-2\\
-6\\
-14
\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
120\\
478\\
-222
\end{bmatrix}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;And check that it's indeed orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f91db167e161815d41ba4a61228aed44d988e419.svg" style="height: 36px;" type="image/svg+xml"&gt;\[a\cdot e = \frac{1}{59}(1\cdot 120 + 3\cdot 478 + 7 \cdot -222)=0\]&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="projection-onto-a-vector-subspace"&gt;
&lt;h2&gt;Projection onto a vector subspace&lt;/h2&gt;
&lt;p&gt;A subspace of a vector space is a subset of vectors from the vector space that's
closed under vector addition and scalar multiplication. For
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;, some common subspaces include lines that go through the
origin and planes that go through the origin.&lt;/p&gt;
&lt;p&gt;Therefore, the &lt;em&gt;projection onto a line&lt;/em&gt; scenario we've discussed so far is just
a special case of a projection onto a subspace. We'll look at the general case
now.&lt;/p&gt;
&lt;p&gt;Suppose we have an m-dimensional vector space &lt;img alt="\mathbb{R}^m" class="valign-0" src="https://eli.thegreenplace.net/images/math/91d9290b46ace1360a8a715bd7a1fa701277697b.png" style="height: 12px;" /&gt;, and a set
of &lt;em&gt;n&lt;/em&gt; linearly independent vectors &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/51b0b13a9bd883c4eaebdf828454602a9639ae0f.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n} \in \mathbb{R}^m&lt;/object&gt;.
We want to find a combination of these vectors that's closest to some target
vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; - in other words, to find the projection of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;
onto the subspace spanned by &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Arbitrary m-dimensional vectors are difficult to visualize, but the derivation
here follows exactly the path we've taken for projections onto lines in 3D.
There, we were looking for a constant &lt;em&gt;c&lt;/em&gt; such that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/8d264117c06befcad931b56c0e6b39d7160971ec.svg" style="height: 13px;" type="image/svg+xml"&gt;c\vec{a}&lt;/object&gt; was the
closest vector to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;. Now, we're looking for a vector
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/34b58e8a800c7893cec26ee8be79f5713d2f75c9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{c}&lt;/object&gt; which represents a linear combination of &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;
that is closest to a target &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;If we organize &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt; as columns into a matrix called &lt;em&gt;A&lt;/em&gt;, we
can express this as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/edde67ce36d6642650ec7aa2fbcce40e1fca6286.svg" style="height: 20px;" type="image/svg+xml"&gt;\[\vec{b_a}=A\vec{c}\]&lt;/object&gt;
&lt;p&gt;This is a matrix multiplication: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/34b58e8a800c7893cec26ee8be79f5713d2f75c9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{c}&lt;/object&gt; is a list of coefficients
that describes some linear combination of the columns of &lt;em&gt;A&lt;/em&gt;. As before,
we want the error vector &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/7326fd9b29ba58374541d60332b01f85c1c23230.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{e}=\vec{b}-\vec{b_a}&lt;/object&gt; to be orthogonal to the
subspace onto which we're projecting: this means it's orthogonal to every
one of &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;.
The fact that vectors &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/537ed914f38fcd3604c0ba95c6d20d9e11e7e47c.svg" style="height: 16px;" type="image/svg+xml"&gt;\vec{a_n}&lt;/object&gt;
are orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; can be expressed as &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f53ea23a2082c5c678880ead067d369bf5747e98.svg" style="height: 88px;" type="image/svg+xml"&gt;\[\begin{align*}
a_{1}^{T}e&amp;amp;=0\\
\vdots\\
a_{n}^{T}e&amp;amp;=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;This is a system of linear equations, and thus it can be represented as a matrix
multiplication by a matrix with vectors &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/94740c57487dc021c3d8f42cd019025e09b9176f.svg" style="height: 20px;" type="image/svg+xml"&gt;a_{k}^T&lt;/object&gt; in its rows; this matrix
is just &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/e5ea66117060e4e5b2e83c1174d29dfc439d817a.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e05f35974fa739141ca1b4acc7d93882b0195cc2.svg" style="height: 17px;" type="image/svg+xml"&gt;\[A^T e=0\]&lt;/object&gt;
&lt;p&gt;But &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b07fe8d5a1561a36a0dd2ce8df88b750cc46fdc2.svg" style="height: 13px;" type="image/svg+xml"&gt;e=b-Ac&lt;/object&gt;, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/55229c55b419090aff119cfb1d680c2684324d90.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
A^T (b-Ac)&amp;amp;=0 \Rightarrow \\
A^Tb&amp;amp;=A^TAc
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Since the columns of &lt;em&gt;A&lt;/em&gt; are linearly independent, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt; is an
invertible matrix &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;, so we can isolate &lt;em&gt;c&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6cb5e5ca73be03d0c2bd7d1f2d6fb9d75eefc5b0.svg" style="height: 22px;" type="image/svg+xml"&gt;\[c=(A^T A)^{-1}A^T b\]&lt;/object&gt;
&lt;p&gt;Then the projection &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a839236c9cec6e8fda2ed32f30ae7eb6cb1a74a1.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec_{b_a}&lt;/object&gt; is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5bf80a966abd492b1f042e69562ca51751e30cd9.svg" style="height: 22px;" type="image/svg+xml"&gt;\[b_a=Ac=A(A^T A)^{-1}A^T b\]&lt;/object&gt;
&lt;p&gt;Similarly to the line example, we can also define a &lt;em&gt;projection matrix&lt;/em&gt;
as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cc188c53b2df3c538d277023816599128c49d17e.svg" style="height: 22px;" type="image/svg+xml"&gt;\[P=A(A^T A)^{-1}A^T\]&lt;/object&gt;
&lt;p&gt;Given a vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;, &lt;em&gt;P&lt;/em&gt; projects it onto the subspace spanned by
the vectors &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c95ad835eda46f7ee7b27dc6b6489a1730e481d8.svg" style="height: 14px;" type="image/svg+xml"&gt;\[b_a=Pb\]&lt;/object&gt;
&lt;p&gt;Let's make sure the dimensions work out. Recall that &lt;em&gt;A&lt;/em&gt; consists of &lt;em&gt;n&lt;/em&gt;
columns, each with &lt;em&gt;m&lt;/em&gt; rows. So we have:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3bfa44eebfc358b6b6dc73f7db4d270698168a07.svg" style="height: 129px;" type="image/svg+xml"&gt;\[\begin{matrix}
A &amp;amp; (m\times n) \\
A^T &amp;amp;  (n\times m)\\
A^T A &amp;amp; (n\times n)  \\
(A^T A)^{-1} &amp;amp; (n\times n) \\
A(A^T A)^{-1} &amp;amp; (m\times n) \\
A(A^T A)^{-1}A^T &amp;amp; (m\times m) \\
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;Since the vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; is m-dimensional, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/93f582c0800ee17439c9ba6b47d0cdf0ccf0c8f5.svg" style="height: 12px;" type="image/svg+xml"&gt;Pb&lt;/object&gt; is valid and the
result is another m-dimensional vector - the projection &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/03ebc36d8150942788540d021bc3c47fedf9a0c3.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{b}_a&lt;/object&gt;.&lt;/p&gt;
&lt;div class="section" id="example-of-subspace-projection"&gt;
&lt;h3&gt;Example of subspace projection&lt;/h3&gt;
&lt;p&gt;At the beginning of this post there's a diagram showing the projection of
an arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto a line and onto a
plane. We'll find the projection matrix for the plane case now. The projection
is onto the &lt;em&gt;xy&lt;/em&gt; plane, which is spanned by these vectors:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/610517a466bd35bd4c63c165b3d4870784ce2b6a.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_x=\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
a_y=\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Collecting them into a single matrix &lt;em&gt;A&lt;/em&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/67a17831ae3518fac78c604c82710fe6a5590c5f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[A=\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;To find &lt;em&gt;P&lt;/em&gt;, let's first calculate &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ec23a776b18efb2511bc583293d7d70cf549f258.svg" style="height: 64px;" type="image/svg+xml"&gt;\[A^T A=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}=
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;This happens to be the identity matrix, so its inverse is itself. Thus, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e618c6d4ea50cf468e938b555c0ea335aa7c5006.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P=A(A^T A)^{-1}A^T=AIA^T=AA^T=
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0
\end{bmatrix}=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;We can now project an arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto this plane by
multiplying it with this P:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c273b8d9daf567854bb34be9f8cf6c2fe091dcac.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}=
\begin{bmatrix}
x \\
y \\
0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Granted, this is a fairly trivial example - but it works in the general case. As
an exercise, pick a different pair of independent vectors and find the
projection matrix onto the plane spanned by them; then, verify that the
resulting error is orthogonal to the plane.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="properties-of-projection-matrices"&gt;
&lt;h2&gt;Properties of projection matrices&lt;/h2&gt;
&lt;p&gt;Projection matrices have some interesting properties that are educational to
review.&lt;/p&gt;
&lt;p&gt;First, projection matrices are &lt;strong&gt;symmetric&lt;/strong&gt;. To understand why, first recall
how a transpose of a matrix product is done:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ef0a9adc5755ad2121bd29acf8c1acae37069fe9.svg" style="height: 22px;" type="image/svg+xml"&gt;\[(AB)^T=B^T A^T\]&lt;/object&gt;
&lt;p&gt;As a warm-up, we can show that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt; is symmetric:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ed041fc5a1e961cb015cce0aa43bae716ff760ec.svg" style="height: 22px;" type="image/svg+xml"&gt;\[(A^T A)^T=A^T (A^T)^T=A^T A\]&lt;/object&gt;
&lt;p&gt;Now, let's transpose &lt;em&gt;P&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9891c74fb6f968ef5fd79fc46586671ae7398a0f.svg" style="height: 109px;" type="image/svg+xml"&gt;\[\begin{align*}
P&amp;amp;=A(A^T A)^{-1}A^T \\
P^T&amp;amp;=(A(A^T A)^{-1}A^T)^T\\
&amp;amp;=((A^T A)^{-1}A^T)^T A^T\\
&amp;amp;=A(A^T A)^{-1}A^T=P
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Here we've used the fact that the inverse of a symmetric matrix is also
symmetric, and we see that indeed &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/359c9df289ef29f45b04ffcbdedb39c5e1929bf5.svg" style="height: 15px;" type="image/svg+xml"&gt;P^T=P&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Second, projection matrices are &lt;strong&gt;idempotent&lt;/strong&gt;: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a1becab925d990bbb1bc40503523bc757abd4ad1.svg" style="height: 15px;" type="image/svg+xml"&gt;P^2=P&lt;/object&gt;; this isn't
hard to prove either:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ab435c0da6c2d7337c546531b022fc8ce34545b7.svg" style="height: 139px;" type="image/svg+xml"&gt;\[\begin{align*}
P^2&amp;amp;=A(A^T A)^{-1}A^T A(A^T A)^{-1}A^T\\
   &amp;amp;=A(A^T A)^{-1}(A^T A)(A^T A)^{-1}A^T\\
   &amp;amp;=A(A^T A)^{-1}[(A^T A)(A^T A)^{-1}]A^T\\
   &amp;amp;=A(A^T A)^{-1}IA^T\\
   &amp;amp;=A(A^T A)^{-1}A^T=P
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Intuitive explanation: think about what a projection does - given some
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;, it calculates
the closest vector to it in the desired subspace. If we
try to project this projection again - what will we get? Well, still the closest
vector in that subspace - itself! In other words:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f008b6de1991c31650bfddea89ff0b8aa55cef90.svg" style="height: 19px;" type="image/svg+xml"&gt;\[b_a=Pb=P(Pb)\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="projections-onto-orthogonal-subspaces"&gt;
&lt;h2&gt;Projections onto orthogonal subspaces&lt;/h2&gt;
&lt;p&gt;There's another special case of projections that is interesting to discuss:
projecting a vector onto orthogonal subspaces. We'll work through this using an
example.&lt;/p&gt;
&lt;p&gt;Consider the vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/148626e74e82c6b2d9e8c6cd31f482fbabeaead8.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_1=\begin{bmatrix}
1 \\
-2 \\
3
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;We'll find the projection matrix for this vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f6497d632c11809e51a21587f2a037eddc927946.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_1=\frac{a_1 a_{1}^T}{a_{1}^T a_1}=
\frac{1}{14}
\begin{bmatrix}
1 &amp;amp; -2 &amp;amp; 3\\
-2 &amp;amp; 4 &amp;amp; -6\\
3 &amp;amp; -6 &amp;amp; 9
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now, consider the following vector, which is orthogonal to &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b7524de3f703a23d5d20820341776bfe30276686.svg" style="height: 16px;" type="image/svg+xml"&gt;\vec{a_1}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b310d91fb1cb0978c5480837b024fe809885fd8c.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_2=\begin{bmatrix}
-3 \\
0 \\
1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Its projection matrix is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2363b2e0296d2c0f409d1a5c122622127e0f84e7.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_2=\frac{a_2 a_{2}^T}{a_{2}^T a_2}=
\frac{1}{10}
\begin{bmatrix}
9 &amp;amp; 0 &amp;amp; -3\\
0 &amp;amp; 0 &amp;amp; 0\\
-3 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;It's trivial to check that both &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/3aba26d01e9d3547d4804518e43131ca778dc418.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1&lt;/object&gt; and &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/97240d9f331df7b57f3d0766ffa6fba38a888857.svg" style="height: 15px;" type="image/svg+xml"&gt;P_2&lt;/object&gt; satisfy the
properties of projective matrices; what's more interesting is that
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/f32736e70c594bb14a4d8b9acb30d27d688689a0.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1 + P_2&lt;/object&gt; does as well - so it's also a proper projection matrix!&lt;/p&gt;
&lt;p&gt;To take it a step further, consider yet another vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b430bb9358af10100ffe7777868b175aed0c0298.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_3=\begin{bmatrix}
-1 \\
-5 \\
-3
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;The vectors &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/412ba4caae8a1f28842a91cc0c19e7db9d1150d7.svg" style="height: 19px;" type="image/svg+xml"&gt;(\vec{a_1},\vec{a_2},\vec{a_3})&lt;/object&gt; are all
mutually orthogonal, and thus form an orthogonal basis for &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;.
We can calculate &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/389d569332ecfb93594c5e799a493f37c74f4759.svg" style="height: 15px;" type="image/svg+xml"&gt;P_3&lt;/object&gt; in the usual way, and get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ea023279be309ce07524a111053220e4cf43138f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_3=\frac{a_3 a_{3}^T}{a_{3}^T a_3}=
\frac{1}{35}
\begin{bmatrix}
1 &amp;amp; 5 &amp;amp; 3\\
5 &amp;amp; 25 &amp;amp; 15\\
3 &amp;amp; 15 &amp;amp; 9
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Not only is &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/42161e8e714578e74e3c1d193afac51c51eb7cd2.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1+P_2+P_3&lt;/object&gt; is a projection matrix, it's a very familiar
matrix in general:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e2a2624bda06b15bfa1595a26f13a0d20d46848f.svg" style="height: 14px;" type="image/svg+xml"&gt;\[P_1+P_2+P_3=I\]&lt;/object&gt;
&lt;p&gt;This is equivalent to saying that for any vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3e8d17f216fa7fa3ed3f7f5e36b9412d2f9c24d2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[(P_1+P_2+P_3)b=b\]&lt;/object&gt;
&lt;p&gt;Hopefully this makes intuitive sense because it's just expressing
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; in an alternative basis for &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We're dealing
with vector spaces, where we don't really have lines - only vectors.
A line is just a visual way to think about certain subspaces of the
vector space &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;. Specifically, a line through the
origin (lines that don't go through the origin belong in
&lt;a class="reference external" href="https://eli.thegreenplace.net/2018/affine-transformations/"&gt;affine spaces&lt;/a&gt;)
is a way to represent &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/0dea7e472c25bda0904291774c1a1a5c72aa09d1.svg" style="height: 17px;" type="image/svg+xml"&gt;\forall c, c\vec{a}&lt;/object&gt; where &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;
is a vector in the same direction as this line and &lt;em&gt;c&lt;/em&gt; is a constant;
in other words it's the subspace of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt; spanned by
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;By the rules of matrix multiplication: we're multiplying a column vector
(a 3x1 matrix) by a row vector (a 1x3 matrix). The multiplication is
allowed because the inner dimensions match, and the result is a 3x3
matrix.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Recall from the earlier example: we're dropping the explicit vector
markings to be able to write matrix arithmetic more naturally. By
default vectors are column vectors, so &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/3ef5d107cf603c9f2987896f4a574a0967ad3059.svg" style="height: 15px;" type="image/svg+xml"&gt;v^T w&lt;/object&gt; expresses the
dot product between vectors &lt;img alt="\vec{v}" class="valign-0" src="https://eli.thegreenplace.net/images/math/39a3a59a8f524cf72620db07b9ba7cdce9fc9391.png" style="height: 13px;" /&gt; and &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d45128696127d3ae74860c6f8b14ce6ca20d15e7.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{w}&lt;/object&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;It's possible to prove this statement, but this post is already long
enough.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This is a special case of a &lt;a class="reference external" href="https://eli.thegreenplace.net/2015/change-of-basis-in-linear-algebra/"&gt;change of basis&lt;/a&gt;,
in which the basis is orthogonal.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Method of differences and Newton polynomials</title><link href="https://eli.thegreenplace.net/2024/method-of-differences-and-newton-polynomials/" rel="alternate"></link><published>2024-04-16T05:54:00-07:00</published><updated>2024-06-26T12:58:37-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-04-16:/2024/method-of-differences-and-newton-polynomials/</id><summary type="html">&lt;p&gt;I was reading about Babbage's &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Difference_engine"&gt;Difference engine&lt;/a&gt; the other
day, and stumbled upon a very interesting application of the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;forward differences&lt;/a&gt;
method.
It turns out that if we get a sequence generated by a polynomial, under certain
conditions we can find the generating polynomial from just a few elements in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was reading about Babbage's &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Difference_engine"&gt;Difference engine&lt;/a&gt; the other
day, and stumbled upon a very interesting application of the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;forward differences&lt;/a&gt;
method.
It turns out that if we get a sequence generated by a polynomial, under certain
conditions we can find the generating polynomial from just a few elements in
the sequence.&lt;/p&gt;
&lt;p&gt;For example, 0, 1, 5, 12, 22, 35, 51... is a sequence known as
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Pentagonal_number"&gt;the pentagonal numbers&lt;/a&gt;,
and we can use this technique to figure out that the polynomial
&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5b260a9d5bbd5288c720d246b76f459a34dc43b2.svg" style="height: 25px;" type="image/svg+xml"&gt;\frac{3n^2}{2}-\frac{n}{2}&lt;/object&gt; generates it &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="notation"&gt;
&lt;h2&gt;Notation&lt;/h2&gt;
&lt;p&gt;Let's start with some mathematical notation. We'll call the underlying function
generating the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt;. In our example, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e5a1616b377fb66cb2a07343c5d3cd1eb232f69f.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)=0&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a259694d0b5a8eb2ab2bdaf5233a65b6564b6971.svg" style="height: 19px;" type="image/svg+xml"&gt;f(1)=1&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/462967692d2cae32bb7c97fff3eba434f18fa36b.svg" style="height: 19px;" type="image/svg+xml"&gt;f(2)=5&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/28e07a49a51fb9429d839ceba23f0f210b7e227f.svg" style="height: 19px;" type="image/svg+xml"&gt;f(3)=12&lt;/object&gt; and so on.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The &lt;em&gt;first difference&lt;/em&gt; is the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/90c44106d8d77c32cb18afdcb981a76f7560e2e0.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)=f(1)-f(0)&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/304708f95d4d8aa49001cce85a05201cf46b7613.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(1)=f(2)-f(1)&lt;/object&gt;, etc.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;second difference&lt;/em&gt; is the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/165a28def741d96ae27b295d3afe0845566e4c41.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)=\Delta f(1) - \Delta f(0)&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e54a7d96a0cdf8a5507cf4242033555f5e9f4129.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(1)=\Delta f(2) - \Delta f(1)&lt;/object&gt; etc.&lt;/li&gt;
&lt;li&gt;In general, the k-th difference is: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1c79f2519166202a7529b72c5aecb083b285a8df.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta ^k f(n)=\Delta ^{k-1}f(n+1) - \Delta ^{k-1}f(n)&lt;/object&gt;.&lt;/li&gt;
&lt;li&gt;As a starting condition in the induction of differences, we can say that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; itself is the 0-th difference.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="difference-table"&gt;
&lt;h2&gt;Difference table&lt;/h2&gt;
&lt;p&gt;We can construct the &lt;em&gt;difference table&lt;/em&gt; for our sequence and observe its
properties. In a difference table, the first column is &lt;em&gt;n&lt;/em&gt;, which runs from 0
to whatever number of elements we have for the sequence. The second column is
the values of the sequence at these &lt;em&gt;n&lt;/em&gt;. Then come the first difference, the
second difference and so on. For our sample sequence we get the table:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ddf012f5699fff847f59b58ff97285c32fa72e99.svg" style="height: 148px;" type="image/svg+xml"&gt;\[\begin{matrix}
n &amp;amp; f(n) &amp;amp; \Delta f(n) &amp;amp; \Delta ^2 f(n)  &amp;amp;  \Delta ^3 f(n) \\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 3 &amp;amp; 0 \\
1 &amp;amp; 1 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
2 &amp;amp; 5 &amp;amp; 7 &amp;amp; 3 &amp;amp; 0 \\
3 &amp;amp; 12 &amp;amp; 10 &amp;amp; 3 &amp;amp; \\
4 &amp;amp; 22 &amp;amp; 13 &amp;amp;  &amp;amp;  \\
5 &amp;amp; 35 &amp;amp; &amp;amp; &amp;amp;
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;Notice how at some point the differences become all-zero! We'll soon see why.&lt;/p&gt;
&lt;p&gt;Obviously, we can construct such a table from only the column of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; -
that's what we just did! A more interesting observation is that if we accept
that all differences (columns) are 0 from a certain point, we can also construct
this table from just the first row! For example, with &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/233b23c15ba8894ccefa4845ae4b1954cc2c0ad7.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)&lt;/object&gt; and
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/607ab76bc5cd741a68f8fa4a934735835cda03c8.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)&lt;/object&gt; in hand, we know &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/3c70ae40a98b1cf4bb34daa3ba2573e31e4bfd34.svg" style="height: 19px;" type="image/svg+xml"&gt;f(1)&lt;/object&gt;; with &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/607ab76bc5cd741a68f8fa4a934735835cda03c8.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)&lt;/object&gt; and
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/673fc969775d75c8845c11bf81e042c20012dedf.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)&lt;/object&gt;, we know &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/90bb6aa06dd392c5af19397ce81dc975b17b5bba.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(1)&lt;/object&gt; etc. Try it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="inferring-the-polynomial-s-degree-from-the-table"&gt;
&lt;h2&gt;Inferring the polynomial's degree from the table&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; has degree &lt;em&gt;k&lt;/em&gt;, then the &lt;em&gt;k&lt;/em&gt;-th difference column
in the table is constant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; this is a general &lt;em&gt;k&lt;/em&gt;-th degree polynomial with coefficients
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/49c985cb16cfb154ce40e4fbc738712a6f649723.svg" style="height: 22px;" type="image/svg+xml"&gt;\[f(n)=a_k n^k + a_{k-1} n^{k-1} + \cdots + a_1 n + a_0\]&lt;/object&gt;
&lt;p&gt;By definition of the first difference, if we expand the polynomial form and
perform the subtraction per power of &lt;em&gt;n&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/84a9b56982e89d627d41a311f021d7dae2c4938a.svg" style="height: 87px;" type="image/svg+xml"&gt;\[\begin{align*}
  \Delta f(n) &amp;amp;= f(n+1)-f(n) = a_k (n+1)^{k} - a_k n^k + a_{k-1} (n+1)^{k-1} - a_{k-1} n^{k-1}+\cdots \\
              &amp;amp;= \sum_{j=0}^{k} a_j(n+1)^j-a_j n^j
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Using the binomial theorem we know that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/07156a3c7411c2da37b620fbb22a4b835c53fcfb.svg" style="height: 104px;" type="image/svg+xml"&gt;\[\begin{align*}
(n+1)^j &amp;amp;= \sum_{i=0}^{j} \binom{j}{i}n^{j-i} \cdot 1^{i} \\
        &amp;amp;= n^j + \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9160683e6cffa23bd374bf3005f43e65dbae8cce.svg" style="height: 43px;" type="image/svg+xml"&gt;\[(n+1)^j - n^j = \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots\]&lt;/object&gt;
&lt;p&gt;Now if we look at the sum we got for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; again, we'll notice
that in each term, the &lt;em&gt;j&lt;/em&gt;-th power of &lt;em&gt;n&lt;/em&gt; gets canceled out. This means that
the highest power of &lt;em&gt;n&lt;/em&gt; in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; is going to be &lt;em&gt;k-1&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can similarly show that in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt;, the highest power
of &lt;em&gt;n&lt;/em&gt; is going to be &lt;em&gt;k-2&lt;/em&gt;. Therefore, the &lt;em&gt;k&lt;/em&gt;-th difference
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; will be constant &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Two observations for extra credit:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Note that the claim goes one way - &lt;em&gt;if&lt;/em&gt; &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; is a &lt;em&gt;k&lt;/em&gt;-th degree
polynomial, the &lt;em&gt;k&lt;/em&gt;-th difference is constant. What we observe in the table
is the &lt;em&gt;k&lt;/em&gt;-th difference is constant, so can we infer that the function is
a &lt;em&gt;k&lt;/em&gt;-th degree polynomial? Not in the general case! The sequence could be
generated by some higher-degree polynomial, or by a completely different
kind of function. That said, since we assume &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; is polynomial and
are seeking to find the simplest (lowest degree) one, this inference is
valid.&lt;/li&gt;
&lt;li&gt;Did you notice the equivalence to derivatives of polynomials? The &lt;em&gt;k&lt;/em&gt;-th
difference of a polynomial of degree &lt;em&gt;k&lt;/em&gt; is constant... but the same is true
for the &lt;em&gt;k&lt;/em&gt;-th derivative! This is not by chance - since we have a discrete
domain, differences play largely the same role as derivatives for continuous
functions. This isn't a rigorous proof - but think about the definition of
derivatives (the one with the limit) - what do you get when you take
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6d56447973863053dfb94416852d0392187be5b6.svg" style="height: 13px;" type="image/svg+xml"&gt;\Delta x&lt;/object&gt; (also sometimes called &lt;em&gt;h&lt;/em&gt;) and set it to 1?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="finding-the-coefficients-with-the-newton-polynomial"&gt;
&lt;h2&gt;Finding the coefficients with the Newton polynomial&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Polynomial_interpolation"&gt;Polynomial interpolation&lt;/a&gt; can fit any N points
(with distinct &lt;em&gt;x&lt;/em&gt; values) with a N-1 degree polynomial. One way of finding such
a polynomial was discovered by Isaac Newton and is called the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Newton_polynomial"&gt;Newton polynomial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our problem of finding a polynomial that generates a given set of points can
be reduced to this interpolation problem, since we've just figured out the
degree of the generating polynomial! Looking at the difference table, we've
found when the difference becomes constant, and that gives us the polynomial's
degree &lt;em&gt;k&lt;/em&gt;. So all we need is the first &lt;em&gt;k+1&lt;/em&gt; points.&lt;/p&gt;
&lt;p&gt;Here's how to develop the Newton polynomial from scratch; we'll start with the
first few coefficients and will then generalize for any &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The Newton polynomial for our set of forward differences can be expressed as
follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8986e72a8615d60d63a271921b27ac86f2e8cbd7.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(n) = b_0 + b_1 n + b_2 n (n-1) + \cdots + b_k n(n-1)(n-2)\cdots (n-k+1)\]&lt;/object&gt;
&lt;p&gt;This polynomial is constructed in a clever way; notice that for any &lt;em&gt;p&lt;/em&gt;, when
we calculate &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/bcc84476cc6a5841b58dda8e57994f3c1f5e225e.svg" style="height: 19px;" type="image/svg+xml"&gt;f(p)&lt;/object&gt; all the elements starting with &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5e9438c006a97c292f46c16b8f25dde4990d3280.svg" style="height: 18px;" type="image/svg+xml"&gt;b_{p+1}&lt;/object&gt; will be
multiplied by zero and vanish. This helps us determine this polynomial's
coefficients in a gradual manner.&lt;/p&gt;
&lt;p&gt;We'll start with the point &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/96703e3d12513bb33306250114aadebf96461433.svg" style="height: 19px;" type="image/svg+xml"&gt;(0, f(0))&lt;/object&gt;, substituting it into the Newton
polynomial:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/7b9efff8279fe3ddb8aa308a1eedfb728080d9f2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(0) = b_0\]&lt;/object&gt;
&lt;p&gt;This gives us the first coefficient &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2a38c2c0c99a2e9da009fc149ea917276bbbd847.svg" style="height: 15px;" type="image/svg+xml"&gt;b_0&lt;/object&gt;. Next, let's look at
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/b303eb9d4aa35ebf9200e8672e6a87f1d2132c35.svg" style="height: 19px;" type="image/svg+xml"&gt;(1, f(1))&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/788b381cf3eac4de29eac423286ce810063e80de.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(1) = b_0 + b_1 \cdot 1 = b_0 + b_1\]&lt;/object&gt;
&lt;p&gt;Since we know that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6799ba33e6a6f88f516dbec5dac8da3208c87712.svg" style="height: 19px;" type="image/svg+xml"&gt;b_0=f(0)&lt;/object&gt;, we can infer that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ecffab1104bbfa1700790488d1592833fd3fc123.svg" style="height: 19px;" type="image/svg+xml"&gt;b_1=f(1)-f(0)&lt;/object&gt;.
Another way to express that is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4772e72ac67c63e4f75cde07fd0185c2581657d0.svg" style="height: 19px;" type="image/svg+xml"&gt;b_1=\Delta f(0)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Continuing to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/45af72369888dfa1a11fdc7346a5cc2d32001013.svg" style="height: 19px;" type="image/svg+xml"&gt;(2, f(2))&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/765c6057d2eacc86cc48329a5c11ba27d2899b65.svg" style="height: 72px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(2) &amp;amp;= b_0 + b_1 \cdot 2 + b_2 \cdot 2 \cdot 1 \\
      &amp;amp;= b_0 + 2 b_1 + 2! b_2 \\
      &amp;amp;= f(0) + 2 \Delta f(0) + 2! b_2
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've substituted the values of &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2a38c2c0c99a2e9da009fc149ea917276bbbd847.svg" style="height: 15px;" type="image/svg+xml"&gt;b_0&lt;/object&gt; and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c7cd24d955e66b8fe5ce45ded69fd98da5c68ba8.svg" style="height: 17px;" type="image/svg+xml"&gt;b_1&lt;/object&gt; that we've found
earlier; let's solve for &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/5c438c10871f01d09b0425b8ddd878e495bf7ff8.svg" style="height: 15px;" type="image/svg+xml"&gt;b_2&lt;/object&gt; now:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e4a3173cf23501f41b1900321533ef57e4cce580.svg" style="height: 170px;" type="image/svg+xml"&gt;\[\begin{align*}
 b_2 &amp;amp;= \frac{f(2) - f(0) - 2 \Delta f(0)}{2!} \\
     &amp;amp;= \frac{f(2) - f(1) + f(1) - f(0) - 2 \Delta f(1)}{2!} \\
     &amp;amp;= \frac{\Delta f(1) + \Delta f(0) - 2 \Delta f(0)}{2!} \\
     &amp;amp;= \frac{\Delta f(1) - \Delta f(0)}{2!}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The last line's numerator is - by definition - &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/673fc969775d75c8845c11bf81e042c20012dedf.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d388e0413c1a96596556e8c123aed73c45dd2e8d.svg" style="height: 39px;" type="image/svg+xml"&gt;\[b_2 = \frac{\Delta^2 f(0)}{2!}\]&lt;/object&gt;
&lt;p&gt;We can keep going with this (feel free to do &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1d56380c7fc945b51537de66595a5cbbbadd1d70.svg" style="height: 19px;" type="image/svg+xml"&gt;(3, f(3))&lt;/object&gt; as an exercise),
but the emerging generalization is that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9e565e1e9470e65eb5df737dfd57248b82e6b22d.svg" style="height: 39px;" type="image/svg+xml"&gt;\[b_i = \frac{\Delta^i f(0)}{i!}\]&lt;/object&gt;
&lt;p&gt;And a concise way to write Newton's polynomial for forward differences is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3136178faae047dc301b8c8e1f3f3bad4e43f1f9.svg" style="height: 53px;" type="image/svg+xml"&gt;\[f(n) = f(0) + \sum_{i=1}^{k} \frac{\Delta^i f(0)}{i!}g_i(n)\]&lt;/object&gt;
&lt;p&gt;Where &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/cfe177e3e161876e13bc7171b7f0314fdc73a651.svg" style="height: 19px;" type="image/svg+xml"&gt;g_i(n)&lt;/object&gt; is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/48bb7fa934e5b366190e384edb51b60b29693aaf.svg" style="height: 55px;" type="image/svg+xml"&gt;\[\prod_{j=0}^{i-1} (n-j)\]&lt;/object&gt;
&lt;p&gt;Note that we only use the differences for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/233b23c15ba8894ccefa4845ae4b1954cc2c0ad7.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)&lt;/object&gt;, meaning that we need
just the first row of the difference table! Let's try it for the pentagonal
numbers example.&lt;/p&gt;
&lt;p&gt;First, we've determined that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt; is a constant, so the degree
of the polynomial is 2. We only need to calculate until &lt;em&gt;i=2&lt;/em&gt; in the sum:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/79edb417f3a560fdb280f2c76f68aa0d32093210.svg" style="height: 39px;" type="image/svg+xml"&gt;\[f(n)=f(0)+\frac{\Delta f(0)}{1!} n + \frac{\Delta^2 f(0)}{2!} n(n-1)\]&lt;/object&gt;
&lt;p&gt;Substituting the values from the difference table we have for &lt;em&gt;n=0&lt;/em&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/25a6b43d36edb1d8a473a661e93eff0e74ce5370.svg" style="height: 127px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(n)&amp;amp;=0+n+\frac{3}{2}n(n-1) \\
     &amp;amp;=n + \frac{3n^2-3n}{2} \\
     &amp;amp;=\frac{3 n^2}{2} -\frac{n}{2}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Which is exactly what we expected!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="another-example"&gt;
&lt;h2&gt;Another example&lt;/h2&gt;
&lt;p&gt;Let's work through another example, taking the sequence -8, -12, -6, 16, 60...
We'll start by constructing the difference table:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c2924d0eb862dbb9fd08ff306c53d4429266ff78.svg" style="height: 126px;" type="image/svg+xml"&gt;\[\begin{matrix}
n &amp;amp; f(n) &amp;amp; \Delta f(n) &amp;amp; \Delta ^2 f(n)  &amp;amp;  \Delta ^3 f(n) \\
0 &amp;amp; -8 &amp;amp; -4 &amp;amp; 10 &amp;amp; 6 \\
1 &amp;amp; -12 &amp;amp; 6 &amp;amp; 16 &amp;amp; 6 \\
2 &amp;amp; -6 &amp;amp; 22 &amp;amp; 22 &amp;amp; \\
3 &amp;amp; 16 &amp;amp; 44 &amp;amp; &amp;amp; \\
4 &amp;amp; 60 &amp;amp;  &amp;amp;  &amp;amp;  \\
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;The difference &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a59fa130d9d0319438e151bf14b2f6536d5db631.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^3 f(n)&lt;/object&gt; appears to be constant, so we can generate
this sequence with a degree 3 polynomial. Let's use the first line of the table
to construct the Newton polynomial for it.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/39486c9596bdb41b9d4fbf24ab3c66674f187fd9.svg" style="height: 135px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(n)&amp;amp;=f(0)+\frac{\Delta f(0)}{1!} n + \frac{\Delta^2 f(0)}{2!} n(n-1) + \frac{\Delta^3 f(0)}{3!} n(n-1)(n-2)\\
     &amp;amp;= -8 + \frac{-4}{1}n + \frac{10}{2}n(n-1) + \frac{6}{6}n(n-1)(n-2) \\
     &amp;amp;= -8-4n+5n^2-5n+n^3-3n^2+2n \\
     &amp;amp;= n^3+2n^2-7n-8
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Verifying that this polynomial generates our sequence as its first 5 elements
is an easy exercise.&lt;/p&gt;
&lt;p&gt;Note that given 5 elements, we can always find a 4th-degree polynomial fitting
it. Here we found a 3rd-degree one, though, leveraging the technique of
differences. This becomes more acute if we have more elements in the sequence -
using differences it's often possible to find significantly simpler polynomials.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="recap"&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;For an integer sequence, if this sequence is generated by a polynomial we can
figure out which polynomial it is - given enough elements. We start by
constructing a difference table and noticing if a column becomes constant from
some point on. This tells us the degree of the generating polynomial. With that
in hand, we can use the Newton polynomial to discover a polynomial that
generates the sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="resources"&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://ztoz.blog/posts/method-differences/"&gt;This blog post&lt;/a&gt; was the
original inspiration. The post explains how
Babbage's difference engine worked, and makes off-hand remarks like &amp;quot;for
a 2nd degree polynomial, we only need up to the second difference to know
everything&amp;quot;. &lt;a class="reference external" href="https://ztoz.blog/posts/differences-applications/"&gt;This follow-up by the same author&lt;/a&gt;
mentioned going from sequences back to polynomials.&lt;/li&gt;
&lt;li&gt;The old &lt;a class="reference external" href="https://oeis.org/EIStext.pdf"&gt;Encyclopedia of integer sequences&lt;/a&gt;
has an intriguing section 2.5, which unfortunately presents several lemmas
with no proofs.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://brilliant.org/wiki/method-of-differences/"&gt;This wiki page from brilliant.org&lt;/a&gt;
is the best single resource, though its proof of constructing the Newton
polynomial is lacking, IMHO.&lt;/li&gt;
&lt;li&gt;Wikipedia:
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;Divided differences&lt;/a&gt;
and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Newton_polynomial"&gt;Newton polynomial&lt;/a&gt;
pages.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=xd7V0OKkEEg"&gt;Newton's forward differences YouTube lecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Knuth covers this &lt;em&gt;very briefly&lt;/em&gt; in section 4.6.4 of Part 2 of TAOCP,
relegating the derivation of the Newton polynomial to an exercise that has
a terse solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-another-way-to-find-the-polynomial-s-coefficients"&gt;
&lt;h2&gt;Appendix: Another way to find the polynomial's coefficients&lt;/h2&gt;
&lt;p&gt;Here's another way to discover the coefficients of the polynomial; the following
discusses the coefficients of the highest power, but can be generalized to
other powers as well. Using Newton's polynomial is simpler, though, so this
is just an appendix for some extra practice in manipulating such polynomials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; has degree &lt;em&gt;k&lt;/em&gt;, its coefficient &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt;
is equal to &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/1f8e42ade75eff67a62076b8124b881e652cc799.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\Delta^k f(n)}{k!}&lt;/object&gt;. Since we've proven that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; is a constant, we can find the precise
value of &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt; this way.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let's go back to the sum formulation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; from
the previous proof.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/17a09986b22f77c6241ff30054322e6cce33da9b.svg" style="height: 56px;" type="image/svg+xml"&gt;\[\sum_{j=0}^{k} a_j(n+1)^j-a_j n^j\]&lt;/object&gt;
&lt;p&gt;Expanding the binomial, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1f0d1ec102ab7eab83dff4cd8478847ce6ddd450.svg" style="height: 56px;" type="image/svg+xml"&gt;\[\sum_{j=0}^{k} a_j \left [ \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots \right ]\]&lt;/object&gt;
&lt;p&gt;The highest power of &lt;em&gt;n&lt;/em&gt; here is &lt;em&gt;k-1&lt;/em&gt;; its coefficient comes only from the
first term of the binomial expansion for &lt;em&gt;j=k&lt;/em&gt;, and is equal to &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/e192c5d30114375941e53a39e20ffb7ea5d3c971.svg" style="height: 15px;" type="image/svg+xml"&gt;k a_k&lt;/object&gt;.
In order not to deal with long sums, let's just focus on the highest-degree
term in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4890eedc2566f89862ec1917a6fdc4cacbc0ceab.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\Delta f(n) = k a_k n^{k-1} + \gamma\]&lt;/object&gt;
&lt;p&gt;Where &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/67833ee2012ec1c6254b6c009dc72bf0dc48aa6d.svg" style="height: 12px;" type="image/svg+xml"&gt;\gamma&lt;/object&gt; represents other elements with lower powers of &lt;em&gt;k&lt;/em&gt;, so we
don't care about them for this discussion &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. Let's move on to the next
difference:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c8fed37ba44785e2230649048abdc723c1f8cb3e.svg" style="height: 178px;" type="image/svg+xml"&gt;\[\begin{align*}
\Delta^2 f(n) &amp;amp;= \Delta f(n+1) - \Delta f(n) \\
 &amp;amp;= k a_k (n+1)^{k-1} + \gamma_1 - k a_k n^{k-1} + \gamma_2 \\
 &amp;amp;= k a_k \left [ n^{k-1} + \binom{k-1}{1} n^{k-2} + \cdots \right ] - k a_k n^{k-1} + \gamma \\
 &amp;amp;= k a_k \left [ \binom{k-1}{1} n^{k-2} + \cdots \right ] + \gamma \\
 &amp;amp;= k(k-1)a_k n^{k-2} + \gamma
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've just found the coefficient of the highest power of &lt;em&gt;n&lt;/em&gt; in
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt;.
It's clear that if we continue doing this, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; will be:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1e260e31360f67ec73b700fd95039aea2fae707b.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\Delta^k f(n)= a_k k(k-1)(k-2)\cdots 1 = a_k k!\]&lt;/object&gt;
&lt;p&gt;In other words, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/750c7571c53a8b53e71d1e645f7ae98060742ecc.svg" style="height: 26px;" type="image/svg+xml"&gt;a_k = \frac{\Delta^k f(n)}{k!}&lt;/object&gt;, meaning that we can know
the coefficient of the highest power of &lt;em&gt;n&lt;/em&gt; in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; from the &lt;em&gt;k&lt;/em&gt;-th
difference &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Naturally, there's an infinitude of potential functions that generate
this sequence; it's more precise to say we're looking for the simplest
(lowest degree)
polynomial that would do this.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In the following calculation, we're playing loose with the
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/67833ee2012ec1c6254b6c009dc72bf0dc48aa6d.svg" style="height: 12px;" type="image/svg+xml"&gt;\gamma&lt;/object&gt;s to represent &amp;quot;anything with lower powers of &lt;em&gt;n&lt;/em&gt;
that we don't care about&amp;quot;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Cubic spline interpolation</title><link href="https://eli.thegreenplace.net/2023/cubic-spline-interpolation/" rel="alternate"></link><published>2023-10-12T05:57:00-07:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-10-12:/2023/cubic-spline-interpolation/</id><summary type="html">&lt;p&gt;This post explains how cubic spline interpolation works, and presents a full
implementation in JavaScript, hooked up to a SVG-based visualization.
As a side effect, it also covers Gaussian elimination and presents a JavaScript
implementation of that as well.&lt;/p&gt;
&lt;p&gt;I love topics that mix math and programming in a meaningful …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post explains how cubic spline interpolation works, and presents a full
implementation in JavaScript, hooked up to a SVG-based visualization.
As a side effect, it also covers Gaussian elimination and presents a JavaScript
implementation of that as well.&lt;/p&gt;
&lt;p&gt;I love topics that mix math and programming in a meaningful way, and cubic
spline interpolation is an excellent example of such a topic. There's a bunch
of linear algebra here and some calculus, all connected with code to create
a useful tool.&lt;/p&gt;
&lt;div class="section" id="motivation"&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In an &lt;em&gt;interpolation&lt;/em&gt; problem, we're given a set of points (we'll be using
2D points &lt;em&gt;X,Y&lt;/em&gt; throughout this post) and are asked to estimate Y values for
Xs not in this original set, specifically for Xs that lie between Xs of the
original set (estimation for Xs outside the bounds of the original set
is called &lt;em&gt;extrapolation&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;As a concrete example, consider the set of points (0, 1), (1, 3), (2, 2); here
they are plotted in the usual coordinate system:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-3points.png" /&gt;
&lt;p&gt;Interpolation is estimating the value of Y for Xs between 0 and 2, given just
this data set. Obviously, the more complex the underlying function/phenomenon,
and the fewer original points we're given, interpolation becomes more difficult
to do accurately.&lt;/p&gt;
&lt;p&gt;There are many techniques to interpolate between a given set of points.
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Polynomial_interpolation"&gt;Polynomial interpolation&lt;/a&gt; can perfectly fit N
points with an N-1 degree polynomial, but this approach can be problematic for
large a N; high-degree polynomials tend to overfit their data, and suffer from
other numerical issues like &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Runge's_phenomenon"&gt;Runge's phenomenon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instead of interpolating all the points with a single function, a very popular
alternative is using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Spline_(mathematics)"&gt;Splines&lt;/a&gt;, which are piece-wise
polynomials. The idea is to fit a low-degree polynomial between every pair of
adjacent points in the original data set; for N points, we get N-1 different
polynomials. The simplest and best known variant of this technique is linear
interpolation:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with linear interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-linear.png" /&gt;
&lt;p&gt;Linear interpolation has clear benefits: it's very fast, and when N is large
it produces reasonable results. However, for small Ns the result isn't great,
and the approximation is very crude. Here's the linear spline interpolation of
the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sinc_function"&gt;Sinc function&lt;/a&gt; sampled
at 7 points:&lt;/p&gt;
&lt;img alt="Sinc function with linear interpolation" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-sinc-linear.png" /&gt;
&lt;p&gt;We can certainly do much better.&lt;/p&gt;
&lt;p&gt;How about higher-degree splines? We can try second degree polynomials, but it's
better to jump straight to cubic (third degree). Here's why: to make our
interpolation realistic and aesthetically pleasing, we want the neighboring
polynomials not only to touch at the original points (the linear splines already
do this), but to actually look like they're part of the same curve. For this
purpose, we want the &lt;em&gt;slope&lt;/em&gt; of the polynomials to be continuous, meaning that
if two polynomials meet at point P, their first derivatives at this point are
equal. Moreover, to ensure smoothness and to minimize needless bending &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;, we
also want the second derivatives of the two polynomials to be equal at P. The
lowest degree of polynomial that gives us this level of control is 3 (since the
second derivative of a quadratic polynomial is constant); hence cubic splines.&lt;/p&gt;
&lt;p&gt;Here's a cubic spline interpolating between the three points of the original
example:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-cubic.png" /&gt;
&lt;p&gt;And the &lt;em&gt;Sinc&lt;/em&gt; function:&lt;/p&gt;
&lt;img alt="Sinc function with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-sinc-cubic.png" /&gt;
&lt;p&gt;Because of the continuity of first and second derivatives, cubic splines look
very natural; on the other hand, since the degree of each polynomial remains
at most 3, they don't overfit too much. Hence they're such a popular tool for
interpolation and design/graphics.&lt;/p&gt;
&lt;p&gt;All the plots in this post have been produced by &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/js-gauss-spline"&gt;JavaScript code&lt;/a&gt;
that implements cubic spline interpolation from scratch. Let's move on to learn
how it works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="setting-up-equations-for-cubic-spline-interpolation"&gt;
&lt;h2&gt;Setting up equations for cubic spline interpolation&lt;/h2&gt;
&lt;p&gt;Given a set of N points, we want to produce N-1 cubic polynomials between these
points. While these are distinct polynomials, they are connected through mutual
constraints on the original points, as we'll see soon.&lt;/p&gt;
&lt;p&gt;More formally, we're going to define N-1 polynomials in the inclusive range
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9b9b2d7bb5787b4e4c6c897fdedd72630c5bb16a.svg" style="height: 19px;" type="image/svg+xml"&gt;i \in\{0 ...N-2\}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2ac0b93cd5715f7041d230d1d057cd4797bdb458.svg" style="height: 22px;" type="image/svg+xml"&gt;\[p_i(x)=a_ix^3+b_ix^2+c_ix+d_i\]&lt;/object&gt;
&lt;p&gt;For each polynomial, we have to find 4 coefficients: &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;d&lt;/em&gt;;
in total, for N-1 polynomials we'll need 4N-4 coefficients. We're going to
find these coefficients by expressing the constraints we have as linear
equations, and then solving a system of linear equations. We'll need 4N-4
equations to ensure we can find a unique solution for 4N-4 unknowns.&lt;/p&gt;
&lt;p&gt;Let's use our sample set of three original points to demonstrate how this
calculation works: (0, 1), (1, 3), (2, 2). Since N is 3, we'll be looking for
two polynomials and a total of 8 coefficients.&lt;/p&gt;
&lt;p&gt;The first set of constraints is obvious - each polynomial has to pass through
the two points it's interpolating between. The first polynomial passes through
the points (0, 1) and (1, 3), so we can write the equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/00090cc844b4a8b06c6ed084b0a814a86c13c148.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0(0)&amp;amp;=0a_0 + 0b_0 + 0c_0 + d_0=1\\
p_0(1)&amp;amp;=a_0+b_0+c_0+d_0=3
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;The second polynomial passes through the points (1, 3) and (2, 2), resulting
in the equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/20edf7b504d79094c9fdcabe836664bf10b42fc6.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
p_1(1)&amp;amp;=a_1+b_1+c_1+d_1=3\\
p_1(2)&amp;amp;=8a_1 + 4b_1 + 2c_1 + d_1=2
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;We have 4 equations, and need 4 more.&lt;/p&gt;
&lt;p&gt;We constrain the first and second derivatives of the polynomials to be equal at
the points where they meet. In our example, there are only two polynomials that
meet at a single point, so we'll get two equations: their derivatives are equal
at point (1, 3).&lt;/p&gt;
&lt;p&gt;Recall that the first and second derivatives of a cubic polynomial are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d26d85730cd1985330ab76d27cb48bb03a9278ae.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_i&amp;#x27;(x)&amp;amp;=3a_ix^2+2b_ix+c_i\\
p_i&amp;#x27;&amp;#x27;(x)&amp;amp;=6a_ix+2b_i
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;The equation we get from equating the first derivatives is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/49c0538e2adf8ce5aae6a0e1fcfb48e9d8ccb946.svg" style="height: 21px;" type="image/svg+xml"&gt;\[p_0&amp;#x27;(1)=3a_0+2b_0+c_0=p_1&amp;#x27;(1)=3a_1+2b_1+c_1\]&lt;/object&gt;
&lt;p&gt;Or, expressed as a linear equation of all coefficients:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d6d310b5eebfe69c6b3adc5f8fb5c33758077afe.svg" style="height: 14px;" type="image/svg+xml"&gt;\[3a_0+2b_0+c_0-3a_1-2b_1-c_1=0\]&lt;/object&gt;
&lt;p&gt;Similarly, the equation we get from equating the second derivatives is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a7803f84d767481ca8cc77b667b511592722d149.svg" style="height: 21px;" type="image/svg+xml"&gt;\[p_0&amp;#x27;&amp;#x27;(1)=6a_0+2=p_1&amp;#x27;&amp;#x27;(1)=6a_1+2\]&lt;/object&gt;
&lt;p&gt;Expressed as a linear equation of all coefficients:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1adf5f1b5b352afa02fab154f8f14488c580cfb9.svg" style="height: 14px;" type="image/svg+xml"&gt;\[6a_0+2-6a_1-2=0\]&lt;/object&gt;
&lt;p&gt;This brings us to a total of 6 equations. The last two equations will come from
&lt;em&gt;boundary conditions&lt;/em&gt;. Notice that - so far - we didn't say much about how our
interpolating polynomials behave at the end points, except that they pass
through them. Boundary conditions are constraints we create to define how our
polynomials behave at these end points.
There are several approaches to this,
but here we'll just discuss the most commonly-used one: a &lt;em&gt;natural&lt;/em&gt; spline.
Mathematically it says that the first polynomial has a second derivative of 0
at the first original point, and the last polynomial has a second derivative of
0 at the last original point. In our example:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/378d9e0421bc85fe06ade8ad93ffd620e47274db.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0&amp;#x27;&amp;#x27;(0)=0\\
p_1&amp;#x27;&amp;#x27;(2)=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Substituting the second derivative equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6cc066ad9122be577dacad8a36b8e7ae1ce387b3.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0&amp;#x27;&amp;#x27;(0)&amp;amp;=2b_0=0\\
p_1&amp;#x27;&amp;#x27;(2)&amp;amp;=12a_1+2b_1=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We have 8 equations now:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a9200a1d69c62aa7c22ce23b2d29d3f4f891f22b.svg" style="height: 203px;" type="image/svg+xml"&gt;\[\begin{align*}
d_0&amp;amp;=1\\
a_0+b_0+c_0+d_0&amp;amp;=3\\
a_1+b_1+c_1+d_1&amp;amp;=3\\
8a_1 + 4b_1 + 2c_1 + d_1&amp;amp;=2\\
3a_0+2b_0+c_0-3a_1-2b_1-c_1&amp;amp;=0\\
6a_0+2-6a_1-2&amp;amp;=0\\
2b_0&amp;amp;=0\\
12a_1+2b_1&amp;amp;=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;To restate the obvious - while our example only uses 2 polynomials, this
approach generalizes to any number. For N original points, we'll interpolate
with N-1 polynomials, resulting in 4N-4 coefficients. We'll get:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2N-2 equations from setting the points these polynomials pass through&lt;/li&gt;
&lt;li&gt;N-2 equations from equating first derivatives at internal points&lt;/li&gt;
&lt;li&gt;N-2 equations from equating second derivatives at internal points&lt;/li&gt;
&lt;li&gt;2 equations from boundary conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a total of 4N-4 equations.&lt;/p&gt;
&lt;p&gt;The code that constructs these equations from a given set of points is available
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2023/js-gauss-spline/spline.js"&gt;in this file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solving-the-equations"&gt;
&lt;h2&gt;Solving the equations&lt;/h2&gt;
&lt;p&gt;We now have 8 equations with 8 variables. Some of them are trivial, so it's
tempting to just solve the system by hand, and indeed one can do it very easily.
In the general case, however, it would be quite difficult - imagine
interpolating 10 polynomials resulting in 36 equations!&lt;/p&gt;
&lt;p&gt;Fortunately, the full power of linear algebra is now at our disposal. We can
express this set of linear equations as a matrix multiplication problem
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/e7d3683a610f89a991289fc2c2c64ba38eb6a004.svg" style="height: 13px;" type="image/svg+xml"&gt;Ax=b&lt;/object&gt;, where &lt;em&gt;A&lt;/em&gt; is a matrix of coefficients, &lt;em&gt;x&lt;/em&gt; is a vector of
unknowns and &lt;em&gt;b&lt;/em&gt; is the vector of right-hand side constants:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5243a06db4c849ae927dff870e2db1f3ab2c217d.svg" style="height: 170px;" type="image/svg+xml"&gt;\[Ax=b\Rightarrow \begin{pmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8 &amp;amp; 4 &amp;amp; 2 &amp;amp; 1\\
3 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; -3 &amp;amp; -2 &amp;amp; -1 &amp;amp; 0\\
6 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; -6 &amp;amp; -2 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 12 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\
\end{pmatrix}\begin{pmatrix}
a_0 \\
b_0 \\
c_0 \\
d_0 \\
a_1 \\
b_1 \\
c_1 \\
d_1\end{pmatrix}=\begin{pmatrix}
1\\
3\\
3\\
2\\
0\\
0\\
0\\
0
\end{pmatrix}\]&lt;/object&gt;
&lt;p&gt;Solving this system is straightforward using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination"&gt;Gaussian elimination&lt;/a&gt;.
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2023/js-gauss-spline/eqsolve.js"&gt;Our JavaScript implementation&lt;/a&gt;
does this in a few steps:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Performs Gaussian elimination to bring &lt;em&gt;A&lt;/em&gt; into row-echelon form, using the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination#Pseudocode"&gt;algorithm outlined on Wikipedia&lt;/a&gt;. This
approach tries to preserve numerical stability by selecting the row with the
largest (in absolute value) value for each column &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Further transforms the resulting matrix into &lt;em&gt;reduced&lt;/em&gt; row-echelon form
(a.k.a. Gauss-Jordan elimination)&lt;/li&gt;
&lt;li&gt;Extracts the solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, the solution ends up being the vector (-0.75, 0, 2.75, 1, 0.75,
-4.5, 7.25, -0.5); therefore, the interpolating polynomials are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/04c71afa0281eb99662ce3d99d4c4546ef08af89.svg" style="height: 50px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0(x)&amp;amp;=-0.75x^3+2.75x+1\\
p_1(x)&amp;amp;=0.75x^3-4.5x^2+7.25x-0.5
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="performing-the-interpolation-itself"&gt;
&lt;h2&gt;Performing the interpolation itself&lt;/h2&gt;
&lt;p&gt;Now that we have the interpolating polynomials, we can generate any number of
interpolated points. For all &lt;em&gt;x&lt;/em&gt; between 0 and 1 we use &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ad5cb52cf88277ad5a1880722c8ae8b3a6edfd42.svg" style="height: 19px;" type="image/svg+xml"&gt;p_0(x)&lt;/object&gt;,
and for &lt;em&gt;x&lt;/em&gt; between 1 and 2 we use &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7c2338e3575da884f060665a36a3503d970957a5.svg" style="height: 19px;" type="image/svg+xml"&gt;p_1(x)&lt;/object&gt;. In our JavaScript
code this is done by the &lt;tt class="docutils literal"&gt;doInterpolate&lt;/tt&gt; function. We've already seen
the result:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-cubic.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The complete code sample for this post &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/js-gauss-spline"&gt;is available on GitHub&lt;/a&gt;.
It includes functions for constructing equations for cubic splines from an
original set of points, code for solving linear equations with Gauss-Jordan
elimination, and a demo HTML page that plots the points and linear/spline
interpolations.&lt;/p&gt;
&lt;p&gt;The code is readable, heavily-commented JavaScript with no dependencies (except
D3 for the plotting).&lt;/p&gt;
&lt;p&gt;An additional demo that uses similar functionality is &lt;a class="reference external" href="https://eliben.github.io/line-plotting/"&gt;line-plotting&lt;/a&gt;; it plots arbitrary mathematical
functions with optional interpolation (when the number of sampled points is
low).&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This requirement actually has neat historical roots. In the days before
computers, &amp;quot;splines&amp;quot; were elastic rulers engineers and drafters would
use to interpolate between points by hand. These rulers would bend and
connect at the original points, and it was considered best practice to
minimize bending.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This helps avoid division by very small numbers, which may cause issues
when using finite-precision floating point.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Javascript"></category></entry><entry><title>My favorite prime number generator</title><link href="https://eli.thegreenplace.net/2023/my-favorite-prime-number-generator/" rel="alternate"></link><published>2023-08-22T20:01:00-07:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-08-22:/2023/my-favorite-prime-number-generator/</id><summary type="html">&lt;p&gt;Many years ago I've re-posted a &lt;a class="reference external" href="https://stackoverflow.com/a/568618/"&gt;Stack Overflow answer&lt;/a&gt; with Python code for a terse prime sieve
function that generates a potentially infinite sequence of prime
numbers (&amp;quot;potentially&amp;quot; because it &lt;em&gt;will&lt;/em&gt; run out of memory eventually). Since
then, I've used this code &lt;em&gt;many&lt;/em&gt; times - mostly because it's short and clear …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many years ago I've re-posted a &lt;a class="reference external" href="https://stackoverflow.com/a/568618/"&gt;Stack Overflow answer&lt;/a&gt; with Python code for a terse prime sieve
function that generates a potentially infinite sequence of prime
numbers (&amp;quot;potentially&amp;quot; because it &lt;em&gt;will&lt;/em&gt; run out of memory eventually). Since
then, I've used this code &lt;em&gt;many&lt;/em&gt; times - mostly because it's short and clear. In
this post I will explain how this code works, where it comes from (I didn't come
up with it), and some potential optimizations. If you want a teaser, here it is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate an infinite sequence of prime numbers.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-sieve-of-eratosthenes"&gt;
&lt;h2&gt;The sieve of Eratosthenes&lt;/h2&gt;
&lt;p&gt;To understand what this code does, we should first start with the basic Sieve
of Eratosthenes; if you're familiar with it, feel free to skip this section.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes"&gt;Sieve of Eratosthenes&lt;/a&gt; is a well-known
algorithm from ancient Greek times for finding all the primes below a certain
number reasonably efficiently using a tabular representation. This animation
from Wikipedia explains it pretty well:&lt;/p&gt;
&lt;img alt="Animated GIF of the Sieve of Eratosthenes in action" class="align-center" src="https://eli.thegreenplace.net/images/2023/eratosthenes-animation-wikipedia.gif" /&gt;
&lt;p&gt;Starting with the first prime (2) it marks all its multiples until the requested
limit. It then takes the next unmarked number, assumes it's a prime (because it
is not a multiple of a smaller prime), and marks &lt;em&gt;its&lt;/em&gt; multiples, and so on
until all the multiples below the limit are marked. The remaining
unmarked numbers are primes.&lt;/p&gt;
&lt;p&gt;Here's a well-commented, basic Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_upto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generates a sequence of primes &amp;lt; n.&lt;/span&gt;

&lt;span class="sd"&gt;    Uses the full sieve of Eratosthenes with O(n) memory.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="c1"&gt;# Initialize table; True means &amp;quot;prime&amp;quot;, initially assuming all numbers&lt;/span&gt;
    &lt;span class="c1"&gt;# are prime.&lt;/span&gt;
    &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="n"&gt;sqrtn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# Starting with 2, for each True (prime) number I in the table, mark all&lt;/span&gt;
    &lt;span class="c1"&gt;# its multiples as composite (starting with I*I, since earlier multiples&lt;/span&gt;
    &lt;span class="c1"&gt;# should have already been marked as multiples of smaller primes).&lt;/span&gt;
    &lt;span class="c1"&gt;# At the end of this process, the remaining True items in the table are&lt;/span&gt;
    &lt;span class="c1"&gt;# primes, and the False items are composites.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sqrtn&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

    &lt;span class="c1"&gt;# Yield all the primes in the table.&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we want a list of all the primes below some known limit,
&lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt; is great, and performs fairly well. There are two issues
with it, though:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We have to know what the limit is ahead of time; this isn't always possible
or convenient.&lt;/li&gt;
&lt;li&gt;Its memory usage is high - O(n); this can be significantly optimized,
however; see the bonus section at the end of the post for details.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="the-infinite-prime-generator"&gt;
&lt;h2&gt;The infinite prime generator&lt;/h2&gt;
&lt;p&gt;Back to the infinite prime generator that's in the focus of this post. Here is
its code again, now with some comments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate an infinite sequence of prime numbers.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Maps composites to primes witnessing their compositeness.&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="c1"&gt;# The running integer that&amp;#39;s checked for primeness&lt;/span&gt;
    &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# q is a new prime.&lt;/span&gt;
            &lt;span class="c1"&gt;# Yield it and mark its first multiple that isn&amp;#39;t&lt;/span&gt;
            &lt;span class="c1"&gt;# already marked in previous iterations&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# q is composite. D[q] holds some of the primes that&lt;/span&gt;
            &lt;span class="c1"&gt;# divide it. Since we&amp;#39;ve reached q, we no longer&lt;/span&gt;
            &lt;span class="c1"&gt;# need it in the map, but we&amp;#39;ll mark the next&lt;/span&gt;
            &lt;span class="c1"&gt;# multiples of its witnesses to prepare for larger&lt;/span&gt;
            &lt;span class="c1"&gt;# numbers&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key to the algorithm is the map &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;. It holds all the primes encountered
so far, but not as keys! Rather, they are stored as values, with the keys being
the next composite number they divide. This lets the program avoid having to
divide each number it encounters by all the primes known so far - it can simply
look in the map. A number that's not in the map is a new prime, and the way
the map updates is not unlike the sieve of Eratosthenes - when a composite is
removed, we add the &lt;em&gt;next&lt;/em&gt; composite multiple of the same prime(s). This is
guaranteed to cover all the composite numbers, while prime numbers should never
be keys in &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I highly recommend instrumenting this function with some printouts and running
through a sample invocation - it makes it easy to understand how the algorithm
makes progress.&lt;/p&gt;
&lt;p&gt;Compared to the full sieve &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;, this function doesn't require
us to know the limit ahead of time - it will keep producing prime numbers ad
infinitum (but will run out of memory eventually). As for memory usage, the
&lt;tt class="docutils literal"&gt;D&lt;/tt&gt; map has all the primes in it &lt;em&gt;somewhere&lt;/em&gt;, but each one appears only once.
So its size is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/d047b6d9c65037f42dcfda7db0732cf2163b8ee7.svg" style="height: 19px;" type="image/svg+xml"&gt;O(\pi(n))&lt;/object&gt;, where &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ac6df731942da5bd58234248a7aa9bd85b9100bd.svg" style="height: 19px;" type="image/svg+xml"&gt;\pi(n)&lt;/object&gt; is the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Prime-counting_function"&gt;Prime-counting function&lt;/a&gt;,
the number of primes smaller or equal to &lt;em&gt;n&lt;/em&gt;. This can be
approximated by &lt;object class="valign-m10" data="https://eli.thegreenplace.net/images/math/8ed6967b3dea41c3ce34ed6e0bd449b2adf5699a.svg" style="height: 24px;" type="image/svg+xml"&gt;O(\frac{n}{ln(n)})&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I don't remember where I first saw this approach mentioned, but all the
breadcrumbs lead to &lt;a class="reference external" href="https://code.activestate.com/recipes/117119-sieve-of-eratosthenes/"&gt;this ActiveState Recipe by David Eppstein&lt;/a&gt; from
way back in 2002.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizing-the-generator"&gt;
&lt;h2&gt;Optimizing the generator&lt;/h2&gt;
&lt;p&gt;I really like &lt;tt class="docutils literal"&gt;gen_primes&lt;/tt&gt;; it's short, easy to understand and gives me as
many primes as I need without forcing me to know what limit to use, and its
memory usage is much more reasonable than the full-blown sieve of Eratosthenes.
It is, however, also quite slow, over 5x slower than &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The aforementioned ActiveState Recipe thread has several optimization ideas;
here's a version that incorporates ideas from Alex Martelli, Tim Hochberg and
Wolfgang Beneicke:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_opt&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;  &lt;span class="c1"&gt;# get odd multiples&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The optimizations are:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Instead of holding a list as the value of &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;, just have a single number.
In cases where we need more than one witness to a composite, find the next
multiple of the witness and assign that instead (this is the &lt;tt class="docutils literal"&gt;while x in D&lt;/tt&gt;
inner loop in the &lt;tt class="docutils literal"&gt;else&lt;/tt&gt; clause). This is a bit like using linear probing
in a hash table instead of having a list per bucket.&lt;/li&gt;
&lt;li&gt;Skip even numbers by starting with 2 and then proceeding from 3 in steps
of 2.&lt;/li&gt;
&lt;li&gt;The loop assigning the next multiple of witnesses may land on even numbers
(when &lt;tt class="docutils literal"&gt;p&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;q&lt;/tt&gt; are both odd). So instead jump to &lt;tt class="docutils literal"&gt;q + p + p&lt;/tt&gt;
directly, which is guaranteed to be odd.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these in place, the function is more than 3x faster than before, and is
now only within 40% or so of &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;, while remaining short and
reasonably clear.&lt;/p&gt;
&lt;p&gt;There are even fancier algorithms that use interesting mathematical tricks to do
less work. Here's &lt;a class="reference external" href="https://stackoverflow.com/a/19391111/"&gt;an approach by Will Ness and Tim Peters&lt;/a&gt; (yes, &lt;em&gt;that&lt;/em&gt; Tim Peters) that's
reportedly faster. It uses the &lt;em&gt;wheels&lt;/em&gt; idea from &lt;a class="reference external" href="https://research.cs.wisc.edu/techreports/1990/TR909.pdf"&gt;this paper by Sorenson&lt;/a&gt;. Some additional
details on this approach are available &lt;a class="reference external" href="https://stackoverflow.com/a/30563958"&gt;here&lt;/a&gt;. This algorithm is both faster and
consumes less memory; on the other hand, it's no longer short and simple.&lt;/p&gt;
&lt;p&gt;To be honest, it always feels a bit odd to me to painfully optimize Python code,
when switching languages provides vastly bigger benefits. For example, I threw
together the same algorithms &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/prime-sieve/go-with-range-over-func"&gt;using Go&lt;/a&gt;
and its &lt;a class="reference external" href="https://github.com/golang/go/issues/61405"&gt;experimental iterator support&lt;/a&gt;; it's 3x faster than the
Python version, with very little effort (even though the new Go iterators and
&lt;tt class="docutils literal"&gt;yield&lt;/tt&gt; functions are still in the proposal stage and aren't optimized). I
can't try to rewrite it in C++ or Rust for now, due to the lack of generator
support; the &lt;tt class="docutils literal"&gt;yield&lt;/tt&gt; statement is what makes this code so nice and elegant,
and alternative idioms are much less convenient.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bonus-segmented-sieve-of-eratosthenes"&gt;
&lt;h2&gt;Bonus: segmented sieve of Eratosthenes&lt;/h2&gt;
&lt;p&gt;The Wikipedia article on the sieve of Eratosthenes mentions a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes#Segmented_sieve"&gt;segmented
approach&lt;/a&gt;, which
is also described in the &lt;a class="reference external" href="https://research.cs.wisc.edu/techreports/1990/TR909.pdf"&gt;Sorenson paper&lt;/a&gt; in section 5.&lt;/p&gt;
&lt;p&gt;The main insight is that we only need the primes up to &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/712f9a224d6c7824add37b6cd766c21f73a40d59.svg" style="height: 18px;" type="image/svg+xml"&gt;\sqrt{n}&lt;/object&gt; to
be able to sieve a table all the way to N. This results in a sieve that uses
only &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/5a41da22acdba46e7c8eeeaddbc58625f49cbfe5.svg" style="height: 19px;" type="image/svg+xml"&gt;O(\sqrt{n})&lt;/object&gt; memory. Here's a commented Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_upto_segmented&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generates a sequence of primes &amp;lt; n.&lt;/span&gt;

&lt;span class="sd"&gt;    Uses the segmented sieve or Eratosthenes algorithm with O(√n) memory.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Simplify boundary cases by hard-coding some small primes.&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="c1"&gt;# We break the range [0..n) into segments of size √n&lt;/span&gt;
    &lt;span class="n"&gt;segsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# Find the primes in the first segment by calling the basic sieve on that&lt;/span&gt;
    &lt;span class="c1"&gt;# segment (its memory usage will be O(√n)). We&amp;#39;ll use these primes to&lt;/span&gt;
    &lt;span class="c1"&gt;# sieve all subsequent segments.&lt;/span&gt;
    &lt;span class="n"&gt;baseprimes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gen_primes_upto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;baseprimes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Create a new table of size √n for each segment; the old table&lt;/span&gt;
        &lt;span class="c1"&gt;# is thrown away, so the total memory use here is √n&lt;/span&gt;
        &lt;span class="c1"&gt;# seg[i] represents the number segstart+i&lt;/span&gt;
        &lt;span class="n"&gt;seg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;baseprimes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# The first multiple of bp in this segment can be calculated using&lt;/span&gt;
            &lt;span class="c1"&gt;# modulo.&lt;/span&gt;
            &lt;span class="n"&gt;first_multiple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# Mark all multiples of bp in the segment as composite.&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_multiple&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

        &lt;span class="c1"&gt;# Sieving is done; yield all composites in the segment (iterating only&lt;/span&gt;
        &lt;span class="c1"&gt;# over the odd ones).&lt;/span&gt;
        &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;break&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for this post - along with tests and benchmarks - is available
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/prime-sieve"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;While this is a strong improvement over &lt;tt class="docutils literal"&gt;O(n)&lt;/tt&gt; (e.g. for a billion
primes, memory usage here is only 5% of the full sieve version), it
still depends on the size of the input. In the unlikely event that you
need to generate truly gigantic primes starting from 2, even the
square-root-space solutions become infeasible. In this case, the whole
approach should be changed; instead, one would just generate random huge
numbers and use probabilistic primality testing to check for their
primeness. This is what real libraries like Go's &lt;a class="reference external" href="https://pkg.go.dev/crypto/rand#Prime"&gt;crypto/rand.Prime&lt;/a&gt;
do.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Python"></category><category term="Go"></category></entry><entry><title>Demystifying Tupper's formula</title><link href="https://eli.thegreenplace.net/2023/demystifying-tuppers-formula/" rel="alternate"></link><published>2023-05-22T19:45:00-07:00</published><updated>2023-06-03T22:14:53-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-05-22:/2023/demystifying-tuppers-formula/</id><summary type="html">&lt;p&gt;A &lt;a class="reference external" href="https://makeanddo4d.com/"&gt;book I was recently reading&lt;/a&gt; mentioned a
mathematical curiosity I haven't seen before - &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tupper%27s_self-referential_formula"&gt;Tupper's self-referential
formula&lt;/a&gt;.
There are some resources about it online, but this post is &lt;em&gt;my&lt;/em&gt; attempt to
explain how it works - along with an interactive implementation you can try
in the browser.&lt;/p&gt;
&lt;div class="section" id="tupper-s-formula"&gt;
&lt;h2&gt;Tupper's formula&lt;/h2&gt;
&lt;p&gt;Here is …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;A &lt;a class="reference external" href="https://makeanddo4d.com/"&gt;book I was recently reading&lt;/a&gt; mentioned a
mathematical curiosity I haven't seen before - &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tupper%27s_self-referential_formula"&gt;Tupper's self-referential
formula&lt;/a&gt;.
There are some resources about it online, but this post is &lt;em&gt;my&lt;/em&gt; attempt to
explain how it works - along with an interactive implementation you can try
in the browser.&lt;/p&gt;
&lt;div class="section" id="tupper-s-formula"&gt;
&lt;h2&gt;Tupper's formula&lt;/h2&gt;
&lt;p&gt;Here is the formula:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a4f48d2debe2ad234574a9cf2a0c2d1b327963c7.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left \lfloor \frac{y}{17}\right \rfloor 2^{-17\lfloor x \rfloor - mod(\lfloor y \rfloor, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;We want to plot this formula, but how?&lt;/p&gt;
&lt;p&gt;For this purpose, it's more useful to think of Tupper's formula not as a
function but as a &lt;em&gt;relation&lt;/em&gt;, in the mathematical sense. In Tupper's paper
this is a relation on &lt;img alt="\mathbb{R}" class="valign-0" src="https://eli.thegreenplace.net/images/math/0ed839b111fe0e3ca2b2f618b940893eaea88a57.png" style="height: 12px;" /&gt;, meaning that it's a set of pairs
in &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6d731263787f024f927178eb8fc44f5e91a79bde.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{R} \times \mathbb{R}&lt;/object&gt; that satisfy the inequality.&lt;/p&gt;
&lt;p&gt;For our task we'll use discrete indices for &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, so the relation is
on &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/536c886d7863df5a4e250a73547be5d968c290c7.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{N}&lt;/object&gt;. We'll plot the relation by using a dark pixel (or
square) for a &lt;tt class="docutils literal"&gt;x,y&lt;/tt&gt; coordinate where the inequality holds and a light pixel
for a coordinate where it doesn't hold.&lt;/p&gt;
&lt;p&gt;The &amp;quot;mind-blowing&amp;quot; fact about Tupper's formula is that when plotted for
a certain range of &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, it produces this:&lt;/p&gt;
&lt;img alt="Tupper's formula own plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-plot.png" /&gt;
&lt;p&gt;Note that while &lt;em&gt;x&lt;/em&gt; runs in the inclusive range of 0-105 on the plot, &lt;em&gt;y&lt;/em&gt; starts
at a mysterious &lt;em&gt;K&lt;/em&gt; and ends at &lt;em&gt;K+16&lt;/em&gt;. For the plot above, &lt;em&gt;K&lt;/em&gt; needs to be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;4858450636189713423582095962494202044581400587983244549483
0930850619347047088099284506447698655243648499972470249151
1911041160573917740785691975432657185544205721044573588368
1829823754139634338225199452191651284348332905131193199953
5024137587652392648746133949068701305622958132194811136853
3953556529085002387509285689269455597428154638651073004910
6723058933586052544096664351265349363643957125565695936815
1843348576052669401612512669514215505395545191537854575257
5659074054015792900176596796548006442782913148854825991472
1248506352686630476300
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The amazement subsides slightly when we discover that for a different &lt;em&gt;K&lt;/em&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;,
we get a different plot:&lt;/p&gt;
&lt;img alt="Tupper's formula producing a pacman plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-pacman.png" /&gt;
&lt;p&gt;And, in fact, this formula can produce any 2D grid of 106x17 pixels, given the
right coordinates. Since the formula itself is so simple, it is quite apparent
that the value of &lt;em&gt;K&lt;/em&gt; is the key here; these are huge numbers with hundreds of
digits, so clearly they encode the image information somehow. Read on to see
how this actually works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-javascript-demo"&gt;
&lt;h2&gt;A JavaScript demo&lt;/h2&gt;
&lt;p&gt;I've implemented a simple online demo of plotting the Tupper formula - available
at &lt;a class="reference external" href="https://eliben.github.io/tupperformula/"&gt;https://eliben.github.io/tupperformula/&lt;/a&gt; (with &lt;a class="reference external" href="https://github.com/eliben/tupperformula"&gt;source code on GitHub&lt;/a&gt;). It was used to produce the images
shown above. The code is fairly straightforward, so I'll just focus on the
interesting part.&lt;/p&gt;
&lt;p&gt;The core of the code is a 2D grid that's plotted for &lt;em&gt;x&lt;/em&gt; running from 0 to
105 and &lt;em&gt;y&lt;/em&gt; from &lt;em&gt;K&lt;/em&gt; to &lt;em&gt;K+16&lt;/em&gt; (both ranges inclusive). The grid is populated
every time the number changes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;106&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;17&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;K&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Knum&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;Grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tupperFormula&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;K&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the use of JavaScript's &lt;tt class="docutils literal"&gt;BigInt&lt;/tt&gt; types here - very handy when dealing
with such huge numbers. Here is &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tupperFormula&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It looks quite different from the mathematical formula at the top of this post;
why? Because - as mentioned before - while Tupper's original formula works on
real numbers, our program only needs the discrete integer range of
&lt;tt class="docutils literal"&gt;x in [0, 105]&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;y in [K, K+16]&lt;/tt&gt;. When we deal with discrete numbers,
the formula can be simplified greatly.&lt;/p&gt;
&lt;p&gt;Let's start with the original formula and simplify it step by step:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a4f48d2debe2ad234574a9cf2a0c2d1b327963c7.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left \lfloor \frac{y}{17}\right \rfloor 2^{-17\lfloor x \rfloor - mod(\lfloor y \rfloor, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;First of all, since &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; are natural numbers, the floor operations on
them don't do anything, so we can drop them (including on the division by
17, if we just assume integer division that rounds down by default):&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f8e9f298cf8e514787979eaa4d7cc6b2b2489cb0.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left ( \frac{y}{17}\right ) 2^{-17x - mod(y, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;Next, since the result of the &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6bfbbf950c2eba80fdd316385a8c430702ef839f.svg" style="height: 19px;" type="image/svg+xml"&gt;mod(N,2)&lt;/object&gt; operation for a natural &lt;em&gt;N&lt;/em&gt; is
either 0 or 1, the comparison to half is just a fancy way of saying &amp;quot;equals 1&amp;quot;;
we can replace the inequality by:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b0e581251daff5ef47433e7e7c50bfc94ad4051a.svg" style="height: 33px;" type="image/svg+xml"&gt;\[mod\left ( \left ( \frac{y}{17}\right ) 2^{-17x - mod(y, 17)}, 2 \right )=1\]&lt;/object&gt;
&lt;p&gt;Note the negative power of 2; multiplying by it is the same as dividing by its
positive counterpart. Another way to express division by &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/339f03051f685e4ffbec605928020a75cc9c05d1.svg" style="height: 12px;" type="image/svg+xml"&gt;2^p&lt;/object&gt; for natural
numbers is a bit shift right by &lt;em&gt;p&lt;/em&gt; bits. So we get the code of the
&lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt; function shown above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="how-the-tupper-formula-works"&gt;
&lt;h2&gt;How the Tupper formula works&lt;/h2&gt;
&lt;p&gt;The distillation of the Tupper to JS code already peels off a few layers of
mystery. Let's now remove the rest of the curtain on its inner workings.&lt;/p&gt;
&lt;p&gt;I'll start by explaining how to take an image we want the formula
to produce and encode it into &lt;em&gt;K&lt;/em&gt;. Here are the first three columns of the
Tupper formula plot:&lt;/p&gt;
&lt;img alt="Closeup of tupper plot with encoding of pixels" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-closeup.png" /&gt;
&lt;p&gt;Each pixel in the plot is converted to a bit (0 for light, 1 for dark). We
start at the bottom left corner (&lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;), which is the LSB
(least-significant bit) and move up through the first column; when we reach the
top (&lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K+16&lt;/em&gt;), we continue from the bottom of the next column
(&lt;em&gt;x=1&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;). In the example above, the first bits (from lowest to highest)
of the number are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;00110010101000100 00101010101111100 ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once we're done with the whole number (106x17 = 1802 bits), we convert it to
decimal - let's call this number &lt;em&gt;IMG&lt;/em&gt;, and multiply by 17. The result is &lt;em&gt;K&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now back to &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt;, looking at how it decodes the image back from
&lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; (recall that &lt;em&gt;y&lt;/em&gt; runs from &lt;em&gt;K&lt;/em&gt; to &lt;em&gt;K+16&lt;/em&gt;). Let's work through
the first coordinate in detail:&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;, in &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt; we get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
substitute x=0, y=K (and recall that K = IMG * 17)
...
d = IMG &amp;gt;&amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In other words, &lt;em&gt;d&lt;/em&gt; is the lowest bit of &lt;em&gt;IMG&lt;/em&gt; - the lowest bit of our image!
We can continue for &lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K+1&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
substitute x=0, y=K+1 (and recall that K = IMG * 17)
...
d = IMG &amp;gt;&amp;gt; 1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here &lt;em&gt;d&lt;/em&gt; is the second lowest bit of &lt;em&gt;IMG&lt;/em&gt;. The pattern should be clear by now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
x=0  y=K+2:  IMG &amp;gt;&amp;gt; (0 + 2)
x=0  y=K+3:  IMG &amp;gt;&amp;gt; (0 + 3)
...
x=0  y=K+16  IMG &amp;gt;&amp;gt; (0 + 16)
x=1  y=K:    IMG &amp;gt;&amp;gt; (17 + 0)
x=1  y=K+1:  IMG &amp;gt;&amp;gt; (17 + 1)
x=1  y=K+2:  IMG &amp;gt;&amp;gt; (17 + 2)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The formula simply calculates the correct bit of &lt;em&gt;IMG&lt;/em&gt; given &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, using
a modular arithmetic trick to &amp;quot;fold&amp;quot; the 2D &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; into a 1D
sequence (this is just customary &lt;a class="reference external" href="https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays"&gt;column-major layout&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is why the formula can plot any 106x17 grid, given the right &lt;em&gt;K&lt;/em&gt;. In the
formula, 17 is not some piece of magic - it's just the height of the grid. As an
exercise, you can modify the formula and code to plot larger or smaller grids.&lt;/p&gt;
&lt;p&gt;As a bonus, the JavaScript demo can also encode a grid back to its
representative &lt;em&gt;K&lt;/em&gt;; here's the code for it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;// Calculate K value from the grid.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;encodeGridToK&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Build up K from MSB to LSB, scanning from the top-right corner down and&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// then moving left by column.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It constructs &lt;em&gt;K&lt;/em&gt; starting with the MSB, but otherwise the code is
straightforward to follow.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="background"&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The formula was first describe by Jeff Tupper in a 2001 paper titled
&amp;quot;Reliable Two-Dimensional Graphing Methods for Mathematical Formulae with Two
Free Variables&amp;quot;. The paper itself focuses on methods of precisely graphing
relations and presents several algorithms to do so. This formula is described
in passing in section 12, and presented as follows:&lt;/p&gt;
&lt;img alt="Screenshot from Tupper's paper describing the formula" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-paper-crop1.png" /&gt;
&lt;p&gt;And Figure 13 is:&lt;/p&gt;
&lt;img alt="Screenshot from Tupper's paper showing the formula itself" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-paper-crop2.png" /&gt;
&lt;p&gt;Interestingly, the &lt;em&gt;K&lt;/em&gt; provided by Tupper's paper renders the formula flipped
on both the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; axes using the standard grid used in this post &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.
This is why my JavaScript demo has flip toggles that let you flip the axes of any
plot.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This would be&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1445202489708975828479425373371945674812777822151507024797
1881396854908873568298734888825132090576643817888323197692
3440016667764749242125128995265907053708020473915320841631
7920255490054180047686572016997304663833949016013743197155
2099618114524978194501906835950051065780432564080119786755
6863142280259694206254096081665642417367403946384170774537
4273196064438999230103793989386750257869294552344763192918
6095761834543224800492172803334941981620674985447203819393
9738513848960476759782673313437697051994580681869819330446
336774047268864
&lt;/pre&gt;&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I can totally see why the &lt;em&gt;y&lt;/em&gt; axis would be flipped: in computer programs
the concept of the &lt;em&gt;y&lt;/em&gt; axis is represented as &lt;em&gt;rows&lt;/em&gt; in a grid which
typically count from 0 on top and downwards. It's less clear to me how
the inversion on the &lt;em&gt;x&lt;/em&gt; axis came to be.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Javascript"></category></entry><entry><title>Sum of same-frequency sinusoids</title><link href="https://eli.thegreenplace.net/2023/sum-of-same-frequency-sinusoids/" rel="alternate"></link><published>2023-03-11T19:44:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-03-11:/2023/sum-of-same-frequency-sinusoids/</id><summary type="html">&lt;p&gt;I was reviewing an electronics textbook the other day, and it made an offhand
comment that &amp;quot;sinusoidal signals of the same frequency always add up to a
sinusoid, even if their magnitudes and phases are different&amp;quot;.
This gave me pause; is that really so? Even with different phases?&lt;/p&gt;
&lt;p&gt;Using EE …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was reviewing an electronics textbook the other day, and it made an offhand
comment that &amp;quot;sinusoidal signals of the same frequency always add up to a
sinusoid, even if their magnitudes and phases are different&amp;quot;.
This gave me pause; is that really so? Even with different phases?&lt;/p&gt;
&lt;p&gt;Using EE notation, a sinusoidal signal with magnitude &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/86684571efbdc2d7a49562ba00dd15056c517135.svg" style="height: 16px;" type="image/svg+xml"&gt;A_1&lt;/object&gt;, frequency
&lt;img alt="w" class="valign-0" src="https://eli.thegreenplace.net/images/math/aff024fe4ab0fece4091de044c58c9ae4233383a.png" style="height: 8px;" /&gt; and phase &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/3d9f2f00378f60e70beb5531aa2169a534bffe40.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi_1&lt;/object&gt; is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6e641a468b62c6db3c69c21e1328e23ad284a748.svg" style="height: 19px;" type="image/svg+xml"&gt;A_1 sin(wt+\phi_1)&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;. The
book's statement amounts to:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a1b13ea5023ad1e0b1f2dde7cf495c7da83f6657.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)=A_3 sin(wt+\phi_3)\]&lt;/object&gt;
&lt;p&gt;The sum is also a sinusoid with the same frequency, but potentially different
magnitude and phase. I couldn't find this equality in any of my reference books,
so why is it true?&lt;/p&gt;
&lt;div class="section" id="empirical-probing"&gt;
&lt;h2&gt;Empirical probing&lt;/h2&gt;
&lt;p&gt;Let's start by asking whether this is true at all? It's not at all obvious that
this should work. &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/sinusoid"&gt;Armed with Python, Numpy and matplotlib&lt;/a&gt;, I
plotted two sinusoidal signals with the same frequency but different magnitudes
and phases:&lt;/p&gt;
&lt;img alt="Two sinusoidal signals plotted together" class="align-center" src="https://eli.thegreenplace.net/images/2023/two-sinusoids.png" /&gt;
&lt;p&gt;Now, plotting their sum in green on the same chart:&lt;/p&gt;
&lt;img alt="Two sinusoidal signals plotted together with their sum signal" class="align-center" src="https://eli.thegreenplace.net/images/2023/sinusoids-with-sum.png" /&gt;
&lt;p&gt;Well, look at that. It seems to be working. I guess it's time to prove it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="proof-using-trig-identities"&gt;
&lt;h2&gt;Proof using trig identities&lt;/h2&gt;
&lt;p&gt;The first proof I want to demonstrate doesn't use any fancy math beyond some
basic trigonometric identities. One of best known ones is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/78bab16c56ec9b9da0b5dc2543c8a5dabee73f08.svg" style="height: 19px;" type="image/svg+xml"&gt;\[sin(a+b)=sin(a)cos(b)+cos(a)sin(b) \hspace{2cm} (id. 1)\]&lt;/object&gt;
&lt;p&gt;Taking our sum of sinusoids:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/72c2f9d4c2be1adde9b7b4ba1bf94f31b3a8dd15.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)\]&lt;/object&gt;
&lt;p&gt;Applying (id.1) to each of the terms, and then regrouping, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/dac6d8532a46bbf7490d4e9083835914b8b61005.svg" style="height: 45px;" type="image/svg+xml"&gt;\[\begin{align*}
&amp;lt;sum&amp;gt;&amp;amp;=A_1\left [sin(wt)cos(\phi_1)+cos(wt)sin(\phi_1)  \right ]+A_2\left [sin(wt)cos(\phi_2)+cos(wt)sin(\phi_2)  \right ]\\
&amp;amp;=\left [A_1 cos(\phi_1) + A_2 cos(\phi_2) \right ]sin(wt)+\left [ A_1 sin(\phi_1) + A_2 sin(\phi_2)\right ]cos(wt)\\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now, a change of variables trick: we'll assume we can solve the following
set of equations for some &lt;img alt="B" class="valign-0" src="https://eli.thegreenplace.net/images/math/ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec.png" style="height: 12px;" /&gt; and &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c605365120049d7ca728477757b12b12efe13fdd.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
Bcos(\theta)&amp;amp;=A_1 cos(\phi_1)+A_2 cos(\phi_2) \hspace{2cm} (1)\\
Bsin(\theta)&amp;amp;=A_1 sin(\phi_1)+A_2 sin(\phi_2) \hspace{2cm} (2)\\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;To find &lt;img alt="B" class="valign-0" src="https://eli.thegreenplace.net/images/math/ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec.png" style="height: 12px;" /&gt;, we can square each of (1) and (2) and then add the
squares together:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4fdeb60073659f55fa75878ec2f0867f9cbe7fd6.svg" style="height: 22px;" type="image/svg+xml"&gt;\[B^2 cos^2 (\theta)+B^2 sin^2 (\theta)=(A_1 cos(\phi_1)+A_2 cos(\phi_2))^2 + (A_1 sin(\phi_1)+A_2 sin(\phi_2))^2\]&lt;/object&gt;
&lt;p&gt;Using the fact that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e188aa0292a1c0d17344548fdcc38dc26faf3429.svg" style="height: 20px;" type="image/svg+xml"&gt;cos^2(a)+sin^2(a)=1&lt;/object&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/28f6325da1ceee1767cdc9508fbc85b90427b1b0.svg" style="height: 23px;" type="image/svg+xml"&gt;\[B=\sqrt{(A_1 cos(\phi_1)+A_2 cos(\phi_2))^2 + (A_1 sin(\phi_1)+A_2 sin(\phi_2))^2}\]&lt;/object&gt;
&lt;p&gt;To solve for &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;, we can divide equation (2) by (1), getting:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a0ecd0ee8c1f85d0ddb6b1224f48f5a8f2c469de.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\frac{sin(\theta)}{cos(\theta)}=tan(\theta)=\frac{A_1 sin(\phi_1)+A_2 sin(\phi_2)}{A_1 cos(\phi_1)+A_2 cos(\phi_2)}\]&lt;/object&gt;
&lt;p&gt;Meaning that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b66eb0f8ef309c637f017134d8c66368452594c8.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\theta = atan{\frac{A_1 sin(\phi_1)+A_2 sin(\phi_2)}{A_1 cos(\phi_1)+A_2 cos(\phi_2)}}\]&lt;/object&gt;
&lt;p&gt;Now that we have the values of &lt;img alt="B" class="valign-0" src="https://eli.thegreenplace.net/images/math/ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec.png" style="height: 12px;" /&gt; and &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;, let's put them aside
for a bit and get back to the final line of our sum of sinusoids equation:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/921c17b6fb04e7cb8cc10d3f5652ed28d6f508ed.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)=\left [A_1 cos(\phi_1) + A_2 cos(\phi_2) \right ]sin(wt)+\left [ A_1 sin(\phi_1) + A_2 sin(\phi_2)\right ]cos(wt)\]&lt;/object&gt;
&lt;p&gt;On the right-hand side, we can apply equations (1) and (2) to get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/580c6c30e3caaf5ab9bb0472620c46f53266a5a2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)=B cos(\theta) sin(wt)+ B sin(\theta) cos(wt)\]&lt;/object&gt;
&lt;p&gt;Applying (id.1) again, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/042c3173428218fb7a3410884288a540ecc78cd1.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)=B sin(wt + \theta)\]&lt;/object&gt;
&lt;p&gt;We've just shown that the sum of sinusoids with the same frequency &lt;img alt="w" class="valign-0" src="https://eli.thegreenplace.net/images/math/aff024fe4ab0fece4091de044c58c9ae4233383a.png" style="height: 8px;" /&gt;
is another sinusoid with frequency &lt;img alt="w" class="valign-0" src="https://eli.thegreenplace.net/images/math/aff024fe4ab0fece4091de044c58c9ae4233383a.png" style="height: 8px;" /&gt;, and we've calculated &lt;img alt="B" class="valign-0" src="https://eli.thegreenplace.net/images/math/ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec.png" style="height: 12px;" /&gt; and
&lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; from the other parameters (&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/86684571efbdc2d7a49562ba00dd15056c517135.svg" style="height: 16px;" type="image/svg+xml"&gt;A_1&lt;/object&gt;, &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/dfcd56bce194520e6f50a8f821c98f338cb9d65c.svg" style="height: 16px;" type="image/svg+xml"&gt;A_2&lt;/object&gt;,
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/3d9f2f00378f60e70beb5531aa2169a534bffe40.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi_1&lt;/object&gt; and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/f80876d413b14edaee0aa2678ab67346f6da633c.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi_2&lt;/object&gt;) &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="proof-using-complex-numbers"&gt;
&lt;h2&gt;Proof using complex numbers&lt;/h2&gt;
&lt;p&gt;The second proof uses a bit more advanced math, but overall feels more elegant
to me. The plan is to use Euler's equation and prove a more general statement
on the complex plane.&lt;/p&gt;
&lt;p&gt;Instead of looking at the sum of real sinusoids, we'll first look at the sum
of two complex exponential functions:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ae18a24d9752babf28d9472d59bdc1e67bdb2074.svg" style="height: 20px;" type="image/svg+xml"&gt;\[A_1 e^{j(wt + \phi_1)} + A_2 e^{j(wt + \phi_2)}\]&lt;/object&gt;
&lt;p&gt;Reminder: Euler's equation for a complex exponential is&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0741c35664a1df4373bf777ddecce046f75eb386.svg" style="height: 21px;" type="image/svg+xml"&gt;\[e^{jx}=cosx+jsinx\]&lt;/object&gt;
&lt;p&gt;Regrouping our sum of exponentials a bit and then applying this equation:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/12458dd213e9b9f3ef8ee7b9d34094410fcd3fbb.svg" style="height: 88px;" type="image/svg+xml"&gt;\[\begin{align*}
A_1 e^{j(wt + \phi_1)} + A_2 e^{j(wt + \phi_2)}&amp;amp;=e^{jwt}\left (A_1 e^{j\phi_1} + A_2 e^{j\phi_2}\right )\\
&amp;amp;=e^{jwt}\left ( A_1 cos(\phi_1) + jA_1 sin(\phi_1) + A_2 cos(\phi_2) + jA_2 sin(\phi_2)\right )\\
&amp;amp;=e^{jwt}\left [\left (A_1 cos(\phi_1) + A_2 cos(\phi_2) \right ) + j\left(A_1 sin(\phi_1) + A_2 sin(\phi_2) \right ) \right ]
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The value inside the square brackets can be viewed as a complex number in its
rectangular form: &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/8e2a949b46783cd572f79c9ad9d6a3887f0fb462.svg" style="height: 16px;" type="image/svg+xml"&gt;x + jy&lt;/object&gt;. We can convert it to its polar form:
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/14073811ee769d485ac4495503a1d32292b73f45.svg" style="height: 15px;" type="image/svg+xml"&gt;re^{j\theta}&lt;/object&gt;, by calculating:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/dd1d145174f17f82bfcffce4c658ac249025aaf2.svg" style="height: 62px;" type="image/svg+xml"&gt;\[\begin{align*}
r&amp;amp;=\sqrt{x^2+y^2}\\
\theta&amp;amp;=atan(\frac{y}{x})
\end{align*}\]&lt;/object&gt;
&lt;p&gt;In our case:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/18a842fb6ac23aa16c0c4b4fe8c974c064933509.svg" style="height: 23px;" type="image/svg+xml"&gt;\[r=\sqrt{(A_1 cos(\phi_1)+A_2 cos(\phi_2))^2 + (A_1 sin(\phi_1)+A_2 sin(\phi_2))^2}\]&lt;/object&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b66eb0f8ef309c637f017134d8c66368452594c8.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\theta = atan{\frac{A_1 sin(\phi_1)+A_2 sin(\phi_2)}{A_1 cos(\phi_1)+A_2 cos(\phi_2)}}\]&lt;/object&gt;
&lt;p&gt;Therefore, the sum of complex exponentials is another complex exponential with
the same frequency, but a different magnitude and phase:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a5028bcfc1af3517f2e52448e4b51f25c80ade80.svg" style="height: 20px;" type="image/svg+xml"&gt;\[A_1 e^{j(wt + \phi_1)} + A_2 e^{j(wt + \phi_2)}= e^{jwt} r e^{j \theta}=r e^{j(wt + \theta)}\]&lt;/object&gt;
&lt;p&gt;From here, we can use Euler's equation again to see the equivalence in terms
of sinusoidal functions:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/dc5490843cd4fc3676a1bcf8fcdf0002ae0f227a.svg" style="height: 45px;" type="image/svg+xml"&gt;\[\begin{align*}
A_1 cos(wt+\phi_1)+jA_1 sin(wt+\phi_1)&amp;amp;+\\
A_2 cos(wt+\phi_2)+jA_2 sin(wt+\phi_2)&amp;amp;=r cos(wt+\theta) + jr sin(wt+\theta)
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;If we only compare the imaginary parts of this equation, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e1ef66dd4f3ec1496cb94bae8f52acf5f77229da.svg" style="height: 19px;" type="image/svg+xml"&gt;\[A_1 sin(wt+\phi_1)+A_2 sin(wt+\phi_2)=r sin(wt+\theta)\]&lt;/object&gt;
&lt;p&gt;With known &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4dc7c9ec434ed06502767136789763ec11d2c4b7.svg" style="height: 8px;" type="image/svg+xml"&gt;r&lt;/object&gt; and &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; we've calculated earlier from the other
constants &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Note that by comparing the real parts of the equation, we can trivially prove a
similar statement about the sum of cosines (which should surprise no one, since
a cosine is just a phase-shifted sine).&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;Electrical engineers prefer their signal frequencies in units of
radian per second.&lt;/p&gt;
&lt;p class="last"&gt;We also like calling the imaginary unit &lt;em&gt;j&lt;/em&gt; instead of &lt;em&gt;i&lt;/em&gt;, because
the latter is used for electrical current.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If you're wondering &amp;quot;hold on, why would this work?&amp;quot;, recall that
any point &lt;em&gt;(x,y)&lt;/em&gt; on the Cartesian plane can be represented using
&lt;em&gt;polar coordinates&lt;/em&gt; with magnitude &lt;img alt="B" class="valign-0" src="https://eli.thegreenplace.net/images/math/ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec.png" style="height: 12px;" /&gt; and angle &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="EE &amp; Embedded"></category></entry><entry><title>Derivative of the Exponential Function</title><link href="https://eli.thegreenplace.net/2022/derivative-of-the-exponential-function/" rel="alternate"></link><published>2022-09-12T20:23:00-07:00</published><updated>2022-10-04T14:08:24-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2022-09-12:/2022/derivative-of-the-exponential-function/</id><summary type="html">&lt;p&gt;It's a known piece of math folklore that &lt;em&gt;e&lt;/em&gt; was &amp;quot;discovered&amp;quot; by Jacob Bernoulli
in the 17th century, when he was &lt;a class="reference external" href="https://en.wikipedia.org/wiki/E_(mathematical_constant)#Compound_interest"&gt;pondering compound interest&lt;/a&gt;,
and defined thus &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b38717ee73c9714fb983060c8880bbcddbd7ece8.svg" style="height: 43px;" type="image/svg+xml"&gt;\[e=\displaystyle \lim_{n \to \infty}\left ( 1+\frac{1}{n}\right )^n\]&lt;/object&gt;
&lt;p&gt;&lt;em&gt;e&lt;/em&gt; is extremely important in mathematics for several reasons …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's a known piece of math folklore that &lt;em&gt;e&lt;/em&gt; was &amp;quot;discovered&amp;quot; by Jacob Bernoulli
in the 17th century, when he was &lt;a class="reference external" href="https://en.wikipedia.org/wiki/E_(mathematical_constant)#Compound_interest"&gt;pondering compound interest&lt;/a&gt;,
and defined thus &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b38717ee73c9714fb983060c8880bbcddbd7ece8.svg" style="height: 43px;" type="image/svg+xml"&gt;\[e=\displaystyle \lim_{n \to \infty}\left ( 1+\frac{1}{n}\right )^n\]&lt;/object&gt;
&lt;p&gt;&lt;em&gt;e&lt;/em&gt; is extremely important in mathematics for several reasons; one of them is
its useful behavior under derivation and integration; specifically, that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4f68283e668f7210912235c67d553b9b5b62e0c6.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{\mathrm{d} }{\mathrm{d} x}e^x=e^x\]&lt;/object&gt;
&lt;p&gt;In this post I want to present a couple of simple proofs of this fundamental
fact.&lt;/p&gt;
&lt;div class="section" id="proof-using-the-limit-definition"&gt;
&lt;h2&gt;Proof using the limit definition&lt;/h2&gt;
&lt;p&gt;As a prerequisite for this proof, let's reorder the original definition of &lt;em&gt;e&lt;/em&gt;
slightly. If we perform a change of variable replacing &lt;em&gt;n&lt;/em&gt; by
&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/4a6dcb043a4e66ef2bd9dd59ca7be7f0048f2ceb.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{1}{m}&lt;/object&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c5d22a14c9370c2a5fcec0b7a0e9517c88812806.svg" style="height: 33px;" type="image/svg+xml"&gt;\[e=\displaystyle \lim_{m \to 0}\left ( 1+m \right )^\frac{1}{m} \tag{1}\]&lt;/object&gt;
&lt;p&gt;This equation will become useful a bit later.&lt;/p&gt;
&lt;p&gt;Let's start our proof by spelling out the definition of a derivative:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3d28b1e82d657f39b1a6dba9fc7e54f31d109dd6.svg" style="height: 45px;" type="image/svg+xml"&gt;\[\frac{\mathrm{d} }{\mathrm{d} x}e^x=\displaystyle \lim_{h \to 0}\left ( \frac{e^{x+h}-e^x}{h} \right )\]&lt;/object&gt;
&lt;p&gt;A bit of algebra and observing that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt; does not depend on &lt;em&gt;h&lt;/em&gt; gives us:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0ea733876d8139ddc90051095c434ac5db44b9d4.svg" style="height: 95px;" type="image/svg+xml"&gt;\[\begin{align*}
\frac{\mathrm{d} }{\mathrm{d} x}e^x=
\displaystyle \lim_{h \to 0}\left ( \frac{e^{x+h}-e^x}{h} \right )&amp;amp;=
\displaystyle \lim_{h \to 0}\left ( \frac{e^x(e^{h}-1)}{h} \right )\\
&amp;amp;=\displaystyle e^x\lim_{h \to 0}\left ( \frac{e^{h}-1}{h} \right )
\end{align*}\]&lt;/object&gt;
&lt;p&gt;At this point we're stuck; clearly as &lt;em&gt;h&lt;/em&gt; approaches 0, both the numerator
and denominator approach 0 as well. The way out - as is often the case in such
scenarios - is a sneaky change of variable. Recall equation (1) - how could we
use it here?&lt;/p&gt;
&lt;p&gt;The change of variable we'll use is &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7ef01f919bc1533e4d71c5eba95a4ee97a6e201c.svg" style="height: 15px;" type="image/svg+xml"&gt;m=e^h-1&lt;/object&gt;, which implies that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/b643eede4aa771a06699ca68e95333b7ffd37688.svg" style="height: 19px;" type="image/svg+xml"&gt;h=ln(m+1)&lt;/object&gt;. Note that as &lt;em&gt;h&lt;/em&gt; approaches zero, so does &lt;em&gt;m&lt;/em&gt;. Rewriting our
last expression, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2f2b6ded322fa9bdd655f9e1964116c7a87201d8.svg" style="height: 114px;" type="image/svg+xml"&gt;\[\begin{align*}
\frac{\mathrm{d} }{\mathrm{d} x}e^x=
\displaystyle e^x\lim_{m \to 0}\left ( \frac{m}{ln(m+1)} \right )
&amp;amp;=\displaystyle e^x\lim_{m \to 0}\left ( \frac{1}{\frac{1}{m}ln(m+1)} \right )\\
&amp;amp;=\displaystyle e^x\lim_{m \to 0}\left ( \frac{1}{ln(m+1)^\frac{1}{m}} \right )
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Equation (1) tells us that as &lt;em&gt;m&lt;/em&gt; approaches zero, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/93610c34906bf7f7a7ebc62b2f10961295a84a46.svg" style="height: 24px;" type="image/svg+xml"&gt;(m+1)^\frac{1}{m}&lt;/object&gt;
approaches &lt;em&gt;e&lt;/em&gt;. Substituting that into the denominator we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/23f82378f542f4af7d7c1855a76e5741cc3b88cb.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\frac{\mathrm{d} }{\mathrm{d} x}e^x=
\displaystyle e^x\lim_{m \to 0}\left ( \frac{1}{ln(e)} \right )=e^x \quad \blacksquare\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="proof-using-power-series-expansion"&gt;
&lt;h2&gt;Proof using power series expansion&lt;/h2&gt;
&lt;p&gt;It's always fun to prove the same thing in multiple ways; while I'm sure there
are &lt;em&gt;many&lt;/em&gt; other techniques to find the derivative of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;, one I
particularly like for its simplicity uses its power series expansion.&lt;/p&gt;
&lt;p&gt;Similarly to the way &lt;em&gt;e&lt;/em&gt; itself was defined empirically, one can show that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1b89b9181cf398f1cc356fd9edb2dfa676a531b6.svg" style="height: 34px;" type="image/svg+xml"&gt;\[e^x=\displaystyle \lim_{n \to \infty}\left ( 1+\frac{x}{n}\right )^n\]&lt;/object&gt;
&lt;p&gt;(For a proof of this equation, see the Appendix)&lt;/p&gt;
&lt;p&gt;Let's use the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Binomial_theorem"&gt;Binomial theorem&lt;/a&gt; to open up the parentheses
inside the limit:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3eedd2e8edac6b04ea7525ca3fa26fa4bfbe6511.svg" style="height: 50px;" type="image/svg+xml"&gt;\[e^x=\lim_{n \to \infty}\left ( 1+\frac{x}{n} \right )^n = \lim_{n \to \infty}\sum_{k=0}^n {n \choose k}1^{n-k}\left (\frac{x}{n}\right )^k\]&lt;/object&gt;
&lt;p&gt;We'll unroll the sum a bit, so it's easier to manipulate algebraically. We can
use the standard formula for &amp;quot;choose &lt;em&gt;n&lt;/em&gt; out of &lt;em&gt;k&lt;/em&gt;&amp;quot; and get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0a37b5437f0fdba18d5675af218b201982254aa7.svg" style="height: 45px;" type="image/svg+xml"&gt;\[e^x=\lim_{n \to \infty}\left ( 1+n\frac{x}{n}+\frac{n(n-1)}{2!}\frac{x^2}{n^2}+\frac{n(n-1)(n-2)}{3!}\frac{x^3}{n^3}+\dotsb \right )\]&lt;/object&gt;
&lt;p&gt;Inside the limit, we can simplify all the &lt;em&gt;n-c&lt;/em&gt; terms
with a constant &lt;em&gt;c&lt;/em&gt; to just &lt;em&gt;n&lt;/em&gt;, since compared to infinity &lt;em&gt;c&lt;/em&gt; is negligible.
This means that all these terms can be simplified as &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/2731efbf321b855767998511f8d7f6cb94993bb0.svg" style="height: 20px;" type="image/svg+xml"&gt;n(n-1)\approx n^2&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1b29d1a4b7009a76de4c892906b7551b9c4f3819.svg" style="height: 20px;" type="image/svg+xml"&gt;n(n-1)(n-2)\approx n^3&lt;/object&gt; and so on. All these powers of &lt;em&gt;n&lt;/em&gt; cancel out in
the numerator and denominator, and we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2c7c3561f4a48090cb92d746e56f659e2d615bfd.svg" style="height: 44px;" type="image/svg+xml"&gt;\[e^x=\lim_{n \to \infty}\left ( 1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\dotsb \right )\]&lt;/object&gt;
&lt;p&gt;And since the contents of the limit don't actually depend on &lt;em&gt;n&lt;/em&gt; any more, this
leaves us with a well-known formula for approximating &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f8a49b0b9cf9928cc7f99f5ca536f6dffce3c5df.svg" style="height: 39px;" type="image/svg+xml"&gt;\[e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\dotsb\]&lt;/object&gt;
&lt;p&gt;We can finally use this power series expansion to calculate the derivative of
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt; quite trivially. Since it's a sum of terms, the derivative is the
sum of the derivatives of the terms:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4385eae99e184247394ad076654b3e7a14376633.svg" style="height: 137px;" type="image/svg+xml"&gt;\[\begin{align*}
(e^x)^\prime&amp;amp;=1^\prime+x^\prime+\left (\frac{x^2}{2!}\right )^\prime+\left (\frac{x^3}{3!}\right )^\prime+\left (\frac{x^4}{4!}\right )^\prime+\dotsb\\
&amp;amp;=0+1+\frac{2x}{2!}+\frac{3x^2}{3!}+\frac{4x^3}{4!}+\dotsb\\
&amp;amp;=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\dotsb
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Look at that, we've got &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt; back, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix"&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;Let's see why:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1b89b9181cf398f1cc356fd9edb2dfa676a531b6.svg" style="height: 34px;" type="image/svg+xml"&gt;\[e^x=\displaystyle \lim_{n \to \infty}\left ( 1+\frac{x}{n}\right )^n\]&lt;/object&gt;
&lt;p&gt;We'll start with the limit and will arrive at &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;.
Using a change of variable &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/2c88dc358b216dce1aee4d5ec181472ecdb6e4f8.svg" style="height: 19px;" type="image/svg+xml"&gt;m=\frac{n}{x}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/df71410313e2414b726a42ed9f58d4240202e885.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\lim_{n \to \infty}\left (1 + \frac{x}{n}\right )^{n}=\lim_{n \to \infty}\left (1 + \frac{1}{m}\right )^{mx}  =\lim_{n \to \infty}\left [\left (1 + \frac{1}{m}\right )^{m}\right ]^{x}\]&lt;/object&gt;
&lt;p&gt;Given our change of variable, since &lt;em&gt;n&lt;/em&gt; approaches infinity, so does &lt;em&gt;m&lt;/em&gt;.
Therefore, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/348b055d7228215462df549a21c8c9ca08a7a161.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\lim_{m \to \infty}\left [\left (1 + \frac{1}{m}\right )^{m}\right ]^{x}\]&lt;/object&gt;
&lt;p&gt;Nothing in the limit depends on &lt;em&gt;x&lt;/em&gt;, so that exponent can be seen as applying to
the whole limit. And the limit is the definition of &lt;em&gt;e&lt;/em&gt;; therefore, we get
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;What I love about this definition is that it's entirely empirical. Try
to substitute successively larger numbers for &lt;em&gt;n&lt;/em&gt; in the equation,
and you'll see that the result approaches the value &lt;em&gt;e&lt;/em&gt; more and more
closely. The limit of this process for an infinite &lt;em&gt;n&lt;/em&gt; was called &lt;em&gt;e&lt;/em&gt;.
Bernoulli did all of this by hand, which is rather tedious. His
best estimate was that &lt;em&gt;e&lt;/em&gt; is &amp;quot;larger than 2 and a half but smaller than
3&amp;quot;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Another way to get this formula is from the Maclaurin series expansion
of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;, but we couldn't use that here since Maclaurin series
require derivatives, while we're trying to figure out what the derivative
of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt; is.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Some clues to understanding Benford's law</title><link href="https://eli.thegreenplace.net/2022/some-clues-to-understanding-benfords-law/" rel="alternate"></link><published>2022-03-12T06:03:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2022-03-12:/2022/some-clues-to-understanding-benfords-law/</id><summary type="html">&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Benford%27s_law"&gt;Benford's law&lt;/a&gt; is a really
fascinating observation that in many real-life sets of numerical data, the
first digit is most likely to be 1, and every digit &lt;tt class="docutils literal"&gt;d&lt;/tt&gt; is more common than
&lt;tt class="docutils literal"&gt;d+1&lt;/tt&gt;. Here's a table of the probability distribution, from Wikipedia:&lt;/p&gt;
&lt;img alt="Benford's law table from wikipedia" class="align-center" src="https://eli.thegreenplace.net/images/2022/wikipedia-benford-table.png" /&gt;
&lt;p&gt;Now, the caveat &amp;quot;&lt;em&gt;real-life&lt;/em&gt; data sets&amp;quot; is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Benford%27s_law"&gt;Benford's law&lt;/a&gt; is a really
fascinating observation that in many real-life sets of numerical data, the
first digit is most likely to be 1, and every digit &lt;tt class="docutils literal"&gt;d&lt;/tt&gt; is more common than
&lt;tt class="docutils literal"&gt;d+1&lt;/tt&gt;. Here's a table of the probability distribution, from Wikipedia:&lt;/p&gt;
&lt;img alt="Benford's law table from wikipedia" class="align-center" src="https://eli.thegreenplace.net/images/2022/wikipedia-benford-table.png" /&gt;
&lt;p&gt;Now, the caveat &amp;quot;&lt;em&gt;real-life&lt;/em&gt; data sets&amp;quot; is really important. Specifically, this
only applies when the data spans several orders of magnitude. Clearly, if we're
measuring the height in inches of some large group of adults, the
overwhelming majority of data will lie between 50 and 85 inches, and won't
follow Benford's law. Another aspect of real-life data is that it's non random;
if we take a bunch of truly random numbers spanning several orders of magnitude,
their leading digit won't follow Benford's law either.&lt;/p&gt;
&lt;p&gt;In this short post I'll try to explain how I understand Benford's law, and why
it intuitively makes sense. During the post I'll collect a set of &lt;strong&gt;clues&lt;/strong&gt;,
which will help get the intuition in place eventually. By the way, we've already
encountered our first clues:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Clue 1&lt;/strong&gt;: Benford's law only works on real-life data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clue 2&lt;/strong&gt;: Benford's law isn't just about the digit 1; 2 is more common than
3, 3 is more common than 4 etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="real-world-example"&gt;
&lt;h2&gt;Real-world example&lt;/h2&gt;
&lt;p&gt;First, let's start with a real-world demonstration of the law in action. I
found a data table of the populations of California's ~480 largest cities, and
ran an analysis of the
population number's leading digit &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;. Clearly, this is real-life data, and it
also spans many orders of magnitude (from LA at 3.9 mln to Amador with 153
inhabitants). Indeed, Benford's law applies beautifully on this data:&lt;/p&gt;
&lt;img alt="Distribution of first digit in population of California cities" class="align-center" src="https://eli.thegreenplace.net/images/2022/benford-ca-cities.png" /&gt;
&lt;p&gt;Eyeballing the city population data, we'll notice something important but also
totally intuitive: most cities are small. There are many more small cities than
large ones. Out of the 480 cities in our data set, only 74 have population over
100k, for example.&lt;/p&gt;
&lt;p&gt;The same is true of other real-world data sets; for example, if we take a
snapshot of stock prices of S&amp;amp;P 500 companies at some historic point, the prices
range from $1806 to $2, though 90% are under $182 and 65% are under $100.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Clue 3&lt;/strong&gt;: in real-world data distributed along many orders of magnitude,
smaller data points are more common than larger data points.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Statistically, this is akin to saying that the data follows the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Pareto_distribution"&gt;Pareto
distribution&lt;/a&gt;, of which the
&amp;quot;80-20 rule&amp;quot; - known as the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Pareto_principle"&gt;Pareto principle&lt;/a&gt; - is a special case.
Another similar mathematical description (applied to discrete probability
distributions) is &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Zipf's law&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="logarithmic-scale"&gt;
&lt;h2&gt;Logarithmic scale&lt;/h2&gt;
&lt;p&gt;To reiterate, a lot of real-world data isn't really uniformly distributed.
Rather, it follows a Pareto distribution where smaller numbers are more common.
Here's a useful logarithmic scale borrowed from Wikipedia - this could be the
X axis of any logarithmic plot:&lt;/p&gt;
&lt;img alt="Logarithmic scale bar from wikipedia" class="align-center" src="https://eli.thegreenplace.net/images/2022/logscale-wikipedia.png" /&gt;
&lt;p&gt;In this image, smaller values get more &amp;quot;real estate&amp;quot; on the X axis, which is
fair for our distribution if smaller numbers are more common than larger
numbers. It should not be hard to convince yourself that every time we &amp;quot;drop a
pin&amp;quot; on this scale, the chance of the leading digit being 1 is the highest.
Another (related) way to look at it is - when smaller numbers are more common it
takes a 100% percent increase to go from leading digit being 1 to it being 2,
but only a 50% increase to go from 2 to 3, etc.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Clue 4&lt;/strong&gt;: on a logarithmic scale, the distance between numbers starting
with 1s and numbers starting with 2s is bigger than the distance between
numbers starting with 2s and numbers starting with 3s, and so on.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can visualize this in another way; let's plot the ratio of numbers starting
with 1 among all numbers up to some point. On the X axis we'll place N which
means &amp;quot;in all numbers up to N&amp;quot;, and on the Y axis we'll place the ratio of
numbers &lt;tt class="docutils literal"&gt;i&lt;/tt&gt; between 0 and N that start with 1:&lt;/p&gt;
&lt;img alt="Jagged graph of P(digit=1) for all numbers up to N" class="align-center" src="https://eli.thegreenplace.net/images/2022/digit1p.png" /&gt;
&lt;p&gt;Note that whenever some new order of magnitude is reached, the ratio starts to
climb steadily until it reaches ~0.5 (because there are just as many numbers
with D digits as numbers starting with 1 and followed by another D digits);
it then starts falling until it reaches ~0.1 just before we flip to the next
order of magnitude (because in all D-digit numbers, numbers starting with each
digit are one tenth of the population). If we calculate the smoothed average of
this graph over time, it ends up at about 0.3, which corresponds to Benford's
law.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;When I'm thinking of Benford's law, the observation that really brings it home
for me is that &amp;quot;smaller numbers are more common than larger numbers&amp;quot; (this is
clue 3). This property of many realistic data sets, along with an understanding
of the logarithmic scale (the penultimate image above) is really all you need
to intuitively grok Benford's law.&lt;/p&gt;
&lt;p&gt;Benford's law is also famous for being scale-invariant (by typically applying
regardless of the unit of measurement) and base-invariant (works in bases other
than 10). Hopefully, this post makes it clear why these properties are expected
to be true.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;All the (hacky Go) code and data required to generate the plots in this
post is available &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2022/benford"&gt;on GitHub&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Computing the Chinese Remainder Theorem</title><link href="https://eli.thegreenplace.net/2020/computing-the-chinese-remainder-theorem/" rel="alternate"></link><published>2020-12-18T05:44:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2020-12-18:/2020/computing-the-chinese-remainder-theorem/</id><summary type="html">&lt;p&gt;Last year, I wrote &lt;a class="reference external" href="https://eli.thegreenplace.net/2019/the-chinese-remainder-theorem/"&gt;a post about the Chinese Remainder Theorem&lt;/a&gt; (CRT),
focusing on the math. Here, I want to talk about implementing solvers for
the CRT.&lt;/p&gt;
&lt;div class="section" id="crt-reminder"&gt;
&lt;h2&gt;CRT reminder&lt;/h2&gt;
&lt;p&gt;Assume &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/3095109bb55e0b34ecac71d33040fa004bfdfc7d.svg" style="height: 12px;" type="image/svg+xml"&gt;n_1,\dots,n_k&lt;/object&gt; are positive integers, pairwise co-prime; that is,
for any &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/eebecf421c4d33eeab4a0c4da6c20ed8d49e6c6c.svg" style="height: 17px;" type="image/svg+xml"&gt;i\neq j&lt;/object&gt;, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/b2820249e9aae164709f509d84fa260d142ee148.svg" style="height: 20px;" type="image/svg+xml"&gt;(n_i,n_j)=1&lt;/object&gt;. Let &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/1399d8e50dc6eadcb3e40b623a13734e492a60f7.svg" style="height: 12px;" type="image/svg+xml"&gt;a_1 …&lt;/object&gt;&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Last year, I wrote &lt;a class="reference external" href="https://eli.thegreenplace.net/2019/the-chinese-remainder-theorem/"&gt;a post about the Chinese Remainder Theorem&lt;/a&gt; (CRT),
focusing on the math. Here, I want to talk about implementing solvers for
the CRT.&lt;/p&gt;
&lt;div class="section" id="crt-reminder"&gt;
&lt;h2&gt;CRT reminder&lt;/h2&gt;
&lt;p&gt;Assume &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/3095109bb55e0b34ecac71d33040fa004bfdfc7d.svg" style="height: 12px;" type="image/svg+xml"&gt;n_1,\dots,n_k&lt;/object&gt; are positive integers, pairwise co-prime; that is,
for any &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/eebecf421c4d33eeab4a0c4da6c20ed8d49e6c6c.svg" style="height: 17px;" type="image/svg+xml"&gt;i\neq j&lt;/object&gt;, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/b2820249e9aae164709f509d84fa260d142ee148.svg" style="height: 20px;" type="image/svg+xml"&gt;(n_i,n_j)=1&lt;/object&gt;. Let &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/1399d8e50dc6eadcb3e40b623a13734e492a60f7.svg" style="height: 12px;" type="image/svg+xml"&gt;a_1,\dots,a_k&lt;/object&gt; be
arbitrary integers. The system of congruences with an unknown &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/93757b2beff989fc02e09cd045a9b4ff5959c8c6.svg" style="height: 82px;" type="image/svg+xml"&gt;\[\begin{align*}
x &amp;amp;\equiv a_1 \pmod{n_1} \\
  &amp;amp;\vdots \\
x &amp;amp;\equiv a_k \pmod{n_k}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;has a single solution modulo the product
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9ee0288589a80853a3cb1ede6482968e0d126e93.svg" style="height: 16px;" type="image/svg+xml"&gt;N=n_1\times n_2\times \cdots \times n_k&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;See the post linked above for a proof of this theorem, along with all the
required number theory prerequisites.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="naive-solution-by-searching-exhaustively"&gt;
&lt;h2&gt;Naive solution by searching exhaustively&lt;/h2&gt;
&lt;p&gt;Suppose you have an actual programming problem that maps to the CRT. How
would you go about solving it?&lt;/p&gt;
&lt;p&gt;To make things more concrete, say the problem is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/aa76cc1b4c48f3926538c970f6884f2afbded1b8.svg" style="height: 72px;" type="image/svg+xml"&gt;\[\begin{align*}
x &amp;amp;\equiv 0 \pmod{3} \\
x &amp;amp;\equiv 3 \pmod{4} \\
x &amp;amp;\equiv 4 \pmod{5} \\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The naive solution is search from 1 to 3*4*5-1 (since by the CRT we expect a
unique solution modulo &lt;em&gt;N&lt;/em&gt;, which is the product of all &lt;em&gt;n&lt;/em&gt;). When we find a
number that satisfies all the congruences - it's the solution! In this
particular case, 39 is a solution. Since the solution is only unique modulo
&lt;em&gt;N&lt;/em&gt;, we can keep adding 60 to our solution to get additional solutions.&lt;/p&gt;
&lt;p&gt;Coding this is trivial:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;crtSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nx"&gt;search&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Does i satisfy all the congruences?&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;search&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This approach works well for small problems, but fails miserably for anything
even moderately large. The problem is obvious: the complexity of this algorithm
is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/32b2d6975b6a09d74eb12de4876d0e5898076d58.svg" style="height: 19px;" type="image/svg+xml"&gt;O(kN)&lt;/object&gt; where &lt;em&gt;k&lt;/em&gt; is the number of congruences. In number theoretic
algorithms, it's common to talk about the problem size as the &lt;em&gt;bit size&lt;/em&gt; of
the numbers involved; in this formulation, this algorithm is exponential (since
&lt;em&gt;N&lt;/em&gt; itself is 2 to the power of its size in bits).&lt;/p&gt;
&lt;p&gt;For a concrete example, let's examine a somewhat larger problem:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e6bdf7dce6f08c0499af3e82e284aa297e19df8a.svg" style="height: 99px;" type="image/svg+xml"&gt;\[\begin{align*}
x &amp;amp;\equiv 2292 &amp;amp;\pmod{77003} \\
x &amp;amp;\equiv 3010 &amp;amp;\pmod{61223} \\
x &amp;amp;\equiv 500 &amp;amp;\pmod{60161} \\
x &amp;amp;\equiv 399 &amp;amp;\pmod{25873} \\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Here the naive algorithm will have to run for up to &lt;em&gt;4N&lt;/em&gt; iterations, where &lt;em&gt;N&lt;/em&gt;
is a rather sizable 19 digit number. That's just not going to cut it &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;; we
need a better approach.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="search-by-sieving"&gt;
&lt;h2&gt;Search by sieving&lt;/h2&gt;
&lt;p&gt;The best way to understand this algorithm is to sit down with a piece of paper
and a pencil and try to work through a CRT problem by hand. A key insight that
may help is that there is a unique solution for every subset of the CRT problem
as well.&lt;/p&gt;
&lt;p&gt;For example, let's take the first problem (remainders 0, 3, 4 and moduli 3, 4,
5); looking only at the first two congruences, we can find a unique solution
(modulo 12) - in this case 3. By the CRT itself, we know this solution is
unique, and any number in the arithmetic progression &lt;em&gt;3+12k&lt;/em&gt; is also a solution.&lt;/p&gt;
&lt;p&gt;Therefore, we can build a solution by induction, starting with the first
congruence and moving to the next each time.&lt;/p&gt;
&lt;p&gt;For just the first congruence, &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/4defb0f4de00afd6d2cb1ddbc91522bc12802e55.svg" style="height: 11px;" type="image/svg+xml"&gt;a_1&lt;/object&gt; itself is a trivial solution. But so
are all &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b778e2bdded7939d33f3142d109347119337215f.svg" style="height: 15px;" type="image/svg+xml"&gt;a_1+kn_1&lt;/object&gt;, for each integer &lt;em&gt;k&lt;/em&gt;. One of these will be a solution
to the second congruence as well. Let's call this solution &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a8728ff397f08f1999170f64ff5838333f755380.svg" style="height: 11px;" type="image/svg+xml"&gt;x_2&lt;/object&gt;; it is a
solution for the first two congruences. We can continue the same approach; since
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a8728ff397f08f1999170f64ff5838333f755380.svg" style="height: 11px;" type="image/svg+xml"&gt;x_2&lt;/object&gt; is unique modulo &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/1ac4f2c443e79ada6cf0475e5e9a3fc6e5c2c082.svg" style="height: 11px;" type="image/svg+xml"&gt;n_1n_2&lt;/object&gt;, we'll start looking for a solution
for the third congruence by checking &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a660c06a3a7954773476538b696410c6f9042a11.svg" style="height: 15px;" type="image/svg+xml"&gt;x_2+kn_1n_2&lt;/object&gt; for each integer &lt;em&gt;k&lt;/em&gt;.
And so on, until we find a solution to all the congruences.&lt;/p&gt;
&lt;p&gt;If you're having trouble following this explanation with a piece of paper, try
reading the Wikipedia description for &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem#Computation"&gt;&amp;quot;Search by sieving&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here is the code that implements this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;crtSieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int64&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;base&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;incr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nx"&gt;nextBase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// This loop goes over the congruences one by one; base is a solution&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// to the congruences seen so far.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Find a solution that works for the new congruence a[i] as well.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;candidate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;base&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;candidate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;candidate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;incr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;candidate&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;base&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;candidate&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;incr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nextBase&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Inner loop exited without finding candidate&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;base&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By the way, for this approach to be maximally effective we should sort the
congruences by decreasing modulo (solve the congruence with the largest &lt;em&gt;n&lt;/em&gt;
first).&lt;/p&gt;
&lt;p&gt;Applied to the larger problem presented at the end of the previous section, this
algorithm solves it in half a millisecond on my machine. Not a bad improvement
vs. the ~forever time it takes using the naive approach!&lt;/p&gt;
&lt;p&gt;That said, this approach is still exponential! For really large problems (think
public cryptography-level numbers) we'll need something better.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-the-proof-construction"&gt;
&lt;h2&gt;Using the proof construction&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://eli.thegreenplace.net/2019/the-chinese-remainder-theorem/"&gt;proof of the CRT&lt;/a&gt; includes a
construction of the solution that we could implement in code. A quick reminder:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/97aca0d663e04fe8ebfadcd87053758dad9b08af.svg" style="height: 21px;" type="image/svg+xml"&gt;\[x=a_1 N_1 N&amp;#x27;_1+a_2 N_2 N&amp;#x27;_2+\cdots +a_k N_k N&amp;#x27;_k\]&lt;/object&gt;
&lt;p&gt;Is a solution, where &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/962577ced41b97a773cca5462d4d68b22375bd8d.svg" style="height: 24px;" type="image/svg+xml"&gt;N_k=\frac{N}{n_k}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/bf042bd7a5b6ab321af6ac1dbba45dd3cba86d40.svg" style="height: 19px;" type="image/svg+xml"&gt;N&amp;#x27;_k&lt;/object&gt; is the
multiplicative inverse of &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/8f94afd90555960e1ac40d2908475e16922594bc.svg" style="height: 15px;" type="image/svg+xml"&gt;N_k&lt;/object&gt; modulo &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b1d70855b10553d5c5a4d03b4018211bcf0114c8.svg" style="height: 11px;" type="image/svg+xml"&gt;n_k&lt;/object&gt;. Finding a
multiplicative modular inverse can be done efficiently with the (extended)
Euclidean algorithm, so we should be good as long as we can handle potentially
enormous numbers.&lt;/p&gt;
&lt;p&gt;In Go, this is easy with the &lt;tt class="docutils literal"&gt;math/big&lt;/tt&gt; package &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. Here's an
implementation; unlike the previous ones, it uses &lt;tt class="docutils literal"&gt;big.Int&lt;/tt&gt; instead of
&lt;tt class="docutils literal"&gt;int64&lt;/tt&gt;, so it can handle integers of arbitrary size (machine memory
permitting):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;crtConstruct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Compute N: product(n[...])&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Mul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// x is the accumulated answer.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Nk = N/nk&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Nk&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Div&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// N&amp;#39;k (Nkp) is the multiplicative inverse of Nk modulo nk.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Nkp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Nkp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ModInverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Nk&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;nk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;big&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;NewInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// x += ak*Nk*Nkp&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Nkp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Mul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Nkp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Mul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Nkp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Nk&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Mod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looking at the formula at the beginning of this section, following this code
should be straightforward. The complexity of this algorithm is quadratic, and
it's &lt;em&gt;much&lt;/em&gt; faster than the sieve method on large inputs.&lt;/p&gt;
&lt;p&gt;To make the comparison fair, I've implemented a version of &lt;tt class="docutils literal"&gt;crtSieve&lt;/tt&gt; that
uses &lt;tt class="docutils literal"&gt;big.Int&lt;/tt&gt; as well (since the version shown above is limited by the size
of &lt;tt class="docutils literal"&gt;int64)&lt;/tt&gt; and ran it vs. &lt;tt class="docutils literal"&gt;crtConstruct&lt;/tt&gt; on a large-ish CRT problem where
the solution has 144 bits (a 44-digit number in decimal). &lt;tt class="docutils literal"&gt;crtConstruct&lt;/tt&gt;
was ~20 times faster, in my measurements.&lt;/p&gt;
&lt;p&gt;You can see all the code for this post, along with some tests and simple
benchmarks, in &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2020/chinese-remainder-theorem"&gt;this repository&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The solution, in case you were wondering, is 4412381708627286819.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I have to admit Go really shines here. Since it comes with an
industrial-strength crypto package which itself relies on arbitrary-sized
integers, &lt;tt class="docutils literal"&gt;math/big&lt;/tt&gt; has a whole bunch of goodies that are useful for
number-theoretical computations. For example the &lt;tt class="docutils literal"&gt;ModInverse&lt;/tt&gt; method is
built-in (and if you need something more general, there's also a &lt;tt class="docutils literal"&gt;GCD&lt;/tt&gt;
method for computing the full extended Euclidean algorithm).&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Go"></category></entry></feed>